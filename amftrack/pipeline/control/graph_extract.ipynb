{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from amftrack.util.sys import (\n",
    "    get_dates_datetime,\n",
    "    get_dirname,\n",
    "    temp_path,\n",
    "    get_data_info,\n",
    "    update_plate_info,\n",
    "    get_current_folders,\n",
    "    get_folders_by_plate_id,\n",
    ")\n",
    "import ast\n",
    "from amftrack.plotutil import plot_t_tp1\n",
    "from scipy import sparse\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "from pymatreader import read_mat\n",
    "from matplotlib import colors\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.filters import frangi\n",
    "from skimage import filters\n",
    "from random import choice\n",
    "import scipy.sparse\n",
    "import os\n",
    "from skimage.feature import hessian_matrix_det\n",
    "from amftrack.pipeline.launching.run_super import *\n",
    "from amftrack.pipeline.launching.run import *\n",
    "\n",
    "from amftrack.pipeline.functions.image_processing.experiment_class_surf import (\n",
    "    Node,\n",
    "    Experiment,\n",
    "    save_graphs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import profile_line\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 1, 1, 2, 2, 2]])\n",
    "img = np.vstack([np.zeros_like(x), x, x, x, np.zeros_like(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 2, 2, 2],\n",
       "       [1, 1, 1, 2, 2, 2],\n",
       "       [1, 1, 1, 2, 2, 2],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/temp/ipykernel_1267798/1864472840.py:1: FutureWarning: Default out of bounds interpolation mode 'constant' is deprecated. In version 0.19 it will be set to 'reflect'. To avoid this warning, set `mode=` explicitly.\n",
      "  profile_line(img, (1, 1), (3, 3))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 2.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_line(img, (1, 1), (3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19bdf52c8f4e4f85b0cc57f473c9d1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "analysed:   0%|          | 0/3881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/sun-temp/TEMP/PRINCE_syncing/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mupdate_plate_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m all_folders \u001b[38;5;241m=\u001b[39m get_current_folders(directory, local\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/pycode/MscThesis/amftrack/util/sys.py:265\u001b[0m, in \u001b[0;36mupdate_plate_info\u001b[0;34m(directory, local, strong_constraint, suffix_data_info)\u001b[0m\n\u001b[1;32m    257\u001b[0m     is_real_folder \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    258\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\n\u001b[1;32m    259\u001b[0m             os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path_snap, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImg_r03_c05.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    263\u001b[0m     )\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_real_folder:\n\u001b[0;32m--> 265\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[43mget_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m     ss \u001b[38;5;241m=\u001b[39m folder\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    267\u001b[0m     ff \u001b[38;5;241m=\u001b[39m folder\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/pycode/MscThesis/amftrack/util/sys.py:165\u001b[0m, in \u001b[0;36mget_param\u001b[0;34m(folder, directory)\u001b[0m\n\u001b[1;32m    163\u001b[0m path_snap \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, folder)\n\u001b[1;32m    164\u001b[0m file1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path_snap, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparam.m\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m Lines \u001b[38;5;241m=\u001b[39m \u001b[43mfile1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m ldict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m Lines:\n",
      "File \u001b[0;32m~/miniconda3/envs/amftrack/lib/python3.9/codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_buffer_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, errors, final):\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;66;03m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;66;03m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m    322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_decode(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors, final)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "directory = r\"/mnt/sun-temp/TEMP/PRINCE_syncing/\"\n",
    "update_plate_info(directory, local=True)\n",
    "\n",
    "all_folders = get_current_folders(directory, local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_name = \"20220708_2350_Plate07\"\n",
    "folders = all_folders.loc[all_folders[\"folder\"] == directory_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***extract_skel***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c2a62778d344a19dccaf155638811d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "folder_treated:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3 /home/ipausers/bisot/pycode/MscThesis/amftrack/pipeline/scripts/image_processing/extract_skel_2.py 30 93 99.5 /mnt/sun/home-folder/bisot/PRINCE_test/ 1664544284288373077 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ipausers/bisot/miniconda3/envs/amftrack/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release\n",
      "/home/ipausers/bisot/pycode/MscThesis/amftrack/pipeline/scripts/image_processing/extract_skel_2.py:75: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning dissapear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  im = imageio.imread(directory + directory_name + imname)\n",
      "/home/ipausers/bisot/pycode/MscThesis/amftrack/pipeline/scripts/image_processing/extract_skel_2.py:77: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning dissapear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  im2 = imageio.imread(directory + directory_name + imname2)\n"
     ]
    }
   ],
   "source": [
    "hyph_width = 30\n",
    "perc_low = 93\n",
    "perc_high = 99.5\n",
    "args = [hyph_width, perc_low, perc_high, directory]\n",
    "run(\n",
    "    \"extract_skel_2.py\",\n",
    "    args,\n",
    "    folders,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***Create graphs***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 507919\n",
      "Submitted batch job 507921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: You will be charged for 1.0 node. A full node consists of 128 CPU cores, 960000 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: You will be charged for 1.0 node. A full node consists of 128 CPU cores, 960000 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n"
     ]
    }
   ],
   "source": [
    "num_parallel = 128\n",
    "time = \"5:00:00\"\n",
    "args = [directory]\n",
    "run_parallel(\n",
    "    \"extract_nx_graph.py\",\n",
    "    args,\n",
    "    folders,\n",
    "    num_parallel,\n",
    "    time,\n",
    "    \"extract_nx\",\n",
    "    cpus=128,\n",
    "    node=\"fat\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "336"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plates = [22, 10, 76, 26, 152, 102]\n",
    "\n",
    "folders = all_folders.loc[\n",
    "    (all_folders[\"/Analysis/nx_graph_pruned.p\"]) & (all_folders[\"Plate\"].isin(plates))\n",
    "]\n",
    "len(folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Extract Width***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 508441\n",
      "Submitted batch job 508442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: You will be charged for 0.25 node. A full node consists of 128 CPU cores, 960000 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: You will be charged for 0.25 node. A full node consists of 128 CPU cores, 960000 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 508443\n",
      "Submitted batch job 508444\n",
      "Submitted batch job 508445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: You will be charged for 0.25 node. A full node consists of 128 CPU cores, 960000 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: You will be charged for 0.25 node. A full node consists of 128 CPU cores, 960000 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: You will be charged for 0.25 node. A full node consists of 128 CPU cores, 960000 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n"
     ]
    }
   ],
   "source": [
    "num_parallel = 128\n",
    "time = \"24:00:00\"\n",
    "skip = False\n",
    "resolution = 50\n",
    "args = [directory, skip, resolution]\n",
    "run_parallel(\n",
    "    \"extract_width.py\",\n",
    "    args,\n",
    "    folders,\n",
    "    num_parallel,\n",
    "    time,\n",
    "    \"extract_width2\",\n",
    "    cpus=32,\n",
    "    node=\"fat\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plates = [22, 10, 76, 26, 152, 102]\n",
    "\n",
    "folders = all_folders.loc[\n",
    "    (1 - all_folders[\"/Analysis/nx_graph_pruned_width.p\"])\n",
    "    & (all_folders[\"Plate\"].isin(plates))\n",
    "    & (all_folders[\"/Analysis/nx_graph_pruned.p\"])\n",
    "]\n",
    "# folders.loc[folders['folder'] == '20210110_1523_Plate30']\n",
    "# int(list(folders['folder'])[0].split('_')[-1][5:])\n",
    "len(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "336"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plates = [22, 10, 76, 26, 152, 102]\n",
    "\n",
    "folders = all_folders.loc[\n",
    "    (all_folders[\"/Analysis/nx_graph_pruned_width.p\"])\n",
    "    & (all_folders[\"Plate\"].isin(plates))\n",
    "]\n",
    "# folders.loc[folders['folder'] == '20210110_1523_Plate30']\n",
    "# int(list(folders['folder'])[0].split('_')[-1][5:])\n",
    "len(folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Identify Nodes***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 535639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: You will be charged for 0.25 node. A full node consists of 128 CPU cores, 960000 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n"
     ]
    }
   ],
   "source": [
    "num_parallel = 32\n",
    "time = \"1:00:00\"\n",
    "limit = 55\n",
    "skip = False\n",
    "args = [directory, limit, skip]\n",
    "run_parallel_all_time(\n",
    "    \"extract_nodes.py\",\n",
    "    args,\n",
    "    folders,\n",
    "    num_parallel,\n",
    "    time,\n",
    "    \"node_id\",\n",
    "    cpus=32,\n",
    "    node=\"fat\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: You will be charged for 1.0 node. A full node consists of 128 CPU cores, 983040 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1089373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: You will be charged for 1.0 node. A full node consists of 128 CPU cores, 983040 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1089374\n",
      "Submitted batch job 1089375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: You will be charged for 1.0 node. A full node consists of 128 CPU cores, 983040 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n"
     ]
    }
   ],
   "source": [
    "plates = list(set(list(folders[\"unique_id\"].values)))\n",
    "args = [directory]\n",
    "num_parallel = 128\n",
    "\n",
    "for unique_id in plates:\n",
    "    select = folders.loc[folders[\"unique_id\"] == unique_id]\n",
    "    time = \"2:00:00\"\n",
    "    run_parallel(\n",
    "        \"track_nodes.py\",\n",
    "        args,\n",
    "        select,\n",
    "        num_parallel,\n",
    "        time,\n",
    "        \"track_node\",\n",
    "        cpus=128,\n",
    "        node=\"fat\",\n",
    "        name_job=\"track.sh\",\n",
    "    )\n",
    "\n",
    "time = \"30:00\"\n",
    "run_parallel_all_time(\n",
    "    \"make_labeled_graphs.py\",\n",
    "    args,\n",
    "    folders,\n",
    "    num_parallel,\n",
    "    time,\n",
    "    \"make_graphs\",\n",
    "    cpus=128,\n",
    "    node=\"fat\",\n",
    "    dependency=True,\n",
    "    name_job=\"track.sh\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-01 21:39:00\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home2/cbisot/pycode/AMFtrack/amftrack/pipeline/functions/image_processing/experiment_class_surf.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.folders[\"date\"], format=\"%d.%m.%Y, %H:%M:\"\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/projects/0/einf914/agg/20210901_2139_Plate34/Analysis/nx_graph_pruned_labeled.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/scratch-local/cbisot/ipykernel_1336691/3241373196.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mselect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfolders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_labeled'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/gpfs/home2/cbisot/pycode/AMFtrack/amftrack/pipeline/functions/image_processing/experiment_class_surf.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, folders, suffix)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Analysis\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf\"nx_graph_pruned{suffix}.p\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mpath_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_snap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mnx_graph_poss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/projects/0/einf914/agg/20210901_2139_Plate34/Analysis/nx_graph_pruned_labeled.p'"
     ]
    }
   ],
   "source": [
    "folders = folders.sort_values(\"datetime\")\n",
    "select = folders.iloc[:10]\n",
    "exp = Experiment(directory)\n",
    "exp.load(select, suffix=\"_labeled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hyphae extraction***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 551041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: You will be charged for 0.25 node. A full node consists of 128 CPU cores, 960000 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n"
     ]
    }
   ],
   "source": [
    "num_parallel = 32\n",
    "time = \"8:00:00\"\n",
    "limit = 55\n",
    "version = 11\n",
    "labeled = True\n",
    "args = [directory, limit, version, labeled]\n",
    "run_parallel_all_time(\n",
    "    \"hyphae_extraction.py\",\n",
    "    args,\n",
    "    folders,\n",
    "    num_parallel,\n",
    "    time,\n",
    "    \"hyphae\",\n",
    "    cpus=32,\n",
    "    node=\"fat\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d978fe5d041c4c0281e8284f18deb999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plate treated:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python c:\\users\\coren\\documents\\phd\\code\\amftrack\\amftrack\\pipeline/scripts/image_processing/hyphae_extraction.py C:\\Users\\coren\\Documents\\PhD\\Code\\old_prince_data/ 10000 1 True 1656015105079141800 0\n"
     ]
    }
   ],
   "source": [
    "limit = 10000\n",
    "version = 1\n",
    "labeled = True\n",
    "args = [directory, limit, version, labeled]\n",
    "run_all_time(\"hyphae_extraction.py\", args, folders, pyt_vers=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folders[\"unique_id\"] = folders[\"unique_id\"].astype(str)\n",
    "folders.to_json(\"test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json(\"test.json\", dtype={\"unique_id\": str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    94\n",
       "1    94\n",
       "2    94\n",
       "3    94\n",
       "4    94\n",
       "Name: Plate, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"Plate\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
