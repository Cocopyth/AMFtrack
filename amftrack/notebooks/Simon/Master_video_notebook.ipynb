{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master video notebook!\n",
    "This title might be a bit ambitious, but this notebook is supposed to be able to do all of the administration work when it comes to analysing videos. Of course, many functions are already inherent to the kymo_class file and the plot_data file. Ultimately, this notebook is about creating a file hierarchy for the analysis files.\n",
    "\n",
    "In step one, the Dropbox is scoured for information about videos. If the videos do not have a VideoInfo.txt, the program will look for a .csv, if there is no .csv, the program will look for a .xlsx file. This is currently in conflict with what is happening in the kymo_class.py file, so that one will have to be amended.\n",
    "\n",
    "### Let's say that there are three ways to initiate a kymograph class:\n",
    "1. No info file is submitted, and the class will look for such a file itself.\n",
    "2. An info file is submitted, and the class will use the data in there.\n",
    "3. A kymograph is submitted, and the class will instantiate with the parameters that are passed with the kymograph.\n",
    "\n",
    "TODO: Make it so in the kymo_class\n",
    "TODO: Streamline the variable storage such that edge properties are stored in the edge_analysis class, and video properties are stored in the video_analysis class.\n",
    "\n",
    "### Below code:\n",
    "Are just import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T11:15:51.788351100Z",
     "start_time": "2023-07-12T11:15:46.902930800Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import re\n",
    "from amftrack.pipeline.development.high_mag_videos.high_mag_analysis import (\n",
    "    HighmagDataset,\n",
    "    VideoDataset,\n",
    "    EdgeDataset\n",
    ")\n",
    "from amftrack.pipeline.development.high_mag_videos.kymo_class import *\n",
    "from amftrack.pipeline.development.high_mag_videos.plot_data import (\n",
    "    save_raw_data,\n",
    "    plot_summary,\n",
    "    read_video_data\n",
    ")\n",
    "import sys\n",
    "import os\n",
    "import imageio.v2 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tifffile import imwrite\n",
    "from tqdm import tqdm\n",
    "%matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from amftrack.pipeline.functions.image_processing.extract_graph import (\n",
    "    from_sparse_to_graph,\n",
    "    generate_nx_graph,\n",
    "    clean_degree_4,\n",
    ")\n",
    "import scipy\n",
    "import matplotlib as mpl\n",
    "\n",
    "from amftrack.pipeline.launching.run import (\n",
    "    run_transfer,\n",
    ")\n",
    "from amftrack.pipeline.launching.run_super import run_parallel_transfer\n",
    "\n",
    "import dropbox\n",
    "from amftrack.util.dbx import upload_folders, download, read_saved_dropbox_state, save_dropbox_state, load_dbx, download, get_dropbox_folders, get_dropbox_video_folders\n",
    "from subprocess import call\n",
    "import logging\n",
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "from amftrack.pipeline.launching.run_super import run_parallel_flows\n",
    "\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.debug)\n",
    "mpl.rcParams['figure.dpi'] = 300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File declaration\n",
    "As this notebook is designed to work with Snellius, two items to separate are the raw video files and the anaylsis. The raw video files are large, bulky and not so easy to flip through. Ideally, the video files would be downloaded and the analysis would be stored on a separate folder structure entirely. That way, large scale analysis of analysis folders can happen when there are thousands of videos in the dataset, without having to have those raw video folders on hand.\n",
    "\n",
    "### Input and output:\n",
    "Please give separately the folder where raw video data is stored, and where the analysis will be stored. \n",
    "Also give the dropbox address of the dataset you want to analyze, this is the first filtering step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T11:17:13.232955900Z",
     "start_time": "2023-07-12T11:17:13.009160100Z"
    }
   },
   "outputs": [],
   "source": [
    "videos_folder = \"F:\\\\AMOLF_Data\\\\videos\\\\\"\n",
    "analysis_folder = \"F:\\\\AMOLF_Data\\\\analysis\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T11:17:13.494779300Z",
     "start_time": "2023-07-12T11:17:13.345856400Z"
    }
   },
   "outputs": [],
   "source": [
    "# dropbox_address = \"/DATA/FLUORESCENCE/DATA_NileRed/\"\n",
    "# dropbox_address=  \"/DATA/MYRISTATE/DATA/\"\n",
    "# dropbox_address = \"/DATA/TransportROOT/DATA/\"\n",
    "dropbox_address = \"/DATA/CocoTransport/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go?\n",
    "If you want to download videos:\n",
    "Use MODULE 1\n",
    "\n",
    "If you want to analyze already downloaded videos:\n",
    "Skip MODULE 1, use MODULE 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Downloading videos from DropBox\n",
    "\n",
    "## Dropbox scrounging\n",
    "The below code is meant to scour the dropbox for information files on the videos. It is also to create a list of the videos within a certain database. The expectation at the very least is that one plate contains many videos, all labeled with a number. The code will take this list, and recreate the hierarchy within the Analysis_Output folder.\n",
    "\n",
    "### Input:\n",
    "The input will be the highest folder of the dropbox that needs to be analyzed. You can also set REDO_SCROUNGING to True if you want the dropbox scrounging to happen no maatter what.\n",
    "\n",
    "### Output\n",
    "The output will be a DataFrame that can be filtered in the next code block to prepare for downloading. In addition, .json files will also be created for the specific file that is recorded in the input. That way, scrounging the dropbox only has to be done once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-07-12T11:17:54.403424200Z",
     "start_time": "2023-07-12T11:17:54.195423700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  folder  Plate number  Date Imaged  \\\n0    Img           403     20230703   \n\n                                  tot_path_drop video  \n0  DATA/CocoTransport/20230703_Plate403/001/Img   Img  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>folder</th>\n      <th>Plate number</th>\n      <th>Date Imaged</th>\n      <th>tot_path_drop</th>\n      <th>video</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Img</td>\n      <td>403</td>\n      <td>20230703</td>\n      <td>DATA/CocoTransport/20230703_Plate403/001/Img</td>\n      <td>Img</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REDO_SCROUNGING = False\n",
    "\n",
    "analysis_json = f\"{analysis_folder}{dropbox_address[6:]}all_folders_drop.json\"\n",
    "if os.path.exists(analysis_json):\n",
    "    all_folders_drop = pd.read_json(analysis_json)\n",
    "excel_json = f\"{analysis_folder}{dropbox_address[6:]}excel_drop.json\"\n",
    "if os.path.exists(excel_json):\n",
    "    excel_drop = pd.read_json(excel_json, typ='series')\n",
    "if not os.path.exists(analysis_json) or REDO_SCROUNGING:\n",
    "    all_folders_drop, excel_drop, txt_drop = get_dropbox_video_folders(dropbox_address, True)\n",
    "\n",
    "    clear_output(wait=False)\n",
    "    \n",
    "    excel_addresses = np.array([re.search(\"^.*Plate.*\\/.*Plate.*$\", entry, re.IGNORECASE) for entry in excel_drop])\n",
    "    excel_addresses = excel_addresses[excel_addresses != None]\n",
    "    excel_addresses = [address.group(0) for address in excel_addresses]\n",
    "    excel_drop = np.concatenate([excel_addresses,txt_drop])\n",
    "    if not os.path.exists(f\"{analysis_folder}{dropbox_address[6:]}\"):\n",
    "        os.makedirs(f\"{analysis_folder}{dropbox_address[6:]}\")\n",
    "    all_folders_drop.to_json(analysis_json)\n",
    "    pd.Series(excel_drop).to_json(excel_json)\n",
    "    print(all_folders_drop['tot_path_drop'][0])\n",
    "    \n",
    "all_folders_drop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files downloaded!\n"
     ]
    }
   ],
   "source": [
    "info_addresses  = []\n",
    "for address in excel_drop:\n",
    "    csv_name_len = len(address.split(os.sep)[-1])\n",
    "    if not os.path.exists(analysis_folder + address[6:-csv_name_len]):\n",
    "        os.makedirs(analysis_folder + address[6:-csv_name_len])\n",
    "    if not os.path.exists(analysis_folder + address[6:]):\n",
    "        download(address, analysis_folder + address[6:])\n",
    "    info_addresses.append(analysis_folder + address[6:])\n",
    "clear_output(wait=False)\n",
    "print(\"All files downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T11:18:02.223277600Z",
     "start_time": "2023-07-12T11:18:02.025107400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 81.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "  imaging_day                storage_path           plate_id    root strain  \\\n0    20230703  Dropbox\\DATA\\CocoTransport  20230703_Plate403  Carrot     C2   \n\n      treatment crossing_day  video_int  time_(s) mode  ...  Fiber Led  \\\n0  001P100N100C     20230626          1      30.0   BF  ...         On   \n\n     xpos  ypos   zpos            unique_id                       tot_path  \\\n0  12.266  0.32  5.993  AMOLF_Data_analysis  rt/20230703_Plate403/001/Img/   \n\n                        tot_path_drop  record_time  days_after_crossing  \\\n0  DATA/rt/20230703_Plate403/001/Img/     16:45:29                    7   \n\n  magnification  \n0          50.0  \n\n[1 rows x 25 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>imaging_day</th>\n      <th>storage_path</th>\n      <th>plate_id</th>\n      <th>root</th>\n      <th>strain</th>\n      <th>treatment</th>\n      <th>crossing_day</th>\n      <th>video_int</th>\n      <th>time_(s)</th>\n      <th>mode</th>\n      <th>...</th>\n      <th>Fiber Led</th>\n      <th>xpos</th>\n      <th>ypos</th>\n      <th>zpos</th>\n      <th>unique_id</th>\n      <th>tot_path</th>\n      <th>tot_path_drop</th>\n      <th>record_time</th>\n      <th>days_after_crossing</th>\n      <th>magnification</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20230703</td>\n      <td>Dropbox\\DATA\\CocoTransport</td>\n      <td>20230703_Plate403</td>\n      <td>Carrot</td>\n      <td>C2</td>\n      <td>001P100N100C</td>\n      <td>20230626</td>\n      <td>1</td>\n      <td>30.0</td>\n      <td>BF</td>\n      <td>...</td>\n      <td>On</td>\n      <td>12.266</td>\n      <td>0.32</td>\n      <td>5.993</td>\n      <td>AMOLF_Data_analysis</td>\n      <td>rt/20230703_Plate403/001/Img/</td>\n      <td>DATA/rt/20230703_Plate403/001/Img/</td>\n      <td>16:45:29</td>\n      <td>7</td>\n      <td>50.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 25 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_frame = read_video_data(info_addresses, all_folders_drop)\n",
    "merge_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T11:18:18.892655Z",
     "start_time": "2023-07-12T11:18:18.875559700Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropbox filtering\n",
    "If you want all videos in a plate or dataset, you can run this block and forget about it.\n",
    "Otherwise, this block is where filtering can take place to only download videos with certain properties, like imaging mode. Then a file structure will be created in the videos and analysis folder, and videoInfo.txt folders are created for every video, in the analysis folder. The intent here is to have a uniform Analysis folder structure that works with the Morrison setup.\n",
    "\n",
    "### Input\n",
    "Use the section between commented lines to filter the DataFrame, otherwise leave blank\n",
    "### Output\n",
    "Within the video and analysis folder, a hierarchy will be created to mimic that of the dropbox folder structure, using the filtered DataFrame. Inside the analysis folder hierarchy, the VideoInfo.txt file will be generated. Either from an existing videoInfo.txt, or the excel/csv sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T11:19:05.809238100Z",
     "start_time": "2023-07-12T11:19:05.651081800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  imaging_day                storage_path           plate_id    root strain  \\\n0    20230703  Dropbox\\DATA\\CocoTransport  20230703_Plate403  Carrot     C2   \n\n      treatment crossing_day  video_int  time_(s) mode  ...  Fiber Led  \\\n0  001P100N100C     20230626          1      30.0   BF  ...         On   \n\n     xpos  ypos   zpos            unique_id                         folder  \\\n0  12.266  0.32  5.993  AMOLF_Data_analysis  rt/20230703_Plate403/001/Img/   \n\n                        tot_path_drop  record_time  days_after_crossing  \\\n0  DATA/rt/20230703_Plate403/001/Img/     16:45:29                    7   \n\n  magnification  \n0          50.0  \n\n[1 rows x 25 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>imaging_day</th>\n      <th>storage_path</th>\n      <th>plate_id</th>\n      <th>root</th>\n      <th>strain</th>\n      <th>treatment</th>\n      <th>crossing_day</th>\n      <th>video_int</th>\n      <th>time_(s)</th>\n      <th>mode</th>\n      <th>...</th>\n      <th>Fiber Led</th>\n      <th>xpos</th>\n      <th>ypos</th>\n      <th>zpos</th>\n      <th>unique_id</th>\n      <th>folder</th>\n      <th>tot_path_drop</th>\n      <th>record_time</th>\n      <th>days_after_crossing</th>\n      <th>magnification</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20230703</td>\n      <td>Dropbox\\DATA\\CocoTransport</td>\n      <td>20230703_Plate403</td>\n      <td>Carrot</td>\n      <td>C2</td>\n      <td>001P100N100C</td>\n      <td>20230626</td>\n      <td>1</td>\n      <td>30.0</td>\n      <td>BF</td>\n      <td>...</td>\n      <td>On</td>\n      <td>12.266</td>\n      <td>0.32</td>\n      <td>5.993</td>\n      <td>AMOLF_Data_analysis</td>\n      <td>rt/20230703_Plate403/001/Img/</td>\n      <td>DATA/rt/20230703_Plate403/001/Img/</td>\n      <td>16:45:29</td>\n      <td>7</td>\n      <td>50.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 25 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################################################\n",
    "### This is where you can apply the filters. Only those videos will be downloaded ###\n",
    "#####################################################################################\n",
    "\n",
    "download_frame = merge_frame\n",
    "\n",
    "#####################################################################################\n",
    "### Below code will prepare for those videos to be downloaded into videos_folder  ###\n",
    "#####################################################################################\n",
    "\n",
    "download_frame = download_frame.rename(columns={'tot_path' : 'folder'})\n",
    "download_frame = download_frame.sort_values('unique_id')\n",
    "download_frame = download_frame.reset_index(drop=True)\n",
    "download_frame = download_frame.loc[:,~download_frame.columns.duplicated()].copy()\n",
    "\n",
    "download_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T11:19:37.089046Z",
     "start_time": "2023-07-12T11:19:36.924875900Z"
    }
   },
   "outputs": [],
   "source": [
    "download_frame['analysis_folder'] = [np.nan for i in range(len(download_frame))]\n",
    "download_frame['videos_folder'] = [np.nan for i in range(len(download_frame))]\n",
    "# download_frame = download_frame.drop(columns=['index'], axis=1)\n",
    "\n",
    "for index, row in download_frame.iterrows():\n",
    "    target_anals_file = f\"{analysis_folder}{row['folder'][:-4]}\"\n",
    "#     print(target_anals_file)\n",
    "    target_video_file = f\"{videos_folder}{row['folder']}\"\n",
    "    \n",
    "    row.loc['analysis_folder'] = target_anals_file\n",
    "    row.loc['videos_folder'] = target_video_file\n",
    "    \n",
    "    if not os.path.exists(target_anals_file):\n",
    "        os.makedirs(target_anals_file)\n",
    "    if not os.path.exists(target_video_file):\n",
    "        os.makedirs(target_video_file)\n",
    "#     print(row.index)\n",
    "    row.to_json(f\"{target_anals_file}/video_data.json\", orient=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading\n",
    "This section, there is one block of code that will ask you one last time whether all of the parameters are correct. The block of code after that will initiate Snellius jobs to download the videos in the DataFrame from the dropbox. Downloading videos is not that costly, but of course we prefer it to be done as efficiently as possible.\n",
    "### Input:\n",
    "Nothing\n",
    "### Output:\n",
    "Print statement with the DataFrame and the folders where everything will be stored.\n",
    "Subsequent block of code will download raw video files to the videos folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T11:21:06.627631Z",
     "start_time": "2023-07-12T11:21:06.470643600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below videos will be downloaded!\n"
     ]
    },
    {
     "data": {
      "text/plain": "  imaging_day                storage_path           plate_id    root strain  \\\n0    20230703  Dropbox\\DATA\\CocoTransport  20230703_Plate403  Carrot     C2   \n\n      treatment crossing_day  video_int  time_(s) mode  ...  ypos   zpos  \\\n0  001P100N100C     20230626          1      30.0   BF  ...  0.32  5.993   \n\n             unique_id                         folder  \\\n0  AMOLF_Data_analysis  rt/20230703_Plate403/001/Img/   \n\n                        tot_path_drop record_time  days_after_crossing  \\\n0  DATA/rt/20230703_Plate403/001/Img/    16:45:29                    7   \n\n   magnification  analysis_folder videos_folder  \n0           50.0              NaN           NaN  \n\n[1 rows x 27 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>imaging_day</th>\n      <th>storage_path</th>\n      <th>plate_id</th>\n      <th>root</th>\n      <th>strain</th>\n      <th>treatment</th>\n      <th>crossing_day</th>\n      <th>video_int</th>\n      <th>time_(s)</th>\n      <th>mode</th>\n      <th>...</th>\n      <th>ypos</th>\n      <th>zpos</th>\n      <th>unique_id</th>\n      <th>folder</th>\n      <th>tot_path_drop</th>\n      <th>record_time</th>\n      <th>days_after_crossing</th>\n      <th>magnification</th>\n      <th>analysis_folder</th>\n      <th>videos_folder</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20230703</td>\n      <td>Dropbox\\DATA\\CocoTransport</td>\n      <td>20230703_Plate403</td>\n      <td>Carrot</td>\n      <td>C2</td>\n      <td>001P100N100C</td>\n      <td>20230626</td>\n      <td>1</td>\n      <td>30.0</td>\n      <td>BF</td>\n      <td>...</td>\n      <td>0.32</td>\n      <td>5.993</td>\n      <td>AMOLF_Data_analysis</td>\n      <td>rt/20230703_Plate403/001/Img/</td>\n      <td>DATA/rt/20230703_Plate403/001/Img/</td>\n      <td>16:45:29</td>\n      <td>7</td>\n      <td>50.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 27 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Below videos will be downloaded!\")\n",
    "download_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent all the jobs! Use the command '$ squeue' in the terminal to see the progress\n"
     ]
    }
   ],
   "source": [
    "run_parallel_transfer(\n",
    "    \"from_drop.py\",\n",
    "    [videos_folder],\n",
    "    download_frame,\n",
    "    1,\n",
    "    \"10:00:00\",\n",
    "    \"transfer_test\"\n",
    ")\n",
    "clear_output(wait=False)\n",
    "\n",
    "print(\"Sent all the jobs! Use the command '$ squeue' in the terminal to see the progress\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Analysis\n",
    "Now that the files have been downloaded, it's time to analyse them. In the below code, you'll be able to either do a complete survey of the analysis folder for as many videos as possible, or use the DataFrame of recently downloaded videos to filter for the videos you want to analyse.\n",
    "\n",
    "### Input:\n",
    "DataFrame filters of all videos to be analysed\n",
    "### Output:\n",
    "Print statements for all parameters of the analysis session that is about to take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/DATA/MYRISTATE/DATA/\n"
     ]
    }
   ],
   "source": [
    "print(dropbox_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T11:18:30.820271300Z",
     "start_time": "2023-07-12T11:18:30.649412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  imaging_day                storage_path           plate_id    root strain  \\\n0    20230703  Dropbox\\DATA\\CocoTransport  20230703_Plate403  Carrot     C2   \n\n      treatment crossing_day video_int time_(s) mode  ...  ypos   zpos  \\\n0  001P100N100C     20230626         1     30.0   BF  ...  0.32  5.993   \n\n               unique_id                                    folder  \\\n0  20230703_Plate403_001  CocoTransport/20230703_Plate403/001/Img/   \n\n                                   tot_path_drop record_time  \\\n0  DATA/CocoTransport/20230703_Plate403/001/Img/    16:45:29   \n\n  days_after_crossing magnification  \\\n0                   7          50.0   \n\n                                     analysis_folder  \\\n0  /gpfs/home6/svstaalduine/Analysis/CocoTranspor...   \n\n                                       videos_folder  \n0  /gpfs/scratch1/shared/amftrackflow/videos/Coco...  \n\n[1 rows x 27 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>imaging_day</th>\n      <th>storage_path</th>\n      <th>plate_id</th>\n      <th>root</th>\n      <th>strain</th>\n      <th>treatment</th>\n      <th>crossing_day</th>\n      <th>video_int</th>\n      <th>time_(s)</th>\n      <th>mode</th>\n      <th>...</th>\n      <th>ypos</th>\n      <th>zpos</th>\n      <th>unique_id</th>\n      <th>folder</th>\n      <th>tot_path_drop</th>\n      <th>record_time</th>\n      <th>days_after_crossing</th>\n      <th>magnification</th>\n      <th>analysis_folder</th>\n      <th>videos_folder</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20230703</td>\n      <td>Dropbox\\DATA\\CocoTransport</td>\n      <td>20230703_Plate403</td>\n      <td>Carrot</td>\n      <td>C2</td>\n      <td>001P100N100C</td>\n      <td>20230626</td>\n      <td>1</td>\n      <td>30.0</td>\n      <td>BF</td>\n      <td>...</td>\n      <td>0.32</td>\n      <td>5.993</td>\n      <td>20230703_Plate403_001</td>\n      <td>CocoTransport/20230703_Plate403/001/Img/</td>\n      <td>DATA/CocoTransport/20230703_Plate403/001/Img/</td>\n      <td>16:45:29</td>\n      <td>7</td>\n      <td>50.0</td>\n      <td>/gpfs/home6/svstaalduine/Analysis/CocoTranspor...</td>\n      <td>/gpfs/scratch1/shared/amftrackflow/videos/Coco...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 27 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_filter = dropbox_address[5:]\n",
    "\n",
    "img_infos = glob.glob(f\"{analysis_folder}{folder_filter}/**/video_data.json\", recursive=True)\n",
    "vid_anls_frame = pd.DataFrame()\n",
    "for address in img_infos:\n",
    "    add_info = pd.read_json(address, orient='index').T\n",
    "    vid_anls_frame = pd.concat([vid_anls_frame, add_info], ignore_index=True)\n",
    "\n",
    "vid_anls_frame = vid_anls_frame.sort_values('unique_id').reset_index(drop=True)\n",
    "# print(vid_anls_frame.columns)\n",
    "# print([(type(entry), entry) for entry in vid_anls_frame.iloc[0]])\n",
    "vid_anls_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T11:18:33.224483Z",
     "start_time": "2023-07-12T11:18:33.076422200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    20230703_Plate403_001\n"
     ]
    }
   ],
   "source": [
    "####################################################################################\n",
    "### This is where you can apply the filters. Only those videos will be analyzed. ###\n",
    "####################################################################################\n",
    "\n",
    "analysis_frame = vid_anls_frame\n",
    "\n",
    "####################################################################################\n",
    "### Below code will prepare for those videos to be downloaded to videos_folder.  ###\n",
    "####################################################################################\n",
    "\n",
    "print(analysis_frame['unique_id'].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Analysis job\n",
    "Two options: For small analysis, use the first block. This will just do the calculations on the machine. For large-scale analysis, use the second block, as it will create a Snellius job.\n",
    "## Input:\n",
    "Snellius job parameters\n",
    "## Output:\n",
    "Analysis folder will be populated with analysis tiffs and csv sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053727\n",
      "Submitted batch job 3053728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n",
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053760\n",
      "Submitted batch job 3053761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n",
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 1.0 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3053765\n"
     ]
    }
   ],
   "source": [
    "### LARGE VIDEO ANALYSIS\n",
    "\n",
    "nr_parallel = np.min([len(analysis_frame.index), 16])\n",
    "\n",
    "run_parallel_flows(\n",
    "    \"flux_extract.py\",\n",
    "    [analysis_folder, 15, 0.95, 0.005, 60, dropbox_address],\n",
    "    analysis_frame,\n",
    "    nr_parallel,\n",
    "    \"2:00:00\",\n",
    "    \"flux_extract\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Bulk Analysis\n",
    "## First part: Assemble Edge DataFrame\n",
    "\n",
    "\n",
    "In this initial part of the bulk analysis, all of the analysis folders will be looked through to find the edge data we're looking for. Additionally, there is an optional part to download the analysis folder back to the analysis folder we specified right at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T11:19:41.998013Z",
     "start_time": "2023-07-12T11:19:41.845700600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\AMOLF_Data\\analysis\\rt/20230703_Plate403/001/\n"
     ]
    },
    {
     "data": {
      "text/plain": "nan"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hannah_set = HighmagDataset(download_frame)\n",
    "print(f\"{analysis_folder}{download_frame['folder'][0][:-4]}\")\n",
    "download_frame['videos_folder'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assuming all the videos are already downloaded:\n",
    "You can use below code to read the video_data.json files that are created during indexing of all the videos"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
