{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master video notebook!\n",
    "This title might be a bit ambitious, but this notebook is supposed to be able to do all of the administration work when it comes to analysing videos. Of course, many functions are already inherent to the kymo_class file and the plot_data file. Ultimately, this notebook is about creating a file hierarchy for the analysis files.\n",
    "\n",
    "In step one, the Dropbox is scoured for information about videos. If the videos do not have a VideoInfo.txt, the program will look for a .csv, if there is no .csv, the program will look for a .xlsx file. This is currently in conflict with what is happening in the kymo_class.py file, so that one will have to be amended.\n",
    "\n",
    "### Let's say that there are three ways to initiate a kymograph class:\n",
    "1. No info file is submitted, and the class will look for such a file itself.\n",
    "2. An info file is submitted, and the class will use the data in there.\n",
    "3. A kymograph is submitted, and the class will instantiate with the parameters that are passed with the kymograph.\n",
    "\n",
    "TODO: Make it so in the kymo_class\n",
    "TODO: Streamline the variable storage such that edge properties are stored in the edge_analysis class, and video properties are stored in the video_analysis class.\n",
    "\n",
    "### Below code:\n",
    "Are just import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import re\n",
    "from amftrack.pipeline.development.high_mag_videos.kymo_class import *\n",
    "from amftrack.pipeline.development.high_mag_videos.plot_data import (\n",
    "    save_raw_data,\n",
    "    plot_summary\n",
    ")\n",
    "import sys\n",
    "import os\n",
    "import imageio.v2 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tifffile import imwrite\n",
    "from tqdm import tqdm\n",
    "%matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from amftrack.pipeline.functions.image_processing.extract_graph import (\n",
    "    from_sparse_to_graph,\n",
    "    generate_nx_graph,\n",
    "    clean_degree_4,\n",
    ")\n",
    "import scipy\n",
    "import matplotlib as mpl\n",
    "\n",
    "from amftrack.pipeline.launching.run import (\n",
    "    run_transfer,\n",
    ")\n",
    "from amftrack.pipeline.launching.run_super import run_parallel_transfer\n",
    "\n",
    "import dropbox\n",
    "from amftrack.util.dbx import upload_folders, download, read_saved_dropbox_state, save_dropbox_state, load_dbx, download, get_dropbox_folders, get_dropbox_video_folders\n",
    "from subprocess import call\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.debug)\n",
    "mpl.rcParams['figure.dpi'] = 300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File declaration\n",
    "As this notebook is designed to work with Snellius, two items to separate are the raw video files and the anaylsis. The raw video files are large, bulky and not so easy to flip through. Ideally, the video files would be downloaded and the analysis would be stored on a separate folder structure entirely. That way, large scale analysis of analysis folders can happen when there are thousands of videos in the dataset, without having to have those raw video folders on hand.\n",
    "\n",
    "### Input and output:\n",
    "Please give separately the folder where raw video data is stored, and where the analysis will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_folder = \"/gpfs/scratch1/shared/amftrackflow/\"\n",
    "analysis_folder = \"/gpfs/home6/svstaalduine/Analysis/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Downloading videos from DropBox\n",
    "\n",
    "## Dropbox scouring\n",
    "The below code is meant to scour the dropbox for information files on the videos. It is also to create a list of the videos within a certain database. The expectation at the very least is that one plate contains many videos, all labeled with a number. The code will take this list, and recreate the hierarchy within the Analysis_Output folder.\n",
    "\n",
    "### Input and output:\n",
    "The input will be the highest folder of the dropbox that needs to be analyzed. The output will be a DataFrame that can be filtered in the next code block to prepare for downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropbox_address = \"/DATA/FLUORESCENCE/DATA_NileRed/20230201_Plate558/\"\n",
    "dropbox_address=  \"/DATA/MYRISTATE/DATA/2_weeks/20230524_Plate049/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder</th>\n",
       "      <th>Plate number</th>\n",
       "      <th>Date Imaged</th>\n",
       "      <th>tot_path_drop</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20230524_Plate049_001/Img/</td>\n",
       "      <td>049</td>\n",
       "      <td>20230524</td>\n",
       "      <td>DATA/MYRISTATE/DATA/2_weeks/20230524_Plate049/...</td>\n",
       "      <td>001/Img/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20230524_Plate049_002/Img/</td>\n",
       "      <td>049</td>\n",
       "      <td>20230524</td>\n",
       "      <td>DATA/MYRISTATE/DATA/2_weeks/20230524_Plate049/...</td>\n",
       "      <td>002/Img/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20230524_Plate049_003/Img/</td>\n",
       "      <td>049</td>\n",
       "      <td>20230524</td>\n",
       "      <td>DATA/MYRISTATE/DATA/2_weeks/20230524_Plate049/...</td>\n",
       "      <td>003/Img/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20230524_Plate049_004/Img/</td>\n",
       "      <td>049</td>\n",
       "      <td>20230524</td>\n",
       "      <td>DATA/MYRISTATE/DATA/2_weeks/20230524_Plate049/...</td>\n",
       "      <td>004/Img/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20230524_Plate049_005/Img/</td>\n",
       "      <td>049</td>\n",
       "      <td>20230524</td>\n",
       "      <td>DATA/MYRISTATE/DATA/2_weeks/20230524_Plate049/...</td>\n",
       "      <td>005/Img/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20230524_Plate049_006/Img/</td>\n",
       "      <td>049</td>\n",
       "      <td>20230524</td>\n",
       "      <td>DATA/MYRISTATE/DATA/2_weeks/20230524_Plate049/...</td>\n",
       "      <td>006/Img/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20230524_Plate049_007/Img/</td>\n",
       "      <td>049</td>\n",
       "      <td>20230524</td>\n",
       "      <td>DATA/MYRISTATE/DATA/2_weeks/20230524_Plate049/...</td>\n",
       "      <td>007/Img/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20230524_Plate049_008/Img/</td>\n",
       "      <td>049</td>\n",
       "      <td>20230524</td>\n",
       "      <td>DATA/MYRISTATE/DATA/2_weeks/20230524_Plate049/...</td>\n",
       "      <td>008/Img/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       folder Plate number Date Imaged  \\\n",
       "0  20230524_Plate049_001/Img/          049    20230524   \n",
       "1  20230524_Plate049_002/Img/          049    20230524   \n",
       "2  20230524_Plate049_003/Img/          049    20230524   \n",
       "3  20230524_Plate049_004/Img/          049    20230524   \n",
       "4  20230524_Plate049_005/Img/          049    20230524   \n",
       "5  20230524_Plate049_006/Img/          049    20230524   \n",
       "6  20230524_Plate049_007/Img/          049    20230524   \n",
       "7  20230524_Plate049_008/Img/          049    20230524   \n",
       "\n",
       "                                       tot_path_drop     video  \n",
       "0  DATA/MYRISTATE/DATA/2_weeks/20230524_Plate049/...  001/Img/  \n",
       "1  DATA/MYRISTATE/DATA/2_weeks/20230524_Plate049/...  002/Img/  \n",
       "2  DATA/MYRISTATE/DATA/2_weeks/20230524_Plate049/...  003/Img/  \n",
       "3  DATA/MYRISTATE/DATA/2_weeks/20230524_Plate049/...  004/Img/  \n",
       "4  DATA/MYRISTATE/DATA/2_weeks/20230524_Plate049/...  005/Img/  \n",
       "5  DATA/MYRISTATE/DATA/2_weeks/20230524_Plate049/...  006/Img/  \n",
       "6  DATA/MYRISTATE/DATA/2_weeks/20230524_Plate049/...  007/Img/  \n",
       "7  DATA/MYRISTATE/DATA/2_weeks/20230524_Plate049/...  008/Img/  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_folders_drop, excel_drop, txt_drop = get_dropbox_video_folders(dropbox_address, True)\n",
    "\n",
    "clear_output(wait=False)\n",
    "\n",
    "all_folders_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv\n"
     ]
    }
   ],
   "source": [
    "excel_addresses = np.array([re.search(\"^.*Plate.*\\/.*Plate.*$\", entry, re.IGNORECASE) for entry in excel_drop])\n",
    "excel_addresses = excel_addresses[excel_addresses != None]\n",
    "excel_addresses = [address.group(0) for address in excel_addresses]\n",
    "\n",
    "for address in excel_addresses:\n",
    "    suffix = address.split('.')[-1]\n",
    "    print(suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropbox filtering\n",
    "If you want all videos in a plate or dataset, you can run this block and forget about it.\n",
    "Otherwise, this block is where filtering can take place to only download videos with certain properties, like imaging mode. Then a file structure will be created in the videos and analysis folder, and videoInfo.txt folders are created for every video, in the analysis folder. The intent here is to have a uniform Analysis folder structure that works with the Morrison setup.\n",
    "\n",
    "### Input\n",
    "Use the section between commented lines to filter the DataFrame, otherwise leave blank\n",
    "### Output\n",
    "Within the video and analysis folder, a hierarchy will be created to mimic that of the dropbox folder structure, using the filtered DataFrame. Inside the analysis folder hierarchy, the VideoInfo.txt file will be generated. Either from an existing videoInfo.txt, or the excel/csv sheets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading\n",
    "This section, there is one block of code that will ask you one last time whether all of the parameters are correct. The block of code after that will initiate Snellius jobs to download the videos in the DataFrame from the dropbox. Downloading videos is not that costly, but of course we prefer it to be done as efficiently as possible.\n",
    "### Input:\n",
    "Nothing\n",
    "### Output:\n",
    "Print statement with the DataFrame and the folders where everything will be stored.\n",
    "Subsequent block of code will download raw video files to the videos folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Analysis\n",
    "Now that the files have been downloaded, it's time to analyse them. In the below code, you'll be able to either do a complete survey of the analysis folder for as many videos as possible, or use the DataFrame of recently downloaded videos to filter for the videos you want to analyse.\n",
    "\n",
    "### Input:\n",
    "DataFrame of all videos to be analysed\n",
    "### Output:\n",
    "Print statements for all parameters of the analysis session that is about to take place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Analysis job\n",
    "Below code will use the aforementioned DataFrame to initiate analysis jobs on Snellius.\n",
    "## Input:\n",
    "Snellius job parameters\n",
    "## Output:\n",
    "Analysis folder will be populated with analysis tiffs and csv sheets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Bulk Analysis\n",
    "## First part: Assemble Edge DataFrame\n",
    "In this initial part of the bulk analysis, all of the analysis folders will be looked through to find the edge data we're looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
