{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master video notebook!\n",
    "This title might be a bit ambitious, but this notebook is supposed to be able to do all of the administration work when it comes to downloading, processing and analysing videos. The most important functions that will be called are stored in other python files, such that this notebook will remain legible. Analysis will be able to be done with a hierarchy structure of dataset, plate, video, hypha.\n",
    "\n",
    "### In MODULE one,\n",
    "the Dropbox is scoured for information about videos. If the videos do not have a VideoInfo.txt, the program will look for a .csv, if there is no .csv, the program will look for a .xlsx file. Once these files have been found, all information will be merged into a pandas dataframe, and saved as a json file for the dataset and for each video. Some datasets contain thousands of videos, so scouring the dropbox for info on all of them is going to be an hours-long affair. Plan your analysis accordingly.\n",
    "\n",
    "After scouring is complete, a final filtering step can be taken, whereupon the whole list of videos can be downloaded. NB: Downloading happens in two ways: videos are downloaded to the specified analysis folder, whereas video parameters and analysis will be downloaded to the specified analysis folder. This separation is done such that videos can be stored on larger storage drives, and analysis folders on faster storage drives.\n",
    "\n",
    "(if Snellius is still used, it is recommended to use your scratch storage to store videos, and your home storage to store analysis. Scratch storage gets wiped every two weeks, but is much larger than home storage. )\n",
    "\n",
    "TODO: Give options to download with SLURM job or manually.\n",
    "\n",
    "### In MODULE two,\n",
    "the downloaded videos with their respective information can be filtered, then analysed with a large SLURM job. In the future there might need to be functionality that allows processing without the use of a SLURM job. If you're reading this in 2024, you better apply for another Snellius grant!\n",
    "\n",
    "### In MODULE three,\n",
    "This is where all the bulk analysis is going to be. In high_mag_analysis.py, there are a number of classes and functions that will help you with parsing the data into meaningful graphs. This MODULE assumes the existence of the video_info.json files that are generated partly in MODULE 1.\n",
    "\n",
    "### Below code:\n",
    "Are just import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T13:36:09.965148400Z",
     "start_time": "2023-07-19T13:36:03.550326500Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home6/svstaalduine/AMF_project/amftrack/util/dbx.py:3: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "2023-07-30 15:37:04.646247: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-30 15:37:05.165141: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/home6/svstaalduine/.local/lib/python3.9/site-packages/cv2/../../lib64:/sw/arch/Centos8/EB_production/2021/software/ZeroMQ/4.3.4-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/util-linux/2.36-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/libsodium/1.0.18-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/OpenPGM/5.2.122-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/Python/3.9.5-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/OpenSSL/1.1/lib:/sw/arch/Centos8/EB_production/2021/software/libffi/3.3-GCCcore-10.3.0/lib64:/sw/arch/Centos8/EB_production/2021/software/GMP/6.2.1-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/XZ/5.2.5-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/SQLite/3.35.4-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/Tcl/8.6.11-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/libreadline/8.1-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/ncurses/6.2-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/bzip2/1.0.8-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/binutils/2.36.1-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/zlib/1.2.11-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/GCCcore/10.3.0/lib64\n",
      "2023-07-30 15:37:05.165236: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-07-30 15:37:08.630313: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/home6/svstaalduine/.local/lib/python3.9/site-packages/cv2/../../lib64:/sw/arch/Centos8/EB_production/2021/software/ZeroMQ/4.3.4-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/util-linux/2.36-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/libsodium/1.0.18-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/OpenPGM/5.2.122-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/Python/3.9.5-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/OpenSSL/1.1/lib:/sw/arch/Centos8/EB_production/2021/software/libffi/3.3-GCCcore-10.3.0/lib64:/sw/arch/Centos8/EB_production/2021/software/GMP/6.2.1-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/XZ/5.2.5-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/SQLite/3.35.4-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/Tcl/8.6.11-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/libreadline/8.1-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/ncurses/6.2-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/bzip2/1.0.8-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/binutils/2.36.1-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/zlib/1.2.11-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/GCCcore/10.3.0/lib64\n",
      "2023-07-30 15:37:08.630900: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/home6/svstaalduine/.local/lib/python3.9/site-packages/cv2/../../lib64:/sw/arch/Centos8/EB_production/2021/software/ZeroMQ/4.3.4-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/util-linux/2.36-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/libsodium/1.0.18-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/OpenPGM/5.2.122-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/Python/3.9.5-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/OpenSSL/1.1/lib:/sw/arch/Centos8/EB_production/2021/software/libffi/3.3-GCCcore-10.3.0/lib64:/sw/arch/Centos8/EB_production/2021/software/GMP/6.2.1-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/XZ/5.2.5-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/SQLite/3.35.4-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/Tcl/8.6.11-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/libreadline/8.1-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/ncurses/6.2-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/bzip2/1.0.8-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/binutils/2.36.1-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/zlib/1.2.11-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/GCCcore/10.3.0/lib64\n",
      "2023-07-30 15:37:08.630917: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-07-30 15:37:13.750306: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/home6/svstaalduine/.local/lib/python3.9/site-packages/cv2/../../lib64:/sw/arch/Centos8/EB_production/2021/software/ZeroMQ/4.3.4-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/util-linux/2.36-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/libsodium/1.0.18-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/OpenPGM/5.2.122-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/Python/3.9.5-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/OpenSSL/1.1/lib:/sw/arch/Centos8/EB_production/2021/software/libffi/3.3-GCCcore-10.3.0/lib64:/sw/arch/Centos8/EB_production/2021/software/GMP/6.2.1-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/XZ/5.2.5-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/SQLite/3.35.4-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/Tcl/8.6.11-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/libreadline/8.1-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/ncurses/6.2-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/bzip2/1.0.8-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/binutils/2.36.1-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/zlib/1.2.11-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/GCCcore/10.3.0/lib64\n",
      "2023-07-30 15:37:13.750411: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-07-30 15:37:13.750466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (int6): /proc/driver/nvidia/version does not exist\n",
      "2023-07-30 15:37:13.751160: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import re\n",
    "from amftrack.pipeline.development.high_mag_videos.plot_data import (\n",
    "    plot_summary,\n",
    "    save_raw_data,\n",
    ")\n",
    "from amftrack.pipeline.development.high_mag_videos.high_mag_analysis import (\n",
    "    HighmagDataset,\n",
    "    VideoDataset,\n",
    "    EdgeDataset,\n",
    "    index_videos_dropbox,\n",
    "    analysis_run,\n",
    ")\n",
    "from amftrack.pipeline.development.high_mag_videos.kymo_class import (\n",
    "    KymoVideoAnalysis,\n",
    "    KymoEdgeAnalysis\n",
    ")\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio.v2 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tifffile import imwrite\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import matplotlib as mpl\n",
    "from pathlib import Path\n",
    "from amftrack.pipeline.launching.run import (\n",
    "    run_transfer,\n",
    ")\n",
    "from amftrack.pipeline.launching.run_super import run_parallel_transfer\n",
    "import dropbox\n",
    "from amftrack.util.dbx import upload_folder, download, read_saved_dropbox_state, save_dropbox_state, load_dbx, get_dropbox_folders, get_dropbox_video_folders, download_video_folders_drop, download_analysis_folders_drop\n",
    "from subprocess import call\n",
    "import logging\n",
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "from amftrack.pipeline.launching.run_super import run_parallel_flows\n",
    "\n",
    "%matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.debug)\n",
    "mpl.rcParams['figure.dpi'] = 300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File declaration\n",
    "As this notebook is designed to work with Snellius (now also on a local computer!), two items to separate are the raw video files and the analysis. The raw video files are large, bulky and not so easy to flip through. Ideally, the video files would be downloaded and the analysis would be stored on a separate folder structure entirely. That way, large scale analysis of analysis folders can happen when there are thousands of videos in the dataset, without having to have those raw video folders on hand.\n",
    "\n",
    "Below function will basically make your folders fertile ground to accept all the video info folders and raw video files.\n",
    "\n",
    "### Input:\n",
    "Please give separately the folder where raw video data is stored, and where the analysis will be stored. Also give the dropbox address of the dataset you want to analyze.\n",
    "\n",
    "### Output:\n",
    "The specified dropbox folder will be looked through, and all relevant video information will be downloaded to an analysis folder structure identical to what is present on teh dropbox. The relevant raw video folder structure will also be generated, if specified so. Will also create cache files in the form of .json files such that next time, the scrounging does not have to happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T13:36:23.245438800Z",
     "start_time": "2023-07-19T13:36:22.554263700Z"
    }
   },
   "outputs": [],
   "source": [
    "# videos_folder = \"E:\\\\AMOLF_Data\\\\videos\\\\\"\n",
    "# analysis_folder = \"E:\\\\AMOLF_Data\\\\analysis\\\\\"\n",
    "\n",
    "videos_folder = \"/gpfs/scratch1/shared/amftrackflow/videos/\"\n",
    "analysis_folder = \"/gpfs/home6/svstaalduine/Analysis/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T13:36:26.389565100Z",
     "start_time": "2023-07-19T13:36:25.900199600Z"
    }
   },
   "outputs": [],
   "source": [
    "# dropbox_address = \"/DATA/FLUORESCENCE/DATA_NileRed/\"\n",
    "# dropbox_address=  \"/DATA/MYRISTATE/DATA/\"\n",
    "# dropbox_address = \"/DATA/TransportROOT/\"\n",
    "dropbox_address = \"/DATA/CocoTransport/\"\n",
    "# dropbox_address = \"/DATA/TRANSPORT/DATA/20230721_Plate301/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T10:27:49.141794200Z",
     "start_time": "2023-07-18T10:25:27.963010800Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files downloaded! Merging files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 736/736 [00:04<00:00, 170.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imaging_day</th>\n",
       "      <th>storage_path</th>\n",
       "      <th>plate_id</th>\n",
       "      <th>root</th>\n",
       "      <th>strain</th>\n",
       "      <th>treatment</th>\n",
       "      <th>crossing_day</th>\n",
       "      <th>video_int</th>\n",
       "      <th>time_(s)</th>\n",
       "      <th>mode</th>\n",
       "      <th>...</th>\n",
       "      <th>ypos</th>\n",
       "      <th>zpos</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>folder</th>\n",
       "      <th>tot_path_drop</th>\n",
       "      <th>record_time</th>\n",
       "      <th>days_after_crossing</th>\n",
       "      <th>magnification</th>\n",
       "      <th>analysis_folder</th>\n",
       "      <th>videos_folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20230703</td>\n",
       "      <td>Dropbox\\DATA\\CocoTransport</td>\n",
       "      <td>20230703_Plate403</td>\n",
       "      <td>Carrot</td>\n",
       "      <td>C2</td>\n",
       "      <td>001P100N100C</td>\n",
       "      <td>20230626</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>BF</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320</td>\n",
       "      <td>5.993</td>\n",
       "      <td>20230703_Plate403_001</td>\n",
       "      <td>CocoTransport/20230703_Plate403/001/Img</td>\n",
       "      <td>DATA/CocoTransport/20230703_Plate403/001/Img</td>\n",
       "      <td>16:45:29</td>\n",
       "      <td>7</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20230714</td>\n",
       "      <td>Dropbox\\DATA\\CocoTransport</td>\n",
       "      <td>20230714_Plate302</td>\n",
       "      <td>Carrot</td>\n",
       "      <td>A5</td>\n",
       "      <td>001P100N100C</td>\n",
       "      <td>20230714</td>\n",
       "      <td>2</td>\n",
       "      <td>30.0</td>\n",
       "      <td>BF</td>\n",
       "      <td>...</td>\n",
       "      <td>17.800</td>\n",
       "      <td>5.567</td>\n",
       "      <td>20230714_Plate302_002</td>\n",
       "      <td>CocoTransport/20230714_Plate302/002/Img</td>\n",
       "      <td>DATA/CocoTransport/20230714_Plate302/002/Img</td>\n",
       "      <td>11:50:33</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20230714</td>\n",
       "      <td>Dropbox\\DATA\\CocoTransport</td>\n",
       "      <td>20230714_Plate302</td>\n",
       "      <td>Carrot</td>\n",
       "      <td>A5</td>\n",
       "      <td>001P100N100C</td>\n",
       "      <td>20230714</td>\n",
       "      <td>3</td>\n",
       "      <td>30.0</td>\n",
       "      <td>BF</td>\n",
       "      <td>...</td>\n",
       "      <td>17.207</td>\n",
       "      <td>5.568</td>\n",
       "      <td>20230714_Plate302_003</td>\n",
       "      <td>CocoTransport/20230714_Plate302/003/Img</td>\n",
       "      <td>DATA/CocoTransport/20230714_Plate302/003/Img</td>\n",
       "      <td>11:51:33</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20230714</td>\n",
       "      <td>Dropbox\\DATA\\CocoTransport</td>\n",
       "      <td>20230714_Plate302</td>\n",
       "      <td>Carrot</td>\n",
       "      <td>A5</td>\n",
       "      <td>001P100N100C</td>\n",
       "      <td>20230714</td>\n",
       "      <td>4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>BF</td>\n",
       "      <td>...</td>\n",
       "      <td>16.904</td>\n",
       "      <td>5.589</td>\n",
       "      <td>20230714_Plate302_004</td>\n",
       "      <td>CocoTransport/20230714_Plate302/004/Img</td>\n",
       "      <td>DATA/CocoTransport/20230714_Plate302/004/Img</td>\n",
       "      <td>11:52:38</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20230714</td>\n",
       "      <td>Dropbox\\DATA\\CocoTransport</td>\n",
       "      <td>20230714_Plate302</td>\n",
       "      <td>Carrot</td>\n",
       "      <td>A5</td>\n",
       "      <td>001P100N100C</td>\n",
       "      <td>20230714</td>\n",
       "      <td>5</td>\n",
       "      <td>30.0</td>\n",
       "      <td>BF</td>\n",
       "      <td>...</td>\n",
       "      <td>16.433</td>\n",
       "      <td>5.621</td>\n",
       "      <td>20230714_Plate302_005</td>\n",
       "      <td>CocoTransport/20230714_Plate302/005/Img</td>\n",
       "      <td>DATA/CocoTransport/20230714_Plate302/005/Img</td>\n",
       "      <td>11:53:37</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  imaging_day                storage_path           plate_id    root strain  \\\n",
       "0    20230703  Dropbox\\DATA\\CocoTransport  20230703_Plate403  Carrot     C2   \n",
       "1    20230714  Dropbox\\DATA\\CocoTransport  20230714_Plate302  Carrot     A5   \n",
       "2    20230714  Dropbox\\DATA\\CocoTransport  20230714_Plate302  Carrot     A5   \n",
       "3    20230714  Dropbox\\DATA\\CocoTransport  20230714_Plate302  Carrot     A5   \n",
       "4    20230714  Dropbox\\DATA\\CocoTransport  20230714_Plate302  Carrot     A5   \n",
       "\n",
       "      treatment crossing_day  video_int  time_(s) mode  ...    ypos   zpos  \\\n",
       "0  001P100N100C     20230626          1      30.0   BF  ...   0.320  5.993   \n",
       "1  001P100N100C     20230714          2      30.0   BF  ...  17.800  5.567   \n",
       "2  001P100N100C     20230714          3      30.0   BF  ...  17.207  5.568   \n",
       "3  001P100N100C     20230714          4      30.0   BF  ...  16.904  5.589   \n",
       "4  001P100N100C     20230714          5      30.0   BF  ...  16.433  5.621   \n",
       "\n",
       "               unique_id                                   folder  \\\n",
       "0  20230703_Plate403_001  CocoTransport/20230703_Plate403/001/Img   \n",
       "1  20230714_Plate302_002  CocoTransport/20230714_Plate302/002/Img   \n",
       "2  20230714_Plate302_003  CocoTransport/20230714_Plate302/003/Img   \n",
       "3  20230714_Plate302_004  CocoTransport/20230714_Plate302/004/Img   \n",
       "4  20230714_Plate302_005  CocoTransport/20230714_Plate302/005/Img   \n",
       "\n",
       "                                  tot_path_drop record_time  \\\n",
       "0  DATA/CocoTransport/20230703_Plate403/001/Img    16:45:29   \n",
       "1  DATA/CocoTransport/20230714_Plate302/002/Img    11:50:33   \n",
       "2  DATA/CocoTransport/20230714_Plate302/003/Img    11:51:33   \n",
       "3  DATA/CocoTransport/20230714_Plate302/004/Img    11:52:38   \n",
       "4  DATA/CocoTransport/20230714_Plate302/005/Img    11:53:37   \n",
       "\n",
       "   days_after_crossing  magnification  analysis_folder videos_folder  \n",
       "0                    7           50.0              NaN           NaN  \n",
       "1                    0           50.0              NaN           NaN  \n",
       "2                    0           50.0              NaN           NaN  \n",
       "3                    0           50.0              NaN           NaN  \n",
       "4                    0           50.0              NaN           NaN  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_param_frame = index_videos_dropbox(analysis_folder, videos_folder, dropbox_address, REDO_SCROUNGING=False)\n",
    "video_param_frame.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go?\n",
    "If you want to download videos:\n",
    "Use MODULE 1\n",
    "\n",
    "If you want to analyze already downloaded videos:\n",
    "Skip MODULE 1, use MODULE 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODULE 1: Downloading\n",
    "This section, there is one block of code that will ask you one last time whether all of the parameters are correct. The block of code after that will initiate Snellius jobs to download the videos in the DataFrame from the dropbox. Downloading videos is not that costly, but of course we prefer it to be done as efficiently as possible.\n",
    "## I'm not on Snellius! How do i download stuff??\n",
    "Easy. Just skip the second block of code. The one below will just use the dropbox API to properly download all your raw data.\n",
    "WARNING: This process can be quite long if you are queueing up a lot of videos. Do not use that block of code on Snellius, they will get mad at you (and prematurely stop your running program), just use the SLURM job in that case.\n",
    "\n",
    "### Input:\n",
    "Nothing\n",
    "### Output:\n",
    "Print statement with the DataFrame and the folders where everything will be stored.\n",
    "Subsequent block of code will download raw video files to the videos folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T10:29:17.899989900Z",
     "start_time": "2023-07-18T10:29:15.993949700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos that will be downloaded: 120\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imaging_day</th>\n",
       "      <th>storage_path</th>\n",
       "      <th>plate_id</th>\n",
       "      <th>root</th>\n",
       "      <th>strain</th>\n",
       "      <th>treatment</th>\n",
       "      <th>crossing_day</th>\n",
       "      <th>video_int</th>\n",
       "      <th>time_(s)</th>\n",
       "      <th>mode</th>\n",
       "      <th>...</th>\n",
       "      <th>ypos</th>\n",
       "      <th>zpos</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>folder</th>\n",
       "      <th>tot_path_drop</th>\n",
       "      <th>record_time</th>\n",
       "      <th>days_after_crossing</th>\n",
       "      <th>magnification</th>\n",
       "      <th>analysis_folder</th>\n",
       "      <th>videos_folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>20230729</td>\n",
       "      <td>Dropbox\\DATA\\CocoTransport</td>\n",
       "      <td>20230729_Plate440</td>\n",
       "      <td>Carrot</td>\n",
       "      <td>C2</td>\n",
       "      <td>001P100N100C</td>\n",
       "      <td>20230723</td>\n",
       "      <td>126</td>\n",
       "      <td>10.0</td>\n",
       "      <td>BF</td>\n",
       "      <td>...</td>\n",
       "      <td>2.904</td>\n",
       "      <td>12.028</td>\n",
       "      <td>20230729_Plate440_126</td>\n",
       "      <td>CocoTransport/20230729_Plate440/126/Img</td>\n",
       "      <td>DATA/CocoTransport/20230729_Plate440/126/Img</td>\n",
       "      <td>17:52:32</td>\n",
       "      <td>6</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>20230729</td>\n",
       "      <td>Dropbox\\DATA\\CocoTransport</td>\n",
       "      <td>20230729_Plate440</td>\n",
       "      <td>Carrot</td>\n",
       "      <td>C2</td>\n",
       "      <td>001P100N100C</td>\n",
       "      <td>20230723</td>\n",
       "      <td>127</td>\n",
       "      <td>10.0</td>\n",
       "      <td>BF</td>\n",
       "      <td>...</td>\n",
       "      <td>2.277</td>\n",
       "      <td>12.058</td>\n",
       "      <td>20230729_Plate440_127</td>\n",
       "      <td>CocoTransport/20230729_Plate440/127/Img</td>\n",
       "      <td>DATA/CocoTransport/20230729_Plate440/127/Img</td>\n",
       "      <td>17:53:07</td>\n",
       "      <td>6</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>20230729</td>\n",
       "      <td>Dropbox\\DATA\\CocoTransport</td>\n",
       "      <td>20230729_Plate440</td>\n",
       "      <td>Carrot</td>\n",
       "      <td>C2</td>\n",
       "      <td>001P100N100C</td>\n",
       "      <td>20230723</td>\n",
       "      <td>128</td>\n",
       "      <td>10.0</td>\n",
       "      <td>BF</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.373</td>\n",
       "      <td>12.028</td>\n",
       "      <td>20230729_Plate440_128</td>\n",
       "      <td>CocoTransport/20230729_Plate440/128/Img</td>\n",
       "      <td>DATA/CocoTransport/20230729_Plate440/128/Img</td>\n",
       "      <td>17:54:51</td>\n",
       "      <td>6</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>20230729</td>\n",
       "      <td>Dropbox\\DATA\\CocoTransport</td>\n",
       "      <td>20230729_Plate440</td>\n",
       "      <td>Carrot</td>\n",
       "      <td>C2</td>\n",
       "      <td>001P100N100C</td>\n",
       "      <td>20230723</td>\n",
       "      <td>129</td>\n",
       "      <td>10.0</td>\n",
       "      <td>BF</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.499</td>\n",
       "      <td>12.028</td>\n",
       "      <td>20230729_Plate440_129</td>\n",
       "      <td>CocoTransport/20230729_Plate440/129/Img</td>\n",
       "      <td>DATA/CocoTransport/20230729_Plate440/129/Img</td>\n",
       "      <td>17:55:07</td>\n",
       "      <td>6</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>20230729</td>\n",
       "      <td>Dropbox\\DATA\\CocoTransport</td>\n",
       "      <td>20230729_Plate440</td>\n",
       "      <td>Carrot</td>\n",
       "      <td>C2</td>\n",
       "      <td>001P100N100C</td>\n",
       "      <td>20230723</td>\n",
       "      <td>130</td>\n",
       "      <td>10.0</td>\n",
       "      <td>BF</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.584</td>\n",
       "      <td>12.020</td>\n",
       "      <td>20230729_Plate440_130</td>\n",
       "      <td>CocoTransport/20230729_Plate440/130/Img</td>\n",
       "      <td>DATA/CocoTransport/20230729_Plate440/130/Img</td>\n",
       "      <td>17:55:31</td>\n",
       "      <td>6</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    imaging_day                storage_path           plate_id    root strain  \\\n",
       "616    20230729  Dropbox\\DATA\\CocoTransport  20230729_Plate440  Carrot     C2   \n",
       "617    20230729  Dropbox\\DATA\\CocoTransport  20230729_Plate440  Carrot     C2   \n",
       "618    20230729  Dropbox\\DATA\\CocoTransport  20230729_Plate440  Carrot     C2   \n",
       "619    20230729  Dropbox\\DATA\\CocoTransport  20230729_Plate440  Carrot     C2   \n",
       "620    20230729  Dropbox\\DATA\\CocoTransport  20230729_Plate440  Carrot     C2   \n",
       "\n",
       "        treatment crossing_day  video_int  time_(s) mode  ...   ypos    zpos  \\\n",
       "616  001P100N100C     20230723        126      10.0   BF  ...  2.904  12.028   \n",
       "617  001P100N100C     20230723        127      10.0   BF  ...  2.277  12.058   \n",
       "618  001P100N100C     20230723        128      10.0   BF  ... -3.373  12.028   \n",
       "619  001P100N100C     20230723        129      10.0   BF  ... -3.499  12.028   \n",
       "620  001P100N100C     20230723        130      10.0   BF  ... -3.584  12.020   \n",
       "\n",
       "                 unique_id                                   folder  \\\n",
       "616  20230729_Plate440_126  CocoTransport/20230729_Plate440/126/Img   \n",
       "617  20230729_Plate440_127  CocoTransport/20230729_Plate440/127/Img   \n",
       "618  20230729_Plate440_128  CocoTransport/20230729_Plate440/128/Img   \n",
       "619  20230729_Plate440_129  CocoTransport/20230729_Plate440/129/Img   \n",
       "620  20230729_Plate440_130  CocoTransport/20230729_Plate440/130/Img   \n",
       "\n",
       "                                    tot_path_drop record_time  \\\n",
       "616  DATA/CocoTransport/20230729_Plate440/126/Img    17:52:32   \n",
       "617  DATA/CocoTransport/20230729_Plate440/127/Img    17:53:07   \n",
       "618  DATA/CocoTransport/20230729_Plate440/128/Img    17:54:51   \n",
       "619  DATA/CocoTransport/20230729_Plate440/129/Img    17:55:07   \n",
       "620  DATA/CocoTransport/20230729_Plate440/130/Img    17:55:31   \n",
       "\n",
       "     days_after_crossing  magnification  analysis_folder videos_folder  \n",
       "616                    6           50.0              NaN           NaN  \n",
       "617                    6           50.0              NaN           NaN  \n",
       "618                    6           50.0              NaN           NaN  \n",
       "619                    6           50.0              NaN           NaN  \n",
       "620                    6           50.0              NaN           NaN  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################################################\n",
    "### This is where you can apply the filters. Only those videos will be downloaded ###\n",
    "#####################################################################################\n",
    "\n",
    "download_frame = video_param_frame[video_param_frame['imaging_day'].ge(\"20230729\")].iloc[125:]\n",
    "\n",
    "#####################################################################################\n",
    "### Below code will prepare for those videos to be downloaded into videos_folder  ###\n",
    "#####################################################################################\n",
    "print(f\"Number of videos that will be downloaded: {len(download_frame)}\")\n",
    "download_frame.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent all the jobs! Use the command '$ squeue' in the terminal to see the progress\n"
     ]
    }
   ],
   "source": [
    "run_parallel_transfer(\n",
    "    \"from_drop_video.py\",\n",
    "    [videos_folder],\n",
    "    download_frame,\n",
    "    1,\n",
    "    \"10:00:00\",\n",
    "    \"transfer_test\"\n",
    ")\n",
    "clear_output(wait=False)\n",
    "\n",
    "print(\"Sent all the jobs! Use the command '$ squeue' in the terminal to see the progress\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download videos from Dropbox (Not a SLURM job)\n",
    "This block of code can be used to download videos individually from dropbox. \n",
    "Be aware:\n",
    "- This is significantly slower than launching a SLURM job\n",
    "- This downloads videos sequentially, not in parallel\n",
    "- If this function is running for too long on Snellius, it might get you booted from the interactive node\n",
    "- Videos are large. Make sure you have the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_video_folders_drop(download_frame, videos_folder)\n",
    "clear_output(wait=False)\n",
    "print(\"All videos downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Analysis folders from Dropbox (not a SLURM job)\n",
    "Similar warnings apply as the video download function above. The file sizes for the analysis folders are, however, vastly smaller than video files. This allows for a bit more wiggle room."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_analysis_folders_drop(analysis_folder, dropbox_address)\n",
    "clear_output(wait=False)\n",
    "print(\"All analysis folders downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Analysis\n",
    "Now that the files have been downloaded, it's time to analyse them. In the below code, you'll be able to either do a complete survey of the analysis folder for as many videos as possible, or use the DataFrame of recently downloaded videos to filter for the videos you want to analyse.\n",
    "\n",
    "Also possible to analyse videos directly in this notebook. Be aware again that this is a sequential, and slower analysis than running a SLURM job. \n",
    "\n",
    "### Input:\n",
    "DataFrame filters of all videos to be analysed\n",
    "### Output:\n",
    "Print statements for all parameters of the analysis session that is about to take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T07:25:04.975950900Z",
     "start_time": "2023-07-19T07:25:01.913828400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/DATA/CocoTransport/\n"
     ]
    }
   ],
   "source": [
    "print(dropbox_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T07:25:23.648201300Z",
     "start_time": "2023-07-19T07:25:11.660905800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imaging_day</th>\n",
       "      <th>storage_path</th>\n",
       "      <th>plate_id</th>\n",
       "      <th>root</th>\n",
       "      <th>strain</th>\n",
       "      <th>treatment</th>\n",
       "      <th>crossing_day</th>\n",
       "      <th>video_int</th>\n",
       "      <th>time_(s)</th>\n",
       "      <th>mode</th>\n",
       "      <th>...</th>\n",
       "      <th>ypos</th>\n",
       "      <th>zpos</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>folder</th>\n",
       "      <th>tot_path_drop</th>\n",
       "      <th>record_time</th>\n",
       "      <th>days_after_crossing</th>\n",
       "      <th>magnification</th>\n",
       "      <th>analysis_folder</th>\n",
       "      <th>videos_folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20230703</td>\n",
       "      <td>Dropbox\\DATA\\CocoTransport</td>\n",
       "      <td>20230703_Plate403</td>\n",
       "      <td>Carrot</td>\n",
       "      <td>C2</td>\n",
       "      <td>001P100N100C</td>\n",
       "      <td>20230626</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>BF</td>\n",
       "      <td>...</td>\n",
       "      <td>0.32</td>\n",
       "      <td>5.993</td>\n",
       "      <td>20230703_Plate403_001</td>\n",
       "      <td>CocoTransport/20230703_Plate403/001/Img</td>\n",
       "      <td>DATA/CocoTransport/20230703_Plate403/001/Img</td>\n",
       "      <td>16:45:29</td>\n",
       "      <td>7</td>\n",
       "      <td>50.0</td>\n",
       "      <td>/gpfs/home6/svstaalduine/Analysis/CocoTranspor...</td>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/videos/Coco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20230714</td>\n",
       "      <td>Dropbox\\DATA\\CocoTransport</td>\n",
       "      <td>20230714_Plate302</td>\n",
       "      <td>Carrot</td>\n",
       "      <td>A5</td>\n",
       "      <td>001P100N100C</td>\n",
       "      <td>20230714</td>\n",
       "      <td>2</td>\n",
       "      <td>30.0</td>\n",
       "      <td>BF</td>\n",
       "      <td>...</td>\n",
       "      <td>17.8</td>\n",
       "      <td>5.567</td>\n",
       "      <td>20230714_Plate302_002</td>\n",
       "      <td>CocoTransport/20230714_Plate302/002/Img</td>\n",
       "      <td>DATA/CocoTransport/20230714_Plate302/002/Img</td>\n",
       "      <td>11:50:33</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>/gpfs/home6/svstaalduine/Analysis/CocoTranspor...</td>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/videos/Coco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20230714</td>\n",
       "      <td>Dropbox\\DATA\\CocoTransport</td>\n",
       "      <td>20230714_Plate302</td>\n",
       "      <td>Carrot</td>\n",
       "      <td>A5</td>\n",
       "      <td>001P100N100C</td>\n",
       "      <td>20230714</td>\n",
       "      <td>3</td>\n",
       "      <td>30.0</td>\n",
       "      <td>BF</td>\n",
       "      <td>...</td>\n",
       "      <td>17.207</td>\n",
       "      <td>5.568</td>\n",
       "      <td>20230714_Plate302_003</td>\n",
       "      <td>CocoTransport/20230714_Plate302/003/Img</td>\n",
       "      <td>DATA/CocoTransport/20230714_Plate302/003/Img</td>\n",
       "      <td>11:51:33</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>/gpfs/home6/svstaalduine/Analysis/CocoTranspor...</td>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/videos/Coco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20230714</td>\n",
       "      <td>Dropbox\\DATA\\CocoTransport</td>\n",
       "      <td>20230714_Plate302</td>\n",
       "      <td>Carrot</td>\n",
       "      <td>A5</td>\n",
       "      <td>001P100N100C</td>\n",
       "      <td>20230714</td>\n",
       "      <td>4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>BF</td>\n",
       "      <td>...</td>\n",
       "      <td>16.904</td>\n",
       "      <td>5.589</td>\n",
       "      <td>20230714_Plate302_004</td>\n",
       "      <td>CocoTransport/20230714_Plate302/004/Img</td>\n",
       "      <td>DATA/CocoTransport/20230714_Plate302/004/Img</td>\n",
       "      <td>11:52:38</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>/gpfs/home6/svstaalduine/Analysis/CocoTranspor...</td>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/videos/Coco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20230714</td>\n",
       "      <td>Dropbox\\DATA\\CocoTransport</td>\n",
       "      <td>20230714_Plate302</td>\n",
       "      <td>Carrot</td>\n",
       "      <td>A5</td>\n",
       "      <td>001P100N100C</td>\n",
       "      <td>20230714</td>\n",
       "      <td>5</td>\n",
       "      <td>30.0</td>\n",
       "      <td>BF</td>\n",
       "      <td>...</td>\n",
       "      <td>16.433</td>\n",
       "      <td>5.621</td>\n",
       "      <td>20230714_Plate302_005</td>\n",
       "      <td>CocoTransport/20230714_Plate302/005/Img</td>\n",
       "      <td>DATA/CocoTransport/20230714_Plate302/005/Img</td>\n",
       "      <td>11:53:37</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>/gpfs/home6/svstaalduine/Analysis/CocoTranspor...</td>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/videos/Coco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  imaging_day                storage_path           plate_id    root strain  \\\n",
       "0    20230703  Dropbox\\DATA\\CocoTransport  20230703_Plate403  Carrot     C2   \n",
       "1    20230714  Dropbox\\DATA\\CocoTransport  20230714_Plate302  Carrot     A5   \n",
       "2    20230714  Dropbox\\DATA\\CocoTransport  20230714_Plate302  Carrot     A5   \n",
       "3    20230714  Dropbox\\DATA\\CocoTransport  20230714_Plate302  Carrot     A5   \n",
       "4    20230714  Dropbox\\DATA\\CocoTransport  20230714_Plate302  Carrot     A5   \n",
       "\n",
       "      treatment crossing_day video_int time_(s) mode  ...    ypos   zpos  \\\n",
       "0  001P100N100C     20230626         1     30.0   BF  ...    0.32  5.993   \n",
       "1  001P100N100C     20230714         2     30.0   BF  ...    17.8  5.567   \n",
       "2  001P100N100C     20230714         3     30.0   BF  ...  17.207  5.568   \n",
       "3  001P100N100C     20230714         4     30.0   BF  ...  16.904  5.589   \n",
       "4  001P100N100C     20230714         5     30.0   BF  ...  16.433  5.621   \n",
       "\n",
       "               unique_id                                   folder  \\\n",
       "0  20230703_Plate403_001  CocoTransport/20230703_Plate403/001/Img   \n",
       "1  20230714_Plate302_002  CocoTransport/20230714_Plate302/002/Img   \n",
       "2  20230714_Plate302_003  CocoTransport/20230714_Plate302/003/Img   \n",
       "3  20230714_Plate302_004  CocoTransport/20230714_Plate302/004/Img   \n",
       "4  20230714_Plate302_005  CocoTransport/20230714_Plate302/005/Img   \n",
       "\n",
       "                                  tot_path_drop record_time  \\\n",
       "0  DATA/CocoTransport/20230703_Plate403/001/Img    16:45:29   \n",
       "1  DATA/CocoTransport/20230714_Plate302/002/Img    11:50:33   \n",
       "2  DATA/CocoTransport/20230714_Plate302/003/Img    11:51:33   \n",
       "3  DATA/CocoTransport/20230714_Plate302/004/Img    11:52:38   \n",
       "4  DATA/CocoTransport/20230714_Plate302/005/Img    11:53:37   \n",
       "\n",
       "  days_after_crossing magnification  \\\n",
       "0                   7          50.0   \n",
       "1                   0          50.0   \n",
       "2                   0          50.0   \n",
       "3                   0          50.0   \n",
       "4                   0          50.0   \n",
       "\n",
       "                                     analysis_folder  \\\n",
       "0  /gpfs/home6/svstaalduine/Analysis/CocoTranspor...   \n",
       "1  /gpfs/home6/svstaalduine/Analysis/CocoTranspor...   \n",
       "2  /gpfs/home6/svstaalduine/Analysis/CocoTranspor...   \n",
       "3  /gpfs/home6/svstaalduine/Analysis/CocoTranspor...   \n",
       "4  /gpfs/home6/svstaalduine/Analysis/CocoTranspor...   \n",
       "\n",
       "                                       videos_folder  \n",
       "0  /gpfs/scratch1/shared/amftrackflow/videos/Coco...  \n",
       "1  /gpfs/scratch1/shared/amftrackflow/videos/Coco...  \n",
       "2  /gpfs/scratch1/shared/amftrackflow/videos/Coco...  \n",
       "3  /gpfs/scratch1/shared/amftrackflow/videos/Coco...  \n",
       "4  /gpfs/scratch1/shared/amftrackflow/videos/Coco...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_filter = dropbox_address[5:]\n",
    "\n",
    "img_infos = glob.glob(f\"{analysis_folder}{folder_filter}/**/video_data.json\", recursive=True)\n",
    "vid_anls_frame = pd.DataFrame()\n",
    "for address in img_infos:\n",
    "    add_info = pd.read_json(address, orient='index').T\n",
    "    vid_anls_frame = pd.concat([vid_anls_frame, add_info], ignore_index=True)\n",
    "\n",
    "vid_anls_frame = vid_anls_frame.sort_values('unique_id').reset_index(drop=True)\n",
    "vid_anls_frame.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/home6/svstaalduine/Analysis/CocoTransport/20230703_Plate403/001\n"
     ]
    }
   ],
   "source": [
    "print(vid_anls_frame['analysis_folder'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T11:04:27.934555600Z",
     "start_time": "2023-07-19T11:03:55.410127600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos to be analyzed: 245\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imaging_day</th>\n",
       "      <th>storage_path</th>\n",
       "      <th>plate_id</th>\n",
       "      <th>root</th>\n",
       "      <th>strain</th>\n",
       "      <th>treatment</th>\n",
       "      <th>crossing_day</th>\n",
       "      <th>video_int</th>\n",
       "      <th>time_(s)</th>\n",
       "      <th>mode</th>\n",
       "      <th>...</th>\n",
       "      <th>ypos</th>\n",
       "      <th>zpos</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>folder</th>\n",
       "      <th>tot_path_drop</th>\n",
       "      <th>record_time</th>\n",
       "      <th>days_after_crossing</th>\n",
       "      <th>magnification</th>\n",
       "      <th>analysis_folder</th>\n",
       "      <th>videos_folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>20230729</td>\n",
       "      <td>Dropbox\\DATA\\CocoTransport</td>\n",
       "      <td>20230729_Plate440</td>\n",
       "      <td>Carrot</td>\n",
       "      <td>C2</td>\n",
       "      <td>001P100N100C</td>\n",
       "      <td>20230723</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>BF</td>\n",
       "      <td>...</td>\n",
       "      <td>1.161</td>\n",
       "      <td>11.893</td>\n",
       "      <td>20230729_Plate440_001</td>\n",
       "      <td>CocoTransport/20230729_Plate440/001/Img</td>\n",
       "      <td>DATA/CocoTransport/20230729_Plate440/001/Img</td>\n",
       "      <td>16:49:37</td>\n",
       "      <td>6</td>\n",
       "      <td>50.0</td>\n",
       "      <td>/gpfs/home6/svstaalduine/Analysis/CocoTranspor...</td>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/videos/Coco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>20230729</td>\n",
       "      <td>Dropbox\\DATA\\CocoTransport</td>\n",
       "      <td>20230729_Plate440</td>\n",
       "      <td>Carrot</td>\n",
       "      <td>C2</td>\n",
       "      <td>001P100N100C</td>\n",
       "      <td>20230723</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>BF</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405</td>\n",
       "      <td>11.912</td>\n",
       "      <td>20230729_Plate440_002</td>\n",
       "      <td>CocoTransport/20230729_Plate440/002/Img</td>\n",
       "      <td>DATA/CocoTransport/20230729_Plate440/002/Img</td>\n",
       "      <td>16:50:04</td>\n",
       "      <td>6</td>\n",
       "      <td>50.0</td>\n",
       "      <td>/gpfs/home6/svstaalduine/Analysis/CocoTranspor...</td>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/videos/Coco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>20230729</td>\n",
       "      <td>Dropbox\\DATA\\CocoTransport</td>\n",
       "      <td>20230729_Plate440</td>\n",
       "      <td>Carrot</td>\n",
       "      <td>C2</td>\n",
       "      <td>001P100N100C</td>\n",
       "      <td>20230723</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>BF</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>11.935</td>\n",
       "      <td>20230729_Plate440_003</td>\n",
       "      <td>CocoTransport/20230729_Plate440/003/Img</td>\n",
       "      <td>DATA/CocoTransport/20230729_Plate440/003/Img</td>\n",
       "      <td>16:50:41</td>\n",
       "      <td>6</td>\n",
       "      <td>50.0</td>\n",
       "      <td>/gpfs/home6/svstaalduine/Analysis/CocoTranspor...</td>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/videos/Coco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>20230729</td>\n",
       "      <td>Dropbox\\DATA\\CocoTransport</td>\n",
       "      <td>20230729_Plate440</td>\n",
       "      <td>Carrot</td>\n",
       "      <td>C2</td>\n",
       "      <td>001P100N100C</td>\n",
       "      <td>20230723</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>BF</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>11.941</td>\n",
       "      <td>20230729_Plate440_004</td>\n",
       "      <td>CocoTransport/20230729_Plate440/004/Img</td>\n",
       "      <td>DATA/CocoTransport/20230729_Plate440/004/Img</td>\n",
       "      <td>16:51:05</td>\n",
       "      <td>6</td>\n",
       "      <td>50.0</td>\n",
       "      <td>/gpfs/home6/svstaalduine/Analysis/CocoTranspor...</td>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/videos/Coco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>20230729</td>\n",
       "      <td>Dropbox\\DATA\\CocoTransport</td>\n",
       "      <td>20230729_Plate440</td>\n",
       "      <td>Carrot</td>\n",
       "      <td>C2</td>\n",
       "      <td>001P100N100C</td>\n",
       "      <td>20230723</td>\n",
       "      <td>5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>BF</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>11.985</td>\n",
       "      <td>20230729_Plate440_005</td>\n",
       "      <td>CocoTransport/20230729_Plate440/005/Img</td>\n",
       "      <td>DATA/CocoTransport/20230729_Plate440/005/Img</td>\n",
       "      <td>16:51:37</td>\n",
       "      <td>6</td>\n",
       "      <td>50.0</td>\n",
       "      <td>/gpfs/home6/svstaalduine/Analysis/CocoTranspor...</td>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/videos/Coco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    imaging_day                storage_path           plate_id    root strain  \\\n",
       "491    20230729  Dropbox\\DATA\\CocoTransport  20230729_Plate440  Carrot     C2   \n",
       "492    20230729  Dropbox\\DATA\\CocoTransport  20230729_Plate440  Carrot     C2   \n",
       "493    20230729  Dropbox\\DATA\\CocoTransport  20230729_Plate440  Carrot     C2   \n",
       "494    20230729  Dropbox\\DATA\\CocoTransport  20230729_Plate440  Carrot     C2   \n",
       "495    20230729  Dropbox\\DATA\\CocoTransport  20230729_Plate440  Carrot     C2   \n",
       "\n",
       "        treatment crossing_day video_int time_(s) mode  ...   ypos    zpos  \\\n",
       "491  001P100N100C     20230723         1     10.0   BF  ...  1.161  11.893   \n",
       "492  001P100N100C     20230723         2     10.0   BF  ...  0.405  11.912   \n",
       "493  001P100N100C     20230723         3     10.0   BF  ... -0.269  11.935   \n",
       "494  001P100N100C     20230723         4     10.0   BF  ... -0.435  11.941   \n",
       "495  001P100N100C     20230723         5     10.0   BF  ...  -0.77  11.985   \n",
       "\n",
       "                 unique_id                                   folder  \\\n",
       "491  20230729_Plate440_001  CocoTransport/20230729_Plate440/001/Img   \n",
       "492  20230729_Plate440_002  CocoTransport/20230729_Plate440/002/Img   \n",
       "493  20230729_Plate440_003  CocoTransport/20230729_Plate440/003/Img   \n",
       "494  20230729_Plate440_004  CocoTransport/20230729_Plate440/004/Img   \n",
       "495  20230729_Plate440_005  CocoTransport/20230729_Plate440/005/Img   \n",
       "\n",
       "                                    tot_path_drop record_time  \\\n",
       "491  DATA/CocoTransport/20230729_Plate440/001/Img    16:49:37   \n",
       "492  DATA/CocoTransport/20230729_Plate440/002/Img    16:50:04   \n",
       "493  DATA/CocoTransport/20230729_Plate440/003/Img    16:50:41   \n",
       "494  DATA/CocoTransport/20230729_Plate440/004/Img    16:51:05   \n",
       "495  DATA/CocoTransport/20230729_Plate440/005/Img    16:51:37   \n",
       "\n",
       "    days_after_crossing magnification  \\\n",
       "491                   6          50.0   \n",
       "492                   6          50.0   \n",
       "493                   6          50.0   \n",
       "494                   6          50.0   \n",
       "495                   6          50.0   \n",
       "\n",
       "                                       analysis_folder  \\\n",
       "491  /gpfs/home6/svstaalduine/Analysis/CocoTranspor...   \n",
       "492  /gpfs/home6/svstaalduine/Analysis/CocoTranspor...   \n",
       "493  /gpfs/home6/svstaalduine/Analysis/CocoTranspor...   \n",
       "494  /gpfs/home6/svstaalduine/Analysis/CocoTranspor...   \n",
       "495  /gpfs/home6/svstaalduine/Analysis/CocoTranspor...   \n",
       "\n",
       "                                         videos_folder  \n",
       "491  /gpfs/scratch1/shared/amftrackflow/videos/Coco...  \n",
       "492  /gpfs/scratch1/shared/amftrackflow/videos/Coco...  \n",
       "493  /gpfs/scratch1/shared/amftrackflow/videos/Coco...  \n",
       "494  /gpfs/scratch1/shared/amftrackflow/videos/Coco...  \n",
       "495  /gpfs/scratch1/shared/amftrackflow/videos/Coco...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################################################################\n",
    "### This is where you can apply the filters. Only those videos will be analyzed. ###\n",
    "####################################################################################\n",
    "\n",
    "analysis_frame = vid_anls_frame[vid_anls_frame['plate_id']==\"20230729_Plate440\"]\n",
    "\n",
    "####################################################################################\n",
    "### Below code will prepare for those videos to be downloaded to videos_folder.  ###\n",
    "####################################################################################\n",
    "\n",
    "print(f\"Number of videos to be analyzed: {len(analysis_frame)}\")\n",
    "analysis_frame.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SLURM Analysis job\n",
    "Two options: For small analysis, use the first block. This will just do the calculations on the machine. For large-scale analysis, use the second block, as it will create a Snellius job.\n",
    "## Input:\n",
    "Snellius job parameters\n",
    "## Output:\n",
    "Analysis folder will be populated with analysis tiffs and csv sheets. At the same time, this analysis folder will also be uploaded to the dropbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent all the jobs! Use the command '$ squeue' in the terminal to see the progress\n"
     ]
    }
   ],
   "source": [
    "### LARGE VIDEO ANALYSIS\n",
    "\n",
    "nr_parallel = np.min([len(analysis_frame.index), 4])\n",
    "\n",
    "run_parallel_flows(\n",
    "    \"flux_extract.py\",\n",
    "    [analysis_folder, 9, 0.95, 0.005, 80, dropbox_address],\n",
    "    analysis_frame,\n",
    "    nr_parallel,\n",
    "    \"2:00:00\",\n",
    "    \"flux_extract\"\n",
    ")\n",
    "clear_output(wait=False)\n",
    "\n",
    "print(\"Sent all the jobs! Use the command '$ squeue' in the terminal to see the progress\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run local analysis\n",
    "First the analysis function is defined, which you can change to fit the parameters you want. Then the next block of code will use that function to go through each row in the video analyis dataframe and executes the analysis. NOTE: This is not code to go through the analysis, that is for MODULE 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T11:16:55.594396400Z",
     "start_time": "2023-07-19T11:05:06.911792Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "edges_objs = analysis_run(analysis_frame, analysis_folder, videos_folder, dropbox_address,\n",
    "             logging=True,                 # Print progress to console\n",
    "             kymo_normalize=True,                 \n",
    "             kymo_section_width=2.6,       # Width of kymograph lines, adjusted for magnification\n",
    "             edge_len_min=20,\n",
    "             save_edge_extraction_plot=True,\n",
    "             make_video=False,             # Make mp4 of raw data TIFFs\n",
    "             create_snapshot=True,         # Save image of edge\n",
    "             create_edge_video=False,      # Save video of edge\n",
    "             photobleach_adjust=False,     # Adjust kymograph for photobleaching\n",
    "             speed_ext_window_number=9,    # Size range to investigate speeds\n",
    "             speed_ext_window_start = 3,   # Start size of window for GST\n",
    "             speed_ext_c_thresh=0.95,      # Confidence threshold for speed determination\n",
    "             speed_ext_c_falloff = 0.005,  # Confidence falloff as window size increases\n",
    "             speed_ext_blur_size = 3,      # Kymograph blur Gaussian kernel size\n",
    "             speed_ext_blur=True,          # Whether to preblur at all\n",
    "             speed_ext_max_thresh = 80,    # Maximum expected speeds (in um/s)\n",
    "             dropbox_upload=False          # Whether to upload results to dropbox\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "static_kymo = edges_objs[0][0].filtered_left[0] + edges_objs[0][0].filtered_right[0]\n",
    "kymo_new = np.full((static_kymo.shape), np.nan)\n",
    "spd_adj = 10\n",
    "offset = 200\n",
    "print(kymo_new.shape)\n",
    "for i, line in enumerate(static_kymo):\n",
    "#     print(-offset+spd_adj*(i-offset))\n",
    "    line_piece = line[-offset+spd_adj*(i-offset):len(line)-offset+spd_adj*(i-offset)]\n",
    "    kymo_new[i][0:len(line_piece)] = line_piece\n",
    "\n",
    "fig, ax = plt.subplots(3, figsize=(5, 15))\n",
    "ax[0].imshow(static_kymo)\n",
    "ax[1].imshow(kymo_new)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Width profile Kymograph analysis\n",
    "This is going to be some special code to extract multiple kymographs from the same edge, all next to each other. Requires running the previous code to get the analysis objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T13:19:01.028742400Z",
     "start_time": "2023-07-19T13:07:23.507976500Z"
    }
   },
   "outputs": [],
   "source": [
    "print([[edge.edge_name for edge in edge_list] for edge_list in edges_objs])\n",
    "edge_interest = edges_objs[0][2]\n",
    "\n",
    "width_len = 8\n",
    "#TODO: Get effective mean speed calculation in here too\n",
    "\n",
    "kymos = edge_interest.extract_multi_kymo(width_len, target_length=90)\n",
    "fourier_kymos = edge_interest.fourier_kymo()\n",
    "speeds, times = edge_interest.extract_speeds(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T13:19:05.489152500Z",
     "start_time": "2023-07-19T13:19:01.263864500Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.array(times).shape)\n",
    "speed_max = 20\n",
    "fig, ax = plt.subplots(width_len)\n",
    "for i in range(width_len):\n",
    "    ax[i].plot(times[i], np.nanmean(speeds[i][0], axis=1))\n",
    "    ax[i].plot(times[i], np.nanmean(speeds[i][1], axis=1))\n",
    "    ax[i].fill_between(times[i], np.nanmean(speeds[i][0], axis=1) + np.nanstd(speeds[i][0], axis=1), np.nanmean(speeds[i][0], axis=1) - np.nanstd(speeds[i][0],axis=1),alpha=0.5, facecolor='tab:blue')\n",
    "    ax[i].fill_between(times[i], np.nanmean(speeds[i][1], axis=1) + np.nanstd(speeds[i][1], axis=1), np.nanmean(speeds[i][1], axis=1) - np.nanstd(speeds[i][1],axis=1),alpha=0.5, facecolor='tab:orange')\n",
    "    ax[i].set_ylim((-speed_max, speed_max))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(width_len):\n",
    "    ax.scatter(i, np.mean(np.nanmean(speeds[i][0], axis=1)[200:300]), c='tab:blue', label='to tip')\n",
    "    ax.errorbar(i, np.mean(np.nanmean(speeds[i][0], axis=1)[200:300]), np.nanstd(speeds[i][0][200:300].flatten()), capsize=5, c='tab:blue')\n",
    "    ax.scatter(i, np.mean(np.nanmean(speeds[i][1], axis=1)[200:300]), c='tab:orange', label='to root')\n",
    "    ax.errorbar(i, np.mean(np.nanmean(speeds[i][1], axis=1)[200:300]), np.nanstd(speeds[i][1][200:300].flatten()), capsize=5, c='tab:orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Bulk Analysis\n",
    "## First part: Assemble Edge DataFrame\n",
    "\n",
    "\n",
    "In this initial part of the bulk analysis, all of the analysis folders will be looked through to find the edge data we're looking for. Additionally, there is an optional part to download the analysis folder back to the analysis folder we specified right at the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assuming all the analysis folders are already downloaded:\n",
    "You can use below code to read the video_data.json files that are created during indexing of all the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T13:37:43.477985Z",
     "start_time": "2023-07-19T13:37:17.323314900Z"
    }
   },
   "outputs": [],
   "source": [
    "folder_filter = dropbox_address[5:]\n",
    "\n",
    "img_infos = glob.glob(f\"{analysis_folder}{folder_filter}/**/video_data.json\", recursive=True)\n",
    "vid_anls_frame = pd.DataFrame()\n",
    "for address in tqdm(img_infos):\n",
    "    add_info = pd.read_json(address, orient='index').T\n",
    "    vid_anls_frame = pd.concat([vid_anls_frame, add_info], ignore_index=True)\n",
    "\n",
    "vid_frame = vid_anls_frame.sort_values('unique_id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T13:37:43.775322200Z",
     "start_time": "2023-07-19T13:37:43.477985Z"
    }
   },
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "### This is where you can apply the filters. Only those videos will be analyzed. ###\n",
    "####################################################################################\n",
    "\n",
    "analysis_frame = vid_frame[vid_frame['imaging_day'].ge(\"20230726\")].reset_index(drop=True)\n",
    "\n",
    "####################################################################################\n",
    "### Below code will prepare for those videos to be downloaded to videos_folder.  ###\n",
    "####################################################################################\n",
    "analysis_frame['plate_int'] = [entry.split('_')[-1] for entry in analysis_frame['plate_id']]\n",
    "analysis_frame['video_int'] = [entry.split('_')[-1] for entry in analysis_frame['unique_id']]\n",
    "analysis_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T13:39:02.719857600Z",
     "start_time": "2023-07-19T13:37:58.680411900Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_obj = HighmagDataset(analysis_frame, analysis_folder, videos_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T13:39:02.939400700Z",
     "start_time": "2023-07-19T13:39:02.731367200Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example code binned violin-plot\n",
    "bin-column represents the value to be binned, then multiple violin plots are graphed on the same axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T13:39:14.149928800Z",
     "start_time": "2023-07-19T13:39:08.130419200Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cover_filter_data = data_obj.filter_edges('coverage_tot', '>=', 0.5)\n",
    "filter_BF = cover_filter_data.filter_edges('mode', '==', 'BF')\n",
    "# filter_BF = cover_filter_data\n",
    "bin_column = 'edge_width'\n",
    "\n",
    "# bins = np.linspace(5, 15, 10)\n",
    "bins = np.linspace(filter_BF.return_edge_frame()[bin_column].min(), filter_BF.return_edge_frame()[bin_column].max(), 7)\n",
    "bin_series = filter_BF.bin_values(bin_column, bins)\n",
    "# print(bin_series)\n",
    "\n",
    "labels = []\n",
    "fig, ax = filter_BF.plot_violins('speed_right', bins, c='tab:orange', labels=labels)\n",
    "fig, ax = filter_BF.plot_violins('speed_left', bins, c='tab:blue', ax=ax, fig=fig, labels=labels)\n",
    "fig, ax = filter_BF.plot_violins('speed_mean', bins, c='tab:red', ax=ax, fig=fig, labels=labels)\n",
    "\n",
    "ax.axhline(c='black', alpha=0.5, linestyle='--')\n",
    "ax.set_ylabel('v $(\\mu m / s)$')\n",
    "ax.set_xlabel('y-position $(\\mu m)$')\n",
    "ax.legend(*zip(*labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example code for bin-less violin plots\n",
    "This can be for comparing videos, plates, anything with a unique ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T13:39:56.241915900Z",
     "start_time": "2023-07-19T13:39:54.183390500Z"
    }
   },
   "outputs": [],
   "source": [
    "cover_filter_data = data_obj.filter_edges('coverage_tot', '>=', 0.3)\n",
    "filter_BF = cover_filter_data\n",
    "\n",
    "labels = []\n",
    "fig, ax = filter_BF.plot_violins('speed_right', bin_separator='plate_id', c='tab:orange', labels=labels)\n",
    "fig, ax = filter_BF.plot_violins('speed_left', bin_separator='plate_id', c='tab:blue', ax=ax, fig=fig, labels=labels)\n",
    "fig, ax = filter_BF.plot_violins('speed_mean', bin_separator='plate_id', c='tab:red', ax=ax, fig=fig, labels=labels)\n",
    "\n",
    "ax.axhline(c='black', alpha=0.5, linestyle='--')\n",
    "ax.set_ylabel('v $(\\mu m / s)$')\n",
    "ax.set_xlabel('Plate id\\'s')\n",
    "ax.legend(*zip(*labels))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_obj.video_frame['video_int'].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example code on visualizing 4x/50x comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T15:05:10.624462400Z",
     "start_time": "2023-07-17T15:04:54.452584500Z"
    }
   },
   "outputs": [],
   "source": [
    "data_4x_filter = data_obj.filter_edges('magnification', '==', 4.0)\n",
    "mag_corr_groups = [data_obj.context_4x(row) for index, row in data_4x_filter.video_frame.iterrows()]\n",
    "for group in tqdm(mag_corr_groups):\n",
    "    group.plot_4x_locs(analysis_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example code for creating different plate maps\n",
    "Below you can see the filtering options for different plates and the plot_plate_locs function that outputs a map with dots or arrows depending on your wishes. Current drawing modes are:\n",
    "- 'scatter' for dots of the videos, separated by magnification\n",
    "- 'speeds_mean' for black arrows denoting the effective mean speed of the flows\n",
    "- 'speeds_both' for blue and orange arrows denoting the effective speed of flows in both directions\n",
    "- 'vid_labels'  for a list of what videos were taken at each position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T14:55:29.603510Z",
     "start_time": "2023-07-19T14:54:06.011630500Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "print(data_obj.video_frame.columns)\n",
    "\n",
    "for plate_id in tqdm(data_obj.video_frame['plate_id'].unique()):\n",
    "    plate_group = data_obj.filter_edges('coverage_tot', '>=', 0.3)\n",
    "    plate_group = data_obj.filter_edges('plate_id', '==', plate_id)\n",
    "    plate_group.plot_plate_locs(analysis_folder, spd_adj=2, modes=['scatter', 'vid_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "spd_maxes= []\n",
    "\n",
    "print(data_obj.video_frame['plate_int'].unique())\n",
    "\n",
    "linear_edges = data_obj.filter_edges('plate_int',  '==', 'Plate440')\n",
    "linear_edges = linear_edges.filter_edges('coverage_left', '>=', 0.3)\n",
    "linear_edges = linear_edges.filter_edges('coverage_right', '>=', 0.3)\n",
    "# linear_edges = linear_edges.filter_edges('speed_left_std', '<=', 0.5)\n",
    "# linear_edges = linear_edges.filter_edges('speed_right_std', '<=', 0.5)\n",
    "# linear_edges = linear_edges.filter_edges('speed_left', '<=', -0.9)\n",
    "# linear_edges = linear_edges.filter_edges('speed_right', '>=', 0.9)\n",
    "for edge in tqdm(linear_edges.edge_objs):\n",
    "    spd_maxes.append(edge.plot_speed_histo(spd_extent=10, spd_tiff_lowbound=0.5, spd_cutoff = 0.5, bin_res=1000, plot_fig=False))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linear_edges.edges_frame['video_int'].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spd_maxes = np.array(spd_maxes)\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.scatter(linear_edges.edges_frame['ypos'].astype(float)[59:75], spd_maxes.T[0][59:75], label='to tip, day 1')\n",
    "ax.scatter(linear_edges.edges_frame['ypos'].astype(float)[59:75], spd_maxes.T[1][59:75], label='to root, day 1')\n",
    "ax.scatter(linear_edges.edges_frame['ypos'].astype(float)[148:178], spd_maxes.T[0][148:178], label='to tip, day 2')\n",
    "ax.scatter(linear_edges.edges_frame['ypos'].astype(float)[148:178], spd_maxes.T[1][148:178], label='to root, day 2')\n",
    "# ax.scatter(linear_edges.edges_frame['ypos'].astype(float)[87:93], spd_maxes.T[0][87:93], label='to tip, hypha 3')\n",
    "# ax.scatter(linear_edges.edges_frame['ypos'].astype(float)[87:93], spd_maxes.T[1][87:93], label='to root, hypha 3')\n",
    "# ax.scatter(linear_edges.edges_frame['ypos'].astype(float)[93:], spd_maxes.T[0][93:], label='to tip, hypha 4')\n",
    "# ax.scatter(linear_edges.edges_frame['ypos'].astype(float)[93:], spd_maxes.T[1][93:], label='to root, hypha 4')\n",
    "# ax.scatter(linear_edges.edges_frame['ypos'].astype(int), spd_maxes.T[0], label='to tip')\n",
    "# ax.scatter(linear_edges.edges_frame['ypos'].astype(int), spd_maxes.T[1], label='to root')\n",
    "ax.grid(True)\n",
    "ax.set_ylabel(\"Measured speed (max of histogram) $(\\mu m /s)$\")\n",
    "ax.set_xlabel(\"y-position (root to tip)\")\n",
    "ax.set_title(\"Speeds along a single hypha\")\n",
    "ax.legend()\n",
    "# print(linear_edges.edges_frame['video_int'].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_vid_index_filter = coco_vid.edges_frame[coco_vid.edges_frame['coverage_tot'] > 0.5].index\n",
    "\n",
    "coco_vid.edge_objs = [coco_vid.edge_objs[i] for i in  coco_vid_index_filter.values]\n",
    "coco_vid.edges_frame = coco_vid.edges_frame.iloc[coco_vid_index_filter]\n",
    "\n",
    "coco_vid.scatter_speeds_video()\n",
    "coco_vid.edges_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for edge in coco_vid.edge_objs:\n",
    "    edge.show_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
