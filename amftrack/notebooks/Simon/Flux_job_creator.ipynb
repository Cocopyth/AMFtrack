{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118935f1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home6/svstaalduine/AMF_project/amftrack/util/dbx.py:3: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/scratch1/shared/amftrackflow/temp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 16:06:08.187737: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-22 16:06:08.772663: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/home6/svstaalduine/.local/lib/python3.9/site-packages/cv2/../../lib64:/sw/arch/Centos8/EB_production/2021/software/ZeroMQ/4.3.4-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/util-linux/2.36-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/libsodium/1.0.18-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/OpenPGM/5.2.122-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/Python/3.9.5-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/OpenSSL/1.1/lib:/sw/arch/Centos8/EB_production/2021/software/libffi/3.3-GCCcore-10.3.0/lib64:/sw/arch/Centos8/EB_production/2021/software/GMP/6.2.1-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/XZ/5.2.5-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/SQLite/3.35.4-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/Tcl/8.6.11-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/libreadline/8.1-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/ncurses/6.2-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/bzip2/1.0.8-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/binutils/2.36.1-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/zlib/1.2.11-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/GCCcore/10.3.0/lib64\n",
      "2023-06-22 16:06:08.772732: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-22 16:06:13.385545: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/home6/svstaalduine/.local/lib/python3.9/site-packages/cv2/../../lib64:/sw/arch/Centos8/EB_production/2021/software/ZeroMQ/4.3.4-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/util-linux/2.36-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/libsodium/1.0.18-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/OpenPGM/5.2.122-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/Python/3.9.5-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/OpenSSL/1.1/lib:/sw/arch/Centos8/EB_production/2021/software/libffi/3.3-GCCcore-10.3.0/lib64:/sw/arch/Centos8/EB_production/2021/software/GMP/6.2.1-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/XZ/5.2.5-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/SQLite/3.35.4-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/Tcl/8.6.11-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/libreadline/8.1-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/ncurses/6.2-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/bzip2/1.0.8-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/binutils/2.36.1-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/zlib/1.2.11-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/GCCcore/10.3.0/lib64\n",
      "2023-06-22 16:06:13.386117: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/home6/svstaalduine/.local/lib/python3.9/site-packages/cv2/../../lib64:/sw/arch/Centos8/EB_production/2021/software/ZeroMQ/4.3.4-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/util-linux/2.36-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/libsodium/1.0.18-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/OpenPGM/5.2.122-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/Python/3.9.5-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/OpenSSL/1.1/lib:/sw/arch/Centos8/EB_production/2021/software/libffi/3.3-GCCcore-10.3.0/lib64:/sw/arch/Centos8/EB_production/2021/software/GMP/6.2.1-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/XZ/5.2.5-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/SQLite/3.35.4-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/Tcl/8.6.11-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/libreadline/8.1-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/ncurses/6.2-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/bzip2/1.0.8-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/binutils/2.36.1-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/zlib/1.2.11-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/GCCcore/10.3.0/lib64\n",
      "2023-06-22 16:06:13.386134: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-06-22 16:06:20.253499: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/home6/svstaalduine/.local/lib/python3.9/site-packages/cv2/../../lib64:/sw/arch/Centos8/EB_production/2021/software/ZeroMQ/4.3.4-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/util-linux/2.36-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/libsodium/1.0.18-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/OpenPGM/5.2.122-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/Python/3.9.5-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/OpenSSL/1.1/lib:/sw/arch/Centos8/EB_production/2021/software/libffi/3.3-GCCcore-10.3.0/lib64:/sw/arch/Centos8/EB_production/2021/software/GMP/6.2.1-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/XZ/5.2.5-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/SQLite/3.35.4-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/Tcl/8.6.11-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/libreadline/8.1-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/ncurses/6.2-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/bzip2/1.0.8-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/binutils/2.36.1-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/zlib/1.2.11-GCCcore-10.3.0/lib:/sw/arch/Centos8/EB_production/2021/software/GCCcore/10.3.0/lib64\n",
      "2023-06-22 16:06:20.253621: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-06-22 16:06:20.253668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (int5): /proc/driver/nvidia/version does not exist\n",
      "2023-06-22 16:06:20.254341: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from amftrack.pipeline.development.high_mag_videos.kymo_class import *\n",
    "import os\n",
    "import pandas as pd\n",
    "import imageio.v2 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "%matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from amftrack.pipeline.functions.image_processing.extract_graph import (\n",
    "    from_sparse_to_graph,\n",
    "    generate_nx_graph,\n",
    "    clean_degree_4,\n",
    ")\n",
    "import scipy\n",
    "from pathlib import Path\n",
    "from amftrack.pipeline.launching.run_super import run_parallel\n",
    "import re\n",
    "from amftrack.util.dbx import upload_folders, upload, download, read_saved_dropbox_state, save_dropbox_state, load_dbx, get_dropbox_folders, get_dropbox_video_folders\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e988655",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "directory_targ = \"/gpfs/scratch1/shared/amftrackflow/Rachael_set/\"\n",
    "upload_targ = \"/DATA/FLUORESCENCE/DATA_NileRed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "836dd4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/scratch1/shared/amftrackflow/Rachael_set/20230123_Plate545\n"
     ]
    }
   ],
   "source": [
    "test_name = glob(directory_targ + '*')[0]\n",
    "imgs_address = test_name\n",
    "print(test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e1ba1fb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/scratch1/shared/amftrackflow/Rachael_set/20230123_Plate545/34/\n",
      "Found NaNs in the excel files! Blame the experimentalists.\n"
     ]
    }
   ],
   "source": [
    "plate_list = glob(directory_targ + \"*_Plate*/*/\")\n",
    "datadict = {'address_total' : plate_list}\n",
    "dataframe = pd.DataFrame(data=datadict)\n",
    "\n",
    "print(dataframe['address_total'][0])\n",
    "\n",
    "dataframe['video_name'] = [address[:-1].split('/')[-1] for address in dataframe['address_total']]\n",
    "dataframe['plate_nr'] = [re.split('_|/', address.split('_')[-1])[0][5:] for address in dataframe['address_total']]\n",
    "dataframe['video_nr'] = [int(re.split('_|/', address[:-1])[-1]) for address in dataframe['address_total']]\n",
    "dataframe['date_imaged'] = [address.split('/')[-3][:8] for address in dataframe['address_total']]\n",
    "\n",
    "# print(dataframe['date_imaged'])\n",
    "\n",
    "parent_folder = []\n",
    "data_table = []\n",
    "magnification = []\n",
    "fps = []\n",
    "mode = []\n",
    "binning = []\n",
    "\n",
    "for row in dataframe.iloc:\n",
    "#     print(row['address_total'])\n",
    "    parent_folder.append(str(Path(row['address_total']).parent))\n",
    "    excel_file = glob(str(Path(row['address_total']).parent) + f'/*{row[\"date_imaged\"]}*{row[\"plate_nr\"]}.xl*')\n",
    "    if len(excel_file) > 0:\n",
    "        data_table.append(excel_file[0])\n",
    "        excel_table = pd.read_excel(excel_file[0])\n",
    "#         print(row['video_name'])\n",
    "#         print(excel_table['Unnamed: 0'][21])\n",
    "#         print(excel_table[excel_table['Unnamed: 0'].str.contains(row['video_name'], case=False, na=False)]['Magnification'])\n",
    "        magnification.append(excel_table[excel_table['Unnamed: 0'].str.contains(row['video_name'], case=False, na=False)]['Magnification'].iloc[0])\n",
    "    \n",
    "#         magnification.append(excel_table[excel_table['Unnamed: 0'].str.split(pat='_')[-1]] == row['video_name']['Magnification'].iloc[0])\n",
    "    \n",
    "        fps.append(excel_table.loc[excel_table['Unnamed: 0'].str.contains(row['video_name'], case=False, na=False)]['FPS'].iloc[0])\n",
    "        mode.append(excel_table.loc[excel_table['Unnamed: 0'].str.contains(row['video_name'], case=False, na=False)]['Bright-field (BF)\\nor\\nFluorescence (F)'].iloc[0])\n",
    "        if 'Binned (Y/N)' in excel_table:\n",
    "            binning.append(excel_table.loc[excel_table['Unnamed: 0'].str.contains(row['video_name'], case=False, na=False)]['Binned (Y/N)'].iloc[0])\n",
    "        else:\n",
    "            binning.append('N')\n",
    "    else:\n",
    "        csv_file = glob(str(Path(row['address_total']).parent) + f'/{row[\"date_imaged\"]}*{row[\"plate_nr\"]}.csv')\n",
    "        if len(csv_file) > 0:\n",
    "            data_table.append(csv_file[0])\n",
    "            df_comma = pd.read_csv(csv_file[0], nrows=1,sep=\",\")\n",
    "            df_semi = pd.read_csv(csv_file[0], nrows=1, sep=\";\")\n",
    "            if df_comma.shape[1]>df_semi.shape[1]:\n",
    "                csv_table = pd.read_csv(csv_file[0], sep=\",\")\n",
    "            else:\n",
    "                csv_table = pd.read_csv(csv_file[0], sep=\";\")\n",
    "                \n",
    "#             print(csv_table['video'])\n",
    "#             print(row['video_name'])\n",
    "            magnification.append(csv_table[csv_table['video']==int(row['video_name'])]['Lens'].iloc[0])\n",
    "            fps.append(csv_table.loc[csv_table['video'] == int(row['video_name'].split(\"_\")[-1])]['fps'].iloc[0])\n",
    "#             print(csv_table.loc[csv_table['video'] == int(row['video_name'].split(\"_\")[-1])]['fps'].iloc[0])\n",
    "            mode.append(csv_table.loc[csv_table['video'] == int(row['video_name'].split(\"_\")[-1])]['Illumination'].iloc[0])\n",
    "        else:\n",
    "            print(\"Halp! No datatable found! Pls add the excel file to the folders.\")\n",
    "\n",
    "dataframe['fps'] = fps\n",
    "dataframe['magnification'] = magnification\n",
    "dataframe['mode'] = mode\n",
    "dataframe['parent_folder'] = [os.path.relpath(address, directory_targ) for address in dataframe['address_total']]\n",
    "dataframe['data_table'] = data_table\n",
    "dataframe['binned'] = binning\n",
    "\n",
    "dataframe = dataframe.sort_values(by='video_name', ignore_index=True)\n",
    "# dataframe = dataframe[dataframe['video_nr']== 9]\n",
    "# dataframe = dataframe[dataframe['plate_nr'] == '046']\n",
    "# print(len(dataframe))\n",
    "# print(data_table)\n",
    "\n",
    "\n",
    "if dataframe.isnull().values.any():\n",
    "    print(\"Found NaNs in the excel files! Blame the experimentalists.\")\n",
    "    dataframe = dataframe.interpolate(method='pad', limit_direction='forward')\n",
    "if dataframe.isnull().values.any():\n",
    "    raise(\"This excel sheet is unworkable, please ask the responsible person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eb97087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address_total</th>\n",
       "      <th>video_name</th>\n",
       "      <th>plate_nr</th>\n",
       "      <th>video_nr</th>\n",
       "      <th>date_imaged</th>\n",
       "      <th>fps</th>\n",
       "      <th>magnification</th>\n",
       "      <th>mode</th>\n",
       "      <th>parent_folder</th>\n",
       "      <th>data_table</th>\n",
       "      <th>binned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/Rachael_set...</td>\n",
       "      <td>001</td>\n",
       "      <td>537</td>\n",
       "      <td>1</td>\n",
       "      <td>20230118</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>F</td>\n",
       "      <td>20230118_Plate537/001</td>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/Rachael_set...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/Rachael_set...</td>\n",
       "      <td>001</td>\n",
       "      <td>530</td>\n",
       "      <td>1</td>\n",
       "      <td>20230123</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>F</td>\n",
       "      <td>20230123_Plate530/001</td>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/Rachael_set...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/Rachael_set...</td>\n",
       "      <td>002</td>\n",
       "      <td>530</td>\n",
       "      <td>2</td>\n",
       "      <td>20230123</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>BF</td>\n",
       "      <td>20230123_Plate530/002</td>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/Rachael_set...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/Rachael_set...</td>\n",
       "      <td>002</td>\n",
       "      <td>537</td>\n",
       "      <td>2</td>\n",
       "      <td>20230118</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>BF</td>\n",
       "      <td>20230118_Plate537/002</td>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/Rachael_set...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/Rachael_set...</td>\n",
       "      <td>003</td>\n",
       "      <td>530</td>\n",
       "      <td>3</td>\n",
       "      <td>20230123</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>F</td>\n",
       "      <td>20230123_Plate530/003</td>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/Rachael_set...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/Rachael_set...</td>\n",
       "      <td>95</td>\n",
       "      <td>462</td>\n",
       "      <td>95</td>\n",
       "      <td>20221109</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>F</td>\n",
       "      <td>20221109_Plate462/95</td>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/Rachael_set...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/Rachael_set...</td>\n",
       "      <td>96</td>\n",
       "      <td>462</td>\n",
       "      <td>96</td>\n",
       "      <td>20221109</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>BF</td>\n",
       "      <td>20221109_Plate462/96</td>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/Rachael_set...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/Rachael_set...</td>\n",
       "      <td>97</td>\n",
       "      <td>462</td>\n",
       "      <td>97</td>\n",
       "      <td>20221109</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>F</td>\n",
       "      <td>20221109_Plate462/97</td>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/Rachael_set...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/Rachael_set...</td>\n",
       "      <td>98</td>\n",
       "      <td>462</td>\n",
       "      <td>98</td>\n",
       "      <td>20221109</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>F</td>\n",
       "      <td>20221109_Plate462/98</td>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/Rachael_set...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/Rachael_set...</td>\n",
       "      <td>99</td>\n",
       "      <td>462</td>\n",
       "      <td>99</td>\n",
       "      <td>20221109</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>BF</td>\n",
       "      <td>20221109_Plate462/99</td>\n",
       "      <td>/gpfs/scratch1/shared/amftrackflow/Rachael_set...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>835 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         address_total video_name plate_nr  \\\n",
       "0    /gpfs/scratch1/shared/amftrackflow/Rachael_set...        001      537   \n",
       "1    /gpfs/scratch1/shared/amftrackflow/Rachael_set...        001      530   \n",
       "2    /gpfs/scratch1/shared/amftrackflow/Rachael_set...        002      530   \n",
       "3    /gpfs/scratch1/shared/amftrackflow/Rachael_set...        002      537   \n",
       "4    /gpfs/scratch1/shared/amftrackflow/Rachael_set...        003      530   \n",
       "..                                                 ...        ...      ...   \n",
       "830  /gpfs/scratch1/shared/amftrackflow/Rachael_set...         95      462   \n",
       "831  /gpfs/scratch1/shared/amftrackflow/Rachael_set...         96      462   \n",
       "832  /gpfs/scratch1/shared/amftrackflow/Rachael_set...         97      462   \n",
       "833  /gpfs/scratch1/shared/amftrackflow/Rachael_set...         98      462   \n",
       "834  /gpfs/scratch1/shared/amftrackflow/Rachael_set...         99      462   \n",
       "\n",
       "     video_nr date_imaged   fps  magnification mode          parent_folder  \\\n",
       "0           1    20230118  10.0            4.0    F  20230118_Plate537/001   \n",
       "1           1    20230123  10.0            4.0    F  20230123_Plate530/001   \n",
       "2           2    20230123  20.0           50.0   BF  20230123_Plate530/002   \n",
       "3           2    20230118  20.0           50.0   BF  20230118_Plate537/002   \n",
       "4           3    20230123  20.0           50.0    F  20230123_Plate530/003   \n",
       "..        ...         ...   ...            ...  ...                    ...   \n",
       "830        95    20221109  20.0           50.0    F   20221109_Plate462/95   \n",
       "831        96    20221109  20.0           50.0   BF   20221109_Plate462/96   \n",
       "832        97    20221109  20.0           50.0    F   20221109_Plate462/97   \n",
       "833        98    20221109  20.0           50.0    F   20221109_Plate462/98   \n",
       "834        99    20221109  20.0           50.0   BF   20221109_Plate462/99   \n",
       "\n",
       "                                            data_table binned  \n",
       "0    /gpfs/scratch1/shared/amftrackflow/Rachael_set...      Y  \n",
       "1    /gpfs/scratch1/shared/amftrackflow/Rachael_set...      Y  \n",
       "2    /gpfs/scratch1/shared/amftrackflow/Rachael_set...      Y  \n",
       "3    /gpfs/scratch1/shared/amftrackflow/Rachael_set...      Y  \n",
       "4    /gpfs/scratch1/shared/amftrackflow/Rachael_set...      Y  \n",
       "..                                                 ...    ...  \n",
       "830  /gpfs/scratch1/shared/amftrackflow/Rachael_set...      Y  \n",
       "831  /gpfs/scratch1/shared/amftrackflow/Rachael_set...      Y  \n",
       "832  /gpfs/scratch1/shared/amftrackflow/Rachael_set...      Y  \n",
       "833  /gpfs/scratch1/shared/amftrackflow/Rachael_set...      Y  \n",
       "834  /gpfs/scratch1/shared/amftrackflow/Rachael_set...      Y  \n",
       "\n",
       "[835 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "dataframe_filtered = dataframe\n",
    "# dataframe_filtered = dataframe_filtered[dataframe_filtered['date_imaged'] == \"20221026\"]\n",
    "# dataframe_filtered['plate_nr'].unique()\n",
    "dataframe_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0c49c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/scratch1/shared/amftrackflow/Rachael_set/\n",
      "/DATA/FLUORESCENCE/DATA_NileRed/\n"
     ]
    }
   ],
   "source": [
    "# FINAL CHECK FOR SOURCE FOLDER AND UPLOAD FOLDER\n",
    "\n",
    "# Please make sure that the upload folder is correct, \n",
    "# as the program WILL overwrite that which is already there.\n",
    "print(directory_targ)\n",
    "print(upload_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26548a30",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/svstaalduine/bash/job.sh\n",
      "Sending jobs with id 1687442819403521392\n",
      "835\n",
      "Submitted batch job 2958293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n",
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958297\n",
      "Submitted batch job 2958298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n",
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958300\n",
      "Submitted batch job 2958301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n",
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958307\n",
      "Submitted batch job 2958308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n",
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958314\n",
      "Submitted batch job 2958315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n",
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958318\n",
      "Submitted batch job 2958319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n",
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958320\n",
      "Submitted batch job 2958321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n",
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958322\n",
      "Submitted batch job 2958323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n",
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958326\n",
      "Submitted batch job 2958327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n",
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2958345\n",
      "Submitted batch job 2958346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 128 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 4 jobs.\n",
      "sbatch: By default shared jobs get 1792 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.25 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    }
   ],
   "source": [
    "nr_parallel = np.min([len(dataframe.index), 16])\n",
    "\n",
    "run_parallel(\n",
    "    \"flux_extract.py\",\n",
    "    [directory_targ, 15, 0.95, 0.001, 60, upload_targ],\n",
    "    dataframe_filtered,\n",
    "    nr_parallel,\n",
    "    \"1:00:00\",\n",
    "    \"flux_extract\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acce42c0",
   "metadata": {},
   "source": [
    "Upload the proper excel files to the analysis folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57f08d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/gpfs/scratch1/shared/amftrackflow/Rachael_set/20230123_Plate545/Fluorescence_ex_20230123_Plate545.xlsx',\n",
       "       '/gpfs/scratch1/shared/amftrackflow/Rachael_set/20221108_Plate462/Fluorescence_ex_20221108_plate462.xlsx',\n",
       "       '/gpfs/scratch1/shared/amftrackflow/Rachael_set/20230125_Plate532/Fluorescence_ex_20230125_Plate532.xlsx',\n",
       "       '/gpfs/scratch1/shared/amftrackflow/Rachael_set/20221109_Plate462/Fluorescence_ex_20221109_plate462.xlsx',\n",
       "       '/gpfs/scratch1/shared/amftrackflow/Rachael_set/20230123_Plate530/Fluorescence_ex_20230123_Plate530.xlsx',\n",
       "       '/gpfs/scratch1/shared/amftrackflow/Rachael_set/20230201_Plate552/Fluorescence_ex_20230201_Plate552.xlsx',\n",
       "       '/gpfs/scratch1/shared/amftrackflow/Rachael_set/20221208_Plate510/Fluorescence_ex_20221208_Plate510.xlsx',\n",
       "       '/gpfs/scratch1/shared/amftrackflow/Rachael_set/20221026_Plate452/Fluorescence_ex_20221026_plate452.xlsx',\n",
       "       '/gpfs/scratch1/shared/amftrackflow/Rachael_set/20230201_Plate558/Fluorescence_ex_20230201_Plate558.xlsx',\n",
       "       '/gpfs/scratch1/shared/amftrackflow/Rachael_set/20221027_Plate452/Fluorescence_ex_20221027_plate452.xlsx',\n",
       "       '/gpfs/scratch1/shared/amftrackflow/Rachael_set/20230126_Plate528/Fluorescence_ex_20230126_Plate528.xlsx',\n",
       "       '/gpfs/scratch1/shared/amftrackflow/Rachael_set/20230126_Plate527/Fluorescence_ex_20230126_Plate527.xlsx',\n",
       "       '/gpfs/scratch1/shared/amftrackflow/Rachael_set/20230118_Plate537/Fluorescence_ex_20230118_Plate537.xlsx'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excel_series = pd.Series(data_table).unique()\n",
    "excel_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51cb59f7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/DATA/FLUORESCENCE/DATA_NileRed/Analysis/20230123_Plate545/Fluorescence_ex_20230123_Plate545.xlsx\n",
      "2023-06-20 11:12:22,949-[INFO]- dropbox:390 -> Refreshing access token.\n",
      "2023-06-20 11:12:23,475-[INFO]- dropbox:474 -> Request to files/upload\n",
      "/DATA/FLUORESCENCE/DATA_NileRed/Analysis/20221108_Plate462/Fluorescence_ex_20221108_plate462.xlsx\n",
      "2023-06-20 11:12:24,281-[INFO]- dropbox:390 -> Refreshing access token.\n",
      "2023-06-20 11:12:24,452-[INFO]- dropbox:474 -> Request to files/upload\n",
      "/DATA/FLUORESCENCE/DATA_NileRed/Analysis/20230125_Plate532/Fluorescence_ex_20230125_Plate532.xlsx\n",
      "2023-06-20 11:12:24,970-[INFO]- dropbox:390 -> Refreshing access token.\n",
      "2023-06-20 11:12:25,158-[INFO]- dropbox:474 -> Request to files/upload\n",
      "/DATA/FLUORESCENCE/DATA_NileRed/Analysis/20221109_Plate462/Fluorescence_ex_20221109_plate462.xlsx\n",
      "2023-06-20 11:12:25,825-[INFO]- dropbox:390 -> Refreshing access token.\n",
      "2023-06-20 11:12:26,029-[INFO]- dropbox:474 -> Request to files/upload\n",
      "/DATA/FLUORESCENCE/DATA_NileRed/Analysis/20230123_Plate530/Fluorescence_ex_20230123_Plate530.xlsx\n",
      "2023-06-20 11:12:26,936-[INFO]- dropbox:390 -> Refreshing access token.\n",
      "2023-06-20 11:12:27,118-[INFO]- dropbox:474 -> Request to files/upload\n",
      "/DATA/FLUORESCENCE/DATA_NileRed/Analysis/20230201_Plate552/Fluorescence_ex_20230201_Plate552.xlsx\n",
      "2023-06-20 11:12:27,802-[INFO]- dropbox:390 -> Refreshing access token.\n",
      "2023-06-20 11:12:27,951-[INFO]- dropbox:474 -> Request to files/upload\n",
      "/DATA/FLUORESCENCE/DATA_NileRed/Analysis/20221208_Plate510/Fluorescence_ex_20221208_Plate510.xlsx\n",
      "2023-06-20 11:12:28,524-[INFO]- dropbox:390 -> Refreshing access token.\n",
      "2023-06-20 11:12:28,674-[INFO]- dropbox:474 -> Request to files/upload\n",
      "/DATA/FLUORESCENCE/DATA_NileRed/Analysis/20221026_Plate452/Fluorescence_ex_20221026_plate452.xlsx\n",
      "2023-06-20 11:12:29,265-[INFO]- dropbox:390 -> Refreshing access token.\n",
      "2023-06-20 11:12:29,418-[INFO]- dropbox:474 -> Request to files/upload\n",
      "/DATA/FLUORESCENCE/DATA_NileRed/Analysis/20230201_Plate558/Fluorescence_ex_20230201_Plate558.xlsx\n",
      "2023-06-20 11:12:30,140-[INFO]- dropbox:390 -> Refreshing access token.\n",
      "2023-06-20 11:12:30,291-[INFO]- dropbox:474 -> Request to files/upload\n",
      "/DATA/FLUORESCENCE/DATA_NileRed/Analysis/20221027_Plate452/Fluorescence_ex_20221027_plate452.xlsx\n",
      "2023-06-20 11:12:31,018-[INFO]- dropbox:390 -> Refreshing access token.\n",
      "2023-06-20 11:12:31,169-[INFO]- dropbox:474 -> Request to files/upload\n",
      "/DATA/FLUORESCENCE/DATA_NileRed/Analysis/20230126_Plate528/Fluorescence_ex_20230126_Plate528.xlsx\n",
      "2023-06-20 11:12:31,820-[INFO]- dropbox:390 -> Refreshing access token.\n",
      "2023-06-20 11:12:31,989-[INFO]- dropbox:474 -> Request to files/upload\n",
      "/DATA/FLUORESCENCE/DATA_NileRed/Analysis/20230126_Plate527/Fluorescence_ex_20230126_Plate527.xlsx\n",
      "2023-06-20 11:12:32,645-[INFO]- dropbox:390 -> Refreshing access token.\n",
      "2023-06-20 11:12:32,794-[INFO]- dropbox:474 -> Request to files/upload\n",
      "/DATA/FLUORESCENCE/DATA_NileRed/Analysis/20230118_Plate537/Fluorescence_ex_20230118_Plate537.xlsx\n",
      "2023-06-20 11:12:33,430-[INFO]- dropbox:390 -> Refreshing access token.\n",
      "2023-06-20 11:12:33,586-[INFO]- dropbox:474 -> Request to files/upload\n"
     ]
    }
   ],
   "source": [
    "for xl_address in excel_series:\n",
    "    file_name = \"Analysis/\" + os.path.relpath(xl_address, directory_targ)\n",
    "    print(upload_targ + file_name)\n",
    "    upload(xl_adress, upload_targ+file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba2118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
