{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95be08ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home6/svstaalduine/AMF_project/amftrack/util/dbx.py:3: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# sys.path.insert(0, os.getenv('HOME')+'/pycode/MscThesis/')\n",
    "# sys.path.insert(0,r'C:\\Users\\coren\\Documents\\PhD\\Code\\AMFtrack')\n",
    "\n",
    "import pandas as pd\n",
    "from amftrack.util.sys import (\n",
    "    get_dates_datetime,\n",
    "    get_dirname,\n",
    "    temp_path,\n",
    "    get_data_info,\n",
    "    update_plate_info,\n",
    "    get_current_folders,\n",
    "    get_folders_by_plate_id,\n",
    ")\n",
    "\n",
    "\n",
    "import ast\n",
    "from amftrack.plotutil import plot_t_tp1\n",
    "from scipy import sparse\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "from pymatreader import read_mat\n",
    "from matplotlib import colors\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.filters import frangi\n",
    "from skimage import filters\n",
    "from random import choice\n",
    "import scipy.sparse\n",
    "import os\n",
    "from amftrack.pipeline.functions.image_processing.extract_graph import (\n",
    "    from_sparse_to_graph,\n",
    "    generate_nx_graph,\n",
    "    sparse_to_doc,\n",
    ")\n",
    "from skimage.feature import hessian_matrix_det\n",
    "\n",
    "# from amftrack.pipeline.functions.image_processing.experiment_class_surf import Experiment\n",
    "from amftrack.pipeline.launching.run import (\n",
    "    run_transfer,\n",
    ")\n",
    "from amftrack.pipeline.launching.run_super import run_parallel_transfer\n",
    "\n",
    "import dropbox\n",
    "from amftrack.util.dbx import upload_folders, download, read_saved_dropbox_state, save_dropbox_state, load_dbx, download, get_dropbox_folders, get_dropbox_video_folders\n",
    "from subprocess import call\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037f25a3",
   "metadata": {},
   "source": [
    "**To transfer data from dropbox to surfsara**\n",
    "- select the folders of interest within the folders of dropbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ab21ced",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "2023-04-24 16:37:04,654-[INFO]- dropbox:390 -> Refreshing access token.\n",
      "2023-04-24 16:37:04,825-[INFO]- dropbox:474 -> Request to files/list_folder\n",
      "2023-04-24 16:37:05,697-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:07,276-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:08,334-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:09,914-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:11,280-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:12,424-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:14,572-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:15,766-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:17,168-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:18,202-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:19,302-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:20,605-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:22,455-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:27,586-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:28,637-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:30,117-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:31,710-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:33,151-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:34,239-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:36,179-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:37,317-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:38,518-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:39,512-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:40,767-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:42,262-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:43,782-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:45,510-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:46,612-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:48,131-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:49,120-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:50,226-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:51,341-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:52,375-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:54,029-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:55,551-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:57,548-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:58,830-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:37:59,933-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:00,954-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:02,105-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:03,179-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:04,175-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:05,264-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:06,315-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:07,574-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:08,566-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:10,949-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:12,055-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:13,585-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:14,603-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:15,601-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:16,651-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:18,111-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:19,099-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:20,941-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:26,239-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:28,597-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:29,660-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:32,327-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:33,401-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:34,831-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:36,905-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:38,203-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:39,211-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:40,266-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:41,406-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:42,577-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:44,058-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:45,072-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:46,241-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:47,488-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:48,443-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:49,656-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:50,596-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:51,593-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:52,598-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:53,938-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:55,101-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:38:58,580-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:39:00,439-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:39:01,503-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:39:02,952-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:39:04,187-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "2023-04-24 16:39:04,730-[INFO]- dropbox:474 -> Request to files/list_folder/continue\n",
      "Hello!\n",
      "Submitted batch job 2651635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n",
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651642\n",
      "Submitted batch job 2651643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n",
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651644\n",
      "Submitted batch job 2651645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n",
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651649\n",
      "Submitted batch job 2651650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n",
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651656\n",
      "Submitted batch job 2651657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n",
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651665\n",
      "Submitted batch job 2651666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n",
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651667\n",
      "Submitted batch job 2651668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n",
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651683\n",
      "Submitted batch job 2651684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n",
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651688\n",
      "Submitted batch job 2651689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n",
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651693\n",
      "Submitted batch job 2651694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n",
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2651706\n",
      "Submitted batch job 2651707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single-node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: A full node consists of 32 CPU cores, 229376 MiB of memory and 0 GPUs and can be shared by up to 32 jobs.\n",
      "sbatch: By default shared jobs get 7168 MiB of memory per CPU core, unless explicitly overridden with --mem-per-cpu, --mem-per-gpu or --mem.\n",
      "sbatch: You will be charged for 0.03125 node, based on the number of CPUs, GPUs and the amount memory that you've requested.\n"
     ]
    }
   ],
   "source": [
    "directory_targ = \"/gpfs/scratch1/shared/amftrackflow/Drp_downs/20221026_Plate452/\"\n",
    "\n",
    "if not os.path.exists(directory_targ):\n",
    "    os.makedirs(directory_targ)\n",
    "\n",
    "all_folders_drop, excel_drop = get_dropbox_video_folders(\"/DATA/FLUORESCENCE/DATA_NileRed/20221026_Plate452/\", True)\n",
    "folders_drop = all_folders_drop.loc[all_folders_drop[\"Plate number\"] == \"452\"]\n",
    "\n",
    "# for xl_adress in excel_drop:\n",
    "#     file_name = xl_adress.split('/')[-1]\n",
    "    \n",
    "#     download(xl_adress, directory_targ+file_name)\n",
    "clear_output(wait=False)\n",
    "\n",
    "    \n",
    "run_parallel_transfer(\n",
    "    \"from_drop.py\",\n",
    "    [directory_targ],\n",
    "    folders_drop,\n",
    "    1,\n",
    "    \"10:00:00\",\n",
    "    \"transfer_test\"\n",
    ")\n",
    "\n",
    "# save_dropbox_state(\"/DATA/FLUORESCENCE/DATA_NileRed/20230201_Plate552/\", is_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a49f377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "841d0860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder</th>\n",
       "      <th>Plate number</th>\n",
       "      <th>Date Imaged</th>\n",
       "      <th>tot_path_drop</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20230126_Plate528_01</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20230126_Plate528_02</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20230126_Plate528_03</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20230126_Plate528_04</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20230126_Plate528_05</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20230126_Plate528_06</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20230126_Plate528_07</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20230126_Plate528_08</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20230126_Plate528_09</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20230126_Plate528_10</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20230126_Plate528_11</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20230126_Plate528_12</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20230126_Plate528_13</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20230126_Plate528_14</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20230126_Plate528_15</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20230126_Plate528_16</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20230126_Plate528_17</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20230126_Plate528_18</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20230126_Plate528_19</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20230126_Plate528_20</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20230126_Plate528_21</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20230126_Plate528_22</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20230126_Plate528_23</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20230126_Plate528_24</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20230126_Plate528_25</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20230126_Plate528_26</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20230126_Plate528_27</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20230126_Plate528_28</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20230126_Plate528_29</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>20230126_Plate528_30</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>20230126_Plate528_31</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>20230126_Plate528_32</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>20230126_Plate528_33</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>20230126_Plate528_34</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>20230126_Plate528_35</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>20230126_Plate528_36</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>20230126_Plate528_37</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>20230126_Plate528_38</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>20230126_Plate528_39</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  folder Plate number Date Imaged  \\\n",
       "0   20230126_Plate528_01          528    20230126   \n",
       "1   20230126_Plate528_02          528    20230126   \n",
       "2   20230126_Plate528_03          528    20230126   \n",
       "3   20230126_Plate528_04          528    20230126   \n",
       "4   20230126_Plate528_05          528    20230126   \n",
       "5   20230126_Plate528_06          528    20230126   \n",
       "6   20230126_Plate528_07          528    20230126   \n",
       "7   20230126_Plate528_08          528    20230126   \n",
       "8   20230126_Plate528_09          528    20230126   \n",
       "9   20230126_Plate528_10          528    20230126   \n",
       "10  20230126_Plate528_11          528    20230126   \n",
       "11  20230126_Plate528_12          528    20230126   \n",
       "12  20230126_Plate528_13          528    20230126   \n",
       "13  20230126_Plate528_14          528    20230126   \n",
       "14  20230126_Plate528_15          528    20230126   \n",
       "15  20230126_Plate528_16          528    20230126   \n",
       "16  20230126_Plate528_17          528    20230126   \n",
       "17  20230126_Plate528_18          528    20230126   \n",
       "18  20230126_Plate528_19          528    20230126   \n",
       "19  20230126_Plate528_20          528    20230126   \n",
       "20  20230126_Plate528_21          528    20230126   \n",
       "21  20230126_Plate528_22          528    20230126   \n",
       "22  20230126_Plate528_23          528    20230126   \n",
       "23  20230126_Plate528_24          528    20230126   \n",
       "24  20230126_Plate528_25          528    20230126   \n",
       "25  20230126_Plate528_26          528    20230126   \n",
       "26  20230126_Plate528_27          528    20230126   \n",
       "27  20230126_Plate528_28          528    20230126   \n",
       "28  20230126_Plate528_29          528    20230126   \n",
       "29  20230126_Plate528_30          528    20230126   \n",
       "30  20230126_Plate528_31          528    20230126   \n",
       "31  20230126_Plate528_32          528    20230126   \n",
       "32  20230126_Plate528_33          528    20230126   \n",
       "33  20230126_Plate528_34          528    20230126   \n",
       "34  20230126_Plate528_35          528    20230126   \n",
       "35  20230126_Plate528_36          528    20230126   \n",
       "36  20230126_Plate528_37          528    20230126   \n",
       "37  20230126_Plate528_38          528    20230126   \n",
       "38  20230126_Plate528_39          528    20230126   \n",
       "\n",
       "                                        tot_path_drop video  \n",
       "0   DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    01  \n",
       "1   DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    02  \n",
       "2   DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    03  \n",
       "3   DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    04  \n",
       "4   DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    05  \n",
       "5   DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    06  \n",
       "6   DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    07  \n",
       "7   DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    08  \n",
       "8   DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    09  \n",
       "9   DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    10  \n",
       "10  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    11  \n",
       "11  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    12  \n",
       "12  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    13  \n",
       "13  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    14  \n",
       "14  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    15  \n",
       "15  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    16  \n",
       "16  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    17  \n",
       "17  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    18  \n",
       "18  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    19  \n",
       "19  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    20  \n",
       "20  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    21  \n",
       "21  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    22  \n",
       "22  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    23  \n",
       "23  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    24  \n",
       "24  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    25  \n",
       "25  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    26  \n",
       "26  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    27  \n",
       "27  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    28  \n",
       "28  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    29  \n",
       "29  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    30  \n",
       "30  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    31  \n",
       "31  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    32  \n",
       "32  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    33  \n",
       "33  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    34  \n",
       "34  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    35  \n",
       "35  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    36  \n",
       "36  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    37  \n",
       "37  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    38  \n",
       "38  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    39  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_folders_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70a8e114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder</th>\n",
       "      <th>Plate number</th>\n",
       "      <th>Date Imaged</th>\n",
       "      <th>tot_path_drop</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20230126_Plate528_01</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20230126_Plate528_02</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20230126_Plate528_03</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20230126_Plate528_04</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20230126_Plate528_05</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20230126_Plate528_06</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20230126_Plate528_07</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20230126_Plate528_08</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20230126_Plate528_09</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20230126_Plate528_10</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20230126_Plate528_11</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20230126_Plate528_12</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20230126_Plate528_13</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20230126_Plate528_14</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20230126_Plate528_15</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20230126_Plate528_16</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20230126_Plate528_17</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20230126_Plate528_18</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20230126_Plate528_19</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20230126_Plate528_20</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20230126_Plate528_21</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20230126_Plate528_22</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20230126_Plate528_23</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20230126_Plate528_24</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20230126_Plate528_25</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20230126_Plate528_26</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20230126_Plate528_27</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20230126_Plate528_28</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20230126_Plate528_29</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>20230126_Plate528_30</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>20230126_Plate528_31</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>20230126_Plate528_32</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>20230126_Plate528_33</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>20230126_Plate528_34</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>20230126_Plate528_35</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>20230126_Plate528_36</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>20230126_Plate528_37</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>20230126_Plate528_38</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>20230126_Plate528_39</td>\n",
       "      <td>528</td>\n",
       "      <td>20230126</td>\n",
       "      <td>DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  folder Plate number Date Imaged  \\\n",
       "0   20230126_Plate528_01          528    20230126   \n",
       "1   20230126_Plate528_02          528    20230126   \n",
       "2   20230126_Plate528_03          528    20230126   \n",
       "3   20230126_Plate528_04          528    20230126   \n",
       "4   20230126_Plate528_05          528    20230126   \n",
       "5   20230126_Plate528_06          528    20230126   \n",
       "6   20230126_Plate528_07          528    20230126   \n",
       "7   20230126_Plate528_08          528    20230126   \n",
       "8   20230126_Plate528_09          528    20230126   \n",
       "9   20230126_Plate528_10          528    20230126   \n",
       "10  20230126_Plate528_11          528    20230126   \n",
       "11  20230126_Plate528_12          528    20230126   \n",
       "12  20230126_Plate528_13          528    20230126   \n",
       "13  20230126_Plate528_14          528    20230126   \n",
       "14  20230126_Plate528_15          528    20230126   \n",
       "15  20230126_Plate528_16          528    20230126   \n",
       "16  20230126_Plate528_17          528    20230126   \n",
       "17  20230126_Plate528_18          528    20230126   \n",
       "18  20230126_Plate528_19          528    20230126   \n",
       "19  20230126_Plate528_20          528    20230126   \n",
       "20  20230126_Plate528_21          528    20230126   \n",
       "21  20230126_Plate528_22          528    20230126   \n",
       "22  20230126_Plate528_23          528    20230126   \n",
       "23  20230126_Plate528_24          528    20230126   \n",
       "24  20230126_Plate528_25          528    20230126   \n",
       "25  20230126_Plate528_26          528    20230126   \n",
       "26  20230126_Plate528_27          528    20230126   \n",
       "27  20230126_Plate528_28          528    20230126   \n",
       "28  20230126_Plate528_29          528    20230126   \n",
       "29  20230126_Plate528_30          528    20230126   \n",
       "30  20230126_Plate528_31          528    20230126   \n",
       "31  20230126_Plate528_32          528    20230126   \n",
       "32  20230126_Plate528_33          528    20230126   \n",
       "33  20230126_Plate528_34          528    20230126   \n",
       "34  20230126_Plate528_35          528    20230126   \n",
       "35  20230126_Plate528_36          528    20230126   \n",
       "36  20230126_Plate528_37          528    20230126   \n",
       "37  20230126_Plate528_38          528    20230126   \n",
       "38  20230126_Plate528_39          528    20230126   \n",
       "\n",
       "                                        tot_path_drop video  \n",
       "0   DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    01  \n",
       "1   DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    02  \n",
       "2   DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    03  \n",
       "3   DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    04  \n",
       "4   DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    05  \n",
       "5   DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    06  \n",
       "6   DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    07  \n",
       "7   DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    08  \n",
       "8   DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    09  \n",
       "9   DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    10  \n",
       "10  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    11  \n",
       "11  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    12  \n",
       "12  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    13  \n",
       "13  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    14  \n",
       "14  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    15  \n",
       "15  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    16  \n",
       "16  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    17  \n",
       "17  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    18  \n",
       "18  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    19  \n",
       "19  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    20  \n",
       "20  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    21  \n",
       "21  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    22  \n",
       "22  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    23  \n",
       "23  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    24  \n",
       "24  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    25  \n",
       "25  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    26  \n",
       "26  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    27  \n",
       "27  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    28  \n",
       "28  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    29  \n",
       "29  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    30  \n",
       "30  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    31  \n",
       "31  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    32  \n",
       "32  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    33  \n",
       "33  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    34  \n",
       "34  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    35  \n",
       "35  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    36  \n",
       "36  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    37  \n",
       "37  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    38  \n",
       "38  DATA/FLUORESCENCE/DATA_NileRed/20230126_Plate5...    39  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders_drop = all_folders_drop.loc[all_folders_drop[\"Plate number\"] == \"528\"]\n",
    "folders_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49166661",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_targ = \"/gpfs/scratch1/shared/amftrackflow/Drp_downs/20230201_Plate528/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a4cc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-11 11:22:49,196-[INFO]- dropbox:390 -> Refreshing access token.\n",
      "2023-04-11 11:22:49,395-[INFO]- dropbox:474 -> Request to files/download\n"
     ]
    }
   ],
   "source": [
    "for xl_adress in excel_drop:\n",
    "    file_name = xl_adress.split('/')[-1]\n",
    "    \n",
    "    download(xl_adress, directory_targ+file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4c6a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_parallel_transfer(\n",
    "    \"from_drop.py\",\n",
    "    [directory_targ],\n",
    "    folders_drop,\n",
    "    1,\n",
    "    \"10:00:00\",\n",
    "    \"transfer_test\"\n",
    ")\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4137b51f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
