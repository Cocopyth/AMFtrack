{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JGYSX0biFfDr"
   },
   "source": [
    "# Model Selection, Underfitting and Overfitting (Book chapter 4.4)\n",
    "\n",
    "We will explore these concepts interactively by fitting polynomials to data.\n",
    "To get started we will install and import our usual packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U3c-pBn1Flk1"
   },
   "outputs": [],
   "source": [
    "! pip3 install mxnet==1.6.0b20190926\n",
    "! pip3 install d2l==0.11.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    },
    "colab": {},
    "colab_type": "code",
    "id": "NzGtU08hFfDy"
   },
   "outputs": [],
   "source": [
    "import d2l\n",
    "from mxnet import gluon, np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bjSRCi6nFfD-"
   },
   "source": [
    "### Generating the Dataset\n",
    "\n",
    "First we need data. Given $x$, we will use the following cubic polynomial to generate the labels on training and test data:\n",
    "\n",
    "$$y = 5 + 1.2x - 3.4\\frac{x^2}{2!} + 5.6 \\frac{x^3}{3!} + \\epsilon \\text{ where }\n",
    "\\epsilon \\sim \\mathcal{N}(0, 0.1).$$\n",
    "\n",
    "The noise term $\\epsilon$ obeys a normal distribution\n",
    "with a mean of 0 and a standard deviation of 0.1.\n",
    "We will synthesize 100 samples each for the training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    },
    "colab": {},
    "colab_type": "code",
    "id": "BA2u9XWbFfEB"
   },
   "outputs": [],
   "source": [
    "maxdegree = 20  # Maximum degree of the polynomial\n",
    "n_train, n_test = 100, 100  # Training and test dataset sizes\n",
    "true_w = np.zeros(maxdegree)  # Allocate lots of empty space\n",
    "true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])\n",
    "\n",
    "features = np.random.normal(size=(n_train + n_test, 1))\n",
    "features = np.random.shuffle(features)\n",
    "poly_features = np.power(features, np.arange(maxdegree).reshape(1, -1))\n",
    "poly_features = poly_features / (\n",
    "    npx.gamma(np.arange(maxdegree) + 1).reshape(1, -1))\n",
    "labels = np.dot(poly_features, true_w)\n",
    "labels += np.random.normal(scale=0.1, size=labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rm0bfnCTFfEJ"
   },
   "source": [
    "For optimization, we typically want to avoid\n",
    "very large values of gradients, losses, etc.\n",
    "This is why the monomials stored in `poly_features`\n",
    "are rescaled from $x^i$ to $\\frac{1}{i!} x^i$.\n",
    "It allows us to avoid very large values for large exponents $i$.\n",
    "Factorials are implemented in Gluon using the Gamma function,\n",
    "where $n! = \\Gamma(n+1)$.\n",
    "\n",
    "Take a look at the first 2 samples from the generated dataset.\n",
    "The value 1 is technically a feature,\n",
    "namely the constant feature corresponding to the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "colab": {},
    "colab_type": "code",
    "id": "4W4eEoVjFfEN",
    "outputId": "5dde73a4-2095-4b46-826e-c2cbf4f28b91"
   },
   "outputs": [],
   "source": [
    "features[:2], poly_features[:2], labels[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cu1HCz77FfEW"
   },
   "source": [
    "### Training and Testing Model\n",
    "\n",
    "Let's first implement a function to evaluate the loss on a given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1P_D0fiYFfEY"
   },
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def evaluate_loss(net, data_iter, loss):\n",
    "    \"\"\"Evaluate the loss of a model on the given dataset.\"\"\"\n",
    "    metric = d2l.Accumulator(2)  # sum_loss, num_examples\n",
    "    for X, y in data_iter:\n",
    "        metric.add(loss(net(X), y).sum(), y.size)\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h_cs17fCFfEh"
   },
   "source": [
    "Now define the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    },
    "colab": {},
    "colab_type": "code",
    "id": "UcuriQQlFfEk"
   },
   "outputs": [],
   "source": [
    "def train(train_features, test_features, train_labels, test_labels,\n",
    "          num_epochs=1000):\n",
    "    loss = gluon.loss.L2Loss()\n",
    "    net = nn.Sequential()\n",
    "    # Switch off the bias since we already catered for it in the polynomial\n",
    "    # features\n",
    "    net.add(nn.Dense(1, use_bias=False))\n",
    "    net.initialize()\n",
    "    batch_size = min(10, train_labels.shape[0])\n",
    "    train_iter = d2l.load_array((train_features, train_labels), batch_size)\n",
    "    test_iter = d2l.load_array((test_features, test_labels), batch_size,\n",
    "                               is_train=False)\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd',\n",
    "                            {'learning_rate': 0.01})\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log',\n",
    "                            xlim=[1, num_epochs], ylim=[1e-3, 1e2],\n",
    "                            legend=['train', 'test'])\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        d2l.train_epoch_ch3(net, train_iter, loss, trainer)\n",
    "        if epoch % 50 == 0:\n",
    "            animator.add(epoch, (d2l.evaluate_loss(net, train_iter, loss),\n",
    "                                 d2l.evaluate_loss(net, test_iter, loss)))\n",
    "    print('weight:', net[0].weight.data().asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ddB0-98DFfEr"
   },
   "source": [
    "### Third-Order Polynomial Function Fitting (Normal)\n",
    "\n",
    "We will begin by first using a third-order polynomial function\n",
    "with the same order as the data generation function.\n",
    "The results show that this model’s training error rate\n",
    "when using the testing dataset is low.\n",
    "The trained model parameters are also close\n",
    "to the true values $w = [5, 1.2, -3.4, 5.6]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    },
    "colab": {},
    "colab_type": "code",
    "id": "euFw1yuhFfEv",
    "outputId": "8e497bc1-0d1d-4c69-e044-beea2233d7b7"
   },
   "outputs": [],
   "source": [
    "# Pick the first four dimensions, i.e., 1, x, x^2, x^3 from the polynomial\n",
    "# features\n",
    "train(poly_features[:n_train, 0:4], poly_features[n_train:, 0:4],\n",
    "      labels[:n_train], labels[n_train:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9bAKfgelFfE5"
   },
   "source": [
    "### Linear Function Fitting (Underfitting)\n",
    "\n",
    "Let’s take another look at linear function fitting.\n",
    "After the decline in the early epoch,\n",
    "it becomes difficult to further decrease\n",
    "this model’s training error rate.\n",
    "After the last epoch iteration has been completed,\n",
    "the training error rate is still high.\n",
    "When used to fit non-linear patterns\n",
    "(like the third-order polynomial function here)\n",
    "linear models are liable to underfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    },
    "colab": {},
    "colab_type": "code",
    "id": "aIBryCmLFfE8",
    "outputId": "692b5bde-9ad5-43c9-ef17-fa708df789cf"
   },
   "outputs": [],
   "source": [
    "# Pick the first four dimensions, i.e., 1, x from the polynomial features\n",
    "train(poly_features[:n_train, 0:3], poly_features[n_train:, 0:3],\n",
    "      labels[:n_train], labels[n_train:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pAIcVjbjFfFD"
   },
   "source": [
    "### Insufficient Training (Overfitting)\n",
    "\n",
    "Now let's try to train the model\n",
    "using a polynomial of too high degree.\n",
    "Here, there is insufficient data to learn that\n",
    "the higher-degree coefficients should have values close to zero.\n",
    "As a result, our overly-complex model\n",
    "is far too susceptible to being influenced\n",
    "by noise in the training data.\n",
    "Of course, our training error will now be low\n",
    "(even lower than if we had the right model!)\n",
    "but our test error will be high.\n",
    "\n",
    "Try out different model complexities (`n_degree`)\n",
    "and training set sizes (`n_subset`)\n",
    "to gain some intuition of what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    },
    "colab": {},
    "colab_type": "code",
    "id": "SSEn4xfHFfFF",
    "outputId": "4167b31b-e468-4d21-8aef-9b9870d74259"
   },
   "outputs": [],
   "source": [
    "n_subset = 100  # Subset of data to train on\n",
    "n_degree = 20  # Degree of polynomials\n",
    "train(poly_features[1:n_subset, 0:n_degree],\n",
    "      poly_features[n_train:, 0:n_degree], labels[1:n_subset],\n",
    "      labels[n_train:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subset = 100  # Subset of data to train on\n",
    "n_degree = 3  # Degree of polynomials\n",
    "train(poly_features[1:n_subset, 0:n_degree],\n",
    "      poly_features[n_train:, 0:n_degree], labels[1:n_subset],\n",
    "      labels[n_train:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dGRsifCOFfFM"
   },
   "source": [
    "## Summary\n",
    "\n",
    "* Since the generalization error rate cannot be estimated based on the training error rate, simply minimizing the training error rate will not necessarily mean a reduction in the generalization error rate. Machine learning models need to be careful to safeguard against overfitting such as to minimize the generalization error.\n",
    "* A validation set can be used for model selection (provided that it is not used too liberally).\n",
    "* Underfitting means that the model is not able to reduce the training error rate while overfitting is a result of the model training error rate being much lower than the testing dataset rate.\n",
    "* We should choose an appropriately complex model and avoid using insufficient training samples.\n",
    "\n",
    "\n",
    "\n",
    "## [Discussions on Slack!](https://wurdeeplearningcourse.slack.com/)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "4.4.underfit-overfit.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
