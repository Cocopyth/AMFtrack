{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week2day2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWCFU80h-S7E",
        "colab_type": "text"
      },
      "source": [
        "# Week 2, day 2 - CNN with less overfitting (by D. Tuia, 2020)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fdofhEXzGQQ",
        "colab_type": "text"
      },
      "source": [
        "**1. Re-load the LeNet of yesterday  and play around with parameters**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNThwPwdzN61",
        "colab_type": "text"
      },
      "source": [
        "*Preliminaries (given to you)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk9ODBrDxwKg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#intall mxp net and load libraries to start the day\n",
        "!pip install mxnet-cu101==1.6.0b20191122\n",
        "from mxnet import autograd, gluon, init, np, npx\n",
        "from mxnet.gluon import nn\n",
        "npx.set_np()\n",
        "\n",
        "#get the data\n",
        "!pip install git+https://github.com/d2l-ai/d2l-en\n",
        "import d2l\n",
        "batch_size = 256\n",
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5Vjqh8yzbsu",
        "colab_type": "text"
      },
      "source": [
        "*Create an instance of LeNet as yesterday*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVaxxBznzrj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create an instance of a LeNet (6.6.1)\n",
        "net = nn.Sequential()\n",
        "\n",
        "#add the different layers\n",
        "net.add(nn.Conv2D(channels=6, kernel_size=5,padding=2,activation='sigmoid'), #1st conv, with padding\n",
        "        nn.AvgPool2D(pool_size=2,strides=2), #1st pooling, non overlapping stride\n",
        "        nn.Conv2D(channels=16, kernel_size=5,activation='sigmoid'), #2nd conv, no padding this time\n",
        "        nn.AvgPool2D(pool_size=2,strides=2), #1st pooling, non overlapping stride        \n",
        "        nn.Dense(120,activation='sigmoid'), #fc1\n",
        "        nn.Dense(84,activation='sigmoid'), #fc2\n",
        "        nn.Dense(10)       # classifier\n",
        "        )\n",
        "\n",
        "#create a random point, the size of a point in MNIST\n",
        "X = np.random.uniform(size=(1,1,28,28))\n",
        "#print(X)\n",
        "#print(net)\n",
        "\n",
        "#initalize all weights with random values\n",
        "net.initialize()\n",
        "\n",
        "#let's visualize if everything works out and the size of all tensors.\n",
        "for layer in net:\n",
        "  #print(layer)\n",
        "  X = layer(X)\n",
        "  print(layer.name, 'output size:\\t', X.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpoU92njzUoG",
        "colab_type": "text"
      },
      "source": [
        "*Load the accuracy evaluation function you wrote yesterday (do not forget to activate the GPU)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R2l033LzbLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ctx = list(net.collect_params().values())[0].list_ctx()[0] #tells us which GPU is being used to train the model.\n",
        "print (ctx)\n",
        "\n",
        "# write the evaluation function\n",
        "def evaluate_accuracy_gpu(net, data_iter, ctx=None): #data_iter is the dataset used for accuracy evaluation\n",
        "    if not ctx:  # Query the first device the first parameter is on\n",
        "        ctx = list(net.collect_params().values())[0].list_ctx()[0] #tells us which GPU is being used to train the model.\n",
        "        print (ctx)\n",
        "    metric = d2l.Accumulator(2)  # num_corrected_examples, num_examples\n",
        "    for X, y in data_iter:\n",
        "        X, y = X.as_in_context(ctx), y.as_in_context(ctx) #as_in_context moves data to the GPU (ctx, which is selected with the if not statement above)\n",
        "        metric.add(d2l.accuracy(net(X), y), y.size)\n",
        "    return metric[0]/metric[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r6tEJYQz0j9",
        "colab_type": "text"
      },
      "source": [
        "*Load the training function you wrote yesterday*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UHZNm9fz51d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_ch5(net, train_iter, test_iter, num_epochs, lr, ctx=d2l.try_gpu()):\n",
        "  net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier()) #re-initialize the net (in case there was some manual stuff going on in between)\n",
        "  loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
        "  trainer = gluon.Trainer(net.collect_params(),'sgd', {'learning_rate': lr})\n",
        "  animator = d2l.Animator(xlabel='epoch', xlim=[0, num_epochs],legend=['train loss', 'train acc', 'test acc'])\n",
        "  timer = d2l.Timer()\n",
        "  \n",
        "  for epoch in range(num_epochs): #going through epochs\n",
        "    metric = d2l.Accumulator(3)  # 3 values: train_loss, train_acc, num_examples\n",
        "    for i, (X, y) in enumerate(train_iter): #going through examples in the batch\n",
        "      timer.start()\n",
        "      X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
        "      with autograd.record():\n",
        "        y_hat = net(X)\n",
        "        l = loss(y_hat, y)\n",
        "      l.backward()\n",
        "      trainer.step(X.shape[0])\n",
        "      metric.add(l.sum(), d2l.accuracy(y_hat, y), X.shape[0]) #adds all metrics for computing loss and accuracies (at every sample)\n",
        "      timer.stop()\n",
        "      train_loss, train_acc = metric[0]/metric[2], metric[1]/metric[2]\n",
        "      if (i+1) % 50 == 0:\n",
        "        animator.add(epoch + i/len(train_iter),(train_loss, train_acc, None)) #updates the train and los every 50 points within the batch\n",
        "    test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
        "    animator.add(epoch+1, (None, None, test_acc)) #updates the test accuracy at each epoch\n",
        "  print('loss %.3f, train acc %.3f, test acc %.3f' % (train_loss, train_acc, test_acc))\n",
        "  print('%.1f examples/sec on %s' % (metric[2]*num_epochs/timer.sum(), ctx))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lFmN6iq0MKe",
        "colab_type": "text"
      },
      "source": [
        "*Train the models with the different hyperparameters and compare the results*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqWqN8Fl0RLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# as yesterday...\n",
        "lr, num_epochs = 0.9, 10\n",
        "train_ch5(net, train_iter, test_iter, num_epochs, lr)\n",
        "\n",
        "#all yours now, use the parameters from the slide.\n",
        "#..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUwa9Ut70mv1",
        "colab_type": "text"
      },
      "source": [
        "***2. Batch Normalization***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huyqR9WU0pXU",
        "colab_type": "text"
      },
      "source": [
        "*Write the BatchNormalization function (7.5)* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da_PXCb00y8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_norm(X, gamma, beta, moving_mean,moving_var, eps, momentum):\n",
        "  # 1. check whether the model is training or not, create two cases\n",
        "  # 2. Define behavior in test\n",
        "  # 3. Define behavior in training\n",
        "  #     - check whether the input is 2D (fully conv) or 4D (normal conv layer)\n",
        "  #         - calculate avg and var in each case\n",
        "  #     - compute y_hat (the normalized values of the activations)\n",
        "  #     - update the moving mean and var for the test phase\n",
        "  #     - update activations using the linear fit (with gamma and beta)\n",
        "  # 4. return relevatn variables"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJVbRnYW1ngX",
        "colab_type": "text"
      },
      "source": [
        "*Create your BatchNorm class* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QJ8vEyk1qSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BatchNorm(nn.Block):\n",
        "\n",
        "  # you'll need 2 functions:\n",
        "  # \n",
        "  # __init__()\n",
        "  #\n",
        "  #   In this one you initialize parameters:  \n",
        "  #     - gamma as a verctor of ones\n",
        "  #     - beta as a vector of zeroes\n",
        "  #     - moving_mean and moving_var as as many zeroes as you have features\n",
        "  #\n",
        "  # forward()\n",
        "  # \n",
        "  # here you need to do 3 things: \n",
        "  #     1. make sure everything is in the same context (basycally copy the moving_mean and _var to the context where the data are)\n",
        "  #     2. run the batchnorm function you just wrote\n",
        "  #     3. return the result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO6HdGcB2hDn",
        "colab_type": "text"
      },
      "source": [
        "*Create an instance of LeNet, but this time with BatchNorm*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C81sFnzO2lLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nu53iUA2oh8",
        "colab_type": "text"
      },
      "source": [
        "*Train it*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APF3v1r52roM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let's say with lr = 0.9 and 10 epochs:\n",
        "lr, num_epochs = 0.9, 10\n",
        "\n",
        "#..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeLH8naX2vmj",
        "colab_type": "text"
      },
      "source": [
        "***3. Dropout***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMfZUdkX3HT2",
        "colab_type": "text"
      },
      "source": [
        "*Write the Dropout function (chapter 4.6)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM03dPjU3OTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dropout(X,drop_prob):"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFLFLmkq3U0H",
        "colab_type": "text"
      },
      "source": [
        "*Test that it is doing what you want*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PNe1VsS3XIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a random array\n",
        "# mask 0, 0.1, 0.5 and 1 of the coefficients. Does it work?\n",
        "\n",
        "Z = np.arange(100)#.reshape(20,5)\n",
        "print(dropout(Z,0.5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o24m4Nuy3fy9",
        "colab_type": "text"
      },
      "source": [
        "*Create your Dropout class* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnAl_Egd3l48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dropout(nn.Block):\n",
        "\n",
        "  # Here you really know the drill. There is no code from the book to help you this time :)\n",
        "  # If you're lost, you can always check out the BatchNorm class you just wrote.\n",
        "  # This one is much simpler than the BatchNorm! \n",
        "  # - In the init function you basically just declare the dropout percentage\n",
        "  # - in the forward, just remember that dropout is used only during training."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuK7zIHM4AqN",
        "colab_type": "text"
      },
      "source": [
        "*Create an instance of LeNet, but this time with BatchNorm AND dropout*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGWWQ_nu4CnU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create and instantiate the new LeNet. BN in between the convolution and the non linear activation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q2IyqRJ4Ei0",
        "colab_type": "text"
      },
      "source": [
        "*Train it!*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_66JZ8d04JEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let's say with lr = 0.9 and 10 epochs:\n",
        "lr, num_epochs = 0.9, 10\n",
        "\n",
        "# if it crashes and your errors say something about \"out of context\", \n",
        "# it is because your dropout mask is not in the GPU with the neurons' weights. \n",
        "# you just need to copyto the mask where the weights are\n",
        "# Go back to your dropout function and fix it! (as you did for the BatchNorm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S_bMxTk401L",
        "colab_type": "text"
      },
      "source": [
        "*Compare the different results obtained (all the learning curves). Can you see the effects of the different strategies (hyperparameters, BatchNorm, Dropout)?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUdB_F_u4wPO",
        "colab_type": "text"
      },
      "source": [
        "**You have made it. Congrats!!!!**"
      ]
    }
  ]
}