{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ee4db3-42df-4832-9369-4577b0c2963b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%matplotlib widget\n",
    "%autoreload 2\n",
    "from amftrack.pipeline.launching.run_super import (\n",
    "    run_launcher,\n",
    "    directory_scratch,\n",
    "    directory_project,\n",
    "    directory_project,\n",
    "    run_parallel_stitch,\n",
    "    run_parallel_transfer,\n",
    ")\n",
    "import os\n",
    "from amftrack.util.sys import (\n",
    "    get_dates_datetime,\n",
    "    get_dirname,\n",
    "    temp_path,\n",
    "    get_data_info,\n",
    "    update_plate_info,\n",
    "    update_analysis_info,\n",
    "    get_analysis_info,\n",
    "    get_current_folders,\n",
    "    get_folders_by_plate_id,\n",
    ")\n",
    "from time import time_ns\n",
    "from amftrack.util.dbx import upload_folders, load_dbx, download, get_dropbox_folders\n",
    "from datetime import datetime\n",
    "\n",
    "from amftrack.pipeline.functions.image_processing.experiment_class_surf import (\n",
    "    load_graphs,\n",
    ")\n",
    "from amftrack.pipeline.functions.post_processing.extract_study_zone import (\n",
    "    load_study_zone,\n",
    ")\n",
    "from amftrack.pipeline.functions.image_processing.experiment_util import (\n",
    "    get_random_edge,\n",
    "    distance_point_edge,\n",
    "    plot_edge,\n",
    "    plot_edge_cropped,\n",
    "    find_nearest_edge,\n",
    "    get_edge_from_node_labels,\n",
    "    plot_full_image_with_features,\n",
    "    get_all_edges,\n",
    "    get_all_nodes,\n",
    "    find_neighboring_edges,\n",
    "    reconstruct_image,\n",
    "    reconstruct_skeletton_from_edges,\n",
    "    reconstruct_skeletton_unicolor,\n",
    "    plot_edge_color_value,\n",
    "    reconstruct_image_from_general,\n",
    "    plot_full,\n",
    "    find_nearest_edge,\n",
    ")\n",
    "from amftrack.pipeline.functions.image_processing.extract_width_fun import (\n",
    "    get_width_info,\n",
    "    get_width_info_new,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from amftrack.pipeline.functions.image_processing.experiment_class_surf import (\n",
    "    Experiment,\n",
    "    save_graphs,\n",
    "    find_node_equ,\n",
    ")\n",
    "import pandas as pd\n",
    "from amftrack.pipeline.functions.spore_processing.spore_id import make_spore_data\n",
    "from amftrack.pipeline.functions.image_processing.hyphae_id_surf import (\n",
    "    resolve_anastomosis_crossing_by_root,\n",
    ")\n",
    "from amftrack.pipeline.functions.post_processing.time_hypha import *\n",
    "from amftrack.pipeline.functions.image_processing.experiment_class_surf import (\n",
    "    Node,\n",
    "    Edge,\n",
    "    Hyphae,\n",
    "    get_distance,\n",
    ")\n",
    "from amftrack.util.sys import (\n",
    "    get_analysis_folders,\n",
    "    get_time_plate_info_from_analysis,\n",
    "    get_time_hypha_info_from_analysis,\n",
    "    get_global_hypha_info_from_analysis,\n",
    ")\n",
    "\n",
    "from datetime import datetime\n",
    "from matplotlib import cm\n",
    "from amftrack.pipeline.functions.post_processing.extract_study_zone import (\n",
    "    load_study_zone,\n",
    ")\n",
    "from IPython.display import clear_output\n",
    "from amftrack.pipeline.functions.post_processing.exp_plot import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da929f66-14b2-4e0c-b4f3-91825fafb5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time(exp, t, tp1):  # redefined here to avoid loop in import\n",
    "    seconds = (exp.dates[tp1] - exp.dates[t]).total_seconds()\n",
    "    return seconds / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0dad9c-5e95-49c3-9ecb-b52b0b2ce7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plates = [\"94_20201123\"]\n",
    "directory_targ = directory_project\n",
    "directory_targ = os.path.join(directory_scratch, \"stitch_temp2\") + \"/\"\n",
    "update_analysis_info(directory_targ)\n",
    "analysis_info = get_analysis_info(directory_targ)\n",
    "analysis_folders = analysis_info.loc[analysis_info[\"unique_id\"].isin(plates)]\n",
    "# update_plate_info(directory_targ, local=True)\n",
    "# all_folders = get_current_folders(directory_targ, local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7207f37e-a7ae-416f-a1f7-a24c37d4d26e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Utils for defining environement variables and defining paths\"\"\"\n",
    "\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from amftrack.pipeline.functions.image_processing.extract_graph import (\n",
    "    sparse_to_doc,\n",
    ")\n",
    "import cv2\n",
    "import json\n",
    "import pandas as pd\n",
    "from amftrack.util.dbx import download, upload, load_dbx, env_config\n",
    "from tqdm.autonotebook import tqdm\n",
    "from time import time_ns\n",
    "from decouple import Config, RepositoryEnv\n",
    "from pymatreader import read_mat\n",
    "import shutil\n",
    "import hashlib\n",
    "\n",
    "from amftrack.util.sys import *\n",
    "\n",
    "\n",
    "def pad_number(number):\n",
    "    \"\"\"\n",
    "    Convert number to string and padd with a zero\n",
    "    Ex:\n",
    "    1 -> 01\n",
    "    23 -> 23\n",
    "    \"\"\"\n",
    "    if number < 10:\n",
    "        return f\"0{number}\"\n",
    "    else:\n",
    "        return str(number)\n",
    "\n",
    "\n",
    "def get_path(date, plate, skeleton, row=None, column=None, extension=\".mat\"):\n",
    "    root_path = (\n",
    "        r\"//sun.amolf.nl/shimizu-data/home-folder/oyartegalvez/Drive_AMFtopology/PRINCE\"\n",
    "    )\n",
    "    date_plate = f\"/2020{date}\"\n",
    "    plate = f\"_Plate{plate}\"\n",
    "    if skeleton:\n",
    "        end = \"/Analysis/Skeleton\" + extension\n",
    "    else:\n",
    "        end = \"/Img\" + f\"/Img_r{pad_number(row)}_c{pad_number(column)}.tif\"\n",
    "    return root_path + date_plate + plate + end\n",
    "\n",
    "\n",
    "def get_dates_datetime(directory, plate):\n",
    "    listdir = os.listdir(directory)\n",
    "    list_dir_interest = [\n",
    "        name\n",
    "        for name in listdir\n",
    "        if name.split(\"_\")[-1] == f'Plate{0 if plate<10 else \"\"}{plate}'\n",
    "    ]\n",
    "    ss = [name.split(\"_\")[0] for name in list_dir_interest]\n",
    "    ff = [name.split(\"_\")[1] for name in list_dir_interest]\n",
    "    dates_datetime = [\n",
    "        datetime(\n",
    "            year=int(ss[i][:4]),\n",
    "            month=int(ss[i][4:6]),\n",
    "            day=int(ss[i][6:8]),\n",
    "            hour=int(ff[i][0:2]),\n",
    "            minute=int(ff[i][2:4]),\n",
    "        )\n",
    "        for i in range(len(list_dir_interest))\n",
    "    ]\n",
    "    dates_datetime.sort()\n",
    "    return dates_datetime\n",
    "\n",
    "\n",
    "def get_dirname(date, folders):\n",
    "    select = folders.loc[folders[\"datetime\"] == date][\"folder\"]\n",
    "    assert len(select) == 1\n",
    "    return select.iloc[0]\n",
    "\n",
    "\n",
    "def shift_skeleton(skeleton, shift):\n",
    "    shifted_skeleton = sparse.dok_matrix(skeleton.shape, dtype=bool)\n",
    "    for pixel in skeleton.keys():\n",
    "        #             print(pixel[0]+shift[0],pixel[1]+shift[1])\n",
    "        if (\n",
    "            skeleton.shape[0] > np.ceil(pixel[0] + shift[0]) > 0\n",
    "            and skeleton.shape[1] > np.ceil(pixel[1] + shift[1]) > 0\n",
    "        ):\n",
    "            shifted_pixel = (\n",
    "                np.round(pixel[0] + shift[0]),\n",
    "                np.round(pixel[1] + shift[1]),\n",
    "            )\n",
    "            shifted_skeleton[shifted_pixel] = 1\n",
    "    return shifted_skeleton\n",
    "\n",
    "\n",
    "def transform_skeleton_final_for_show(skeleton_doc, Rot, trans):\n",
    "    skeleton_transformed = {}\n",
    "    transformed_keys = np.round(\n",
    "        np.transpose(np.dot(Rot, np.transpose(np.array(list(skeleton_doc.keys())))))\n",
    "        + trans\n",
    "    ).astype(np.int)\n",
    "    i = 0\n",
    "    for pixel in list(transformed_keys):\n",
    "        i += 1\n",
    "        skeleton_transformed[(pixel[0], pixel[1])] = 1\n",
    "    skeleton_transformed_sparse = sparse.lil_matrix((27000, 60000))\n",
    "    for pixel in list(skeleton_transformed.keys()):\n",
    "        i += 1\n",
    "        skeleton_transformed_sparse[(pixel[0], pixel[1])] = 1\n",
    "    return skeleton_transformed_sparse\n",
    "\n",
    "\n",
    "def get_skeleton(exp, boundaries, t, directory):\n",
    "    i = t\n",
    "    plate = exp.prince_pos\n",
    "    listdir = os.listdir(directory)\n",
    "    dates = exp.dates\n",
    "    date = dates[i]\n",
    "    directory_name = get_dirname(date, exp.folders)\n",
    "    path_snap = directory + directory_name\n",
    "    skel = read_mat(path_snap + \"/Analysis/skeleton_pruned_realigned.mat\")\n",
    "    skelet = skel[\"skeleton\"]\n",
    "    skelet = sparse_to_doc(skelet)\n",
    "    Rot = skel[\"R\"]\n",
    "    trans = skel[\"t\"]\n",
    "    skel_aligned = transform_skeleton_final_for_show(\n",
    "        skelet, np.array([[1, 0], [0, 1]]), np.array([0, 0])\n",
    "    )\n",
    "    output = skel_aligned[\n",
    "        boundaries[2] : boundaries[3], boundaries[0] : boundaries[1]\n",
    "    ].todense()\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    output = cv2.dilate(output.astype(np.uint8), kernel, iterations=2)\n",
    "    return (output, Rot, trans)\n",
    "\n",
    "\n",
    "def get_param(\n",
    "    folder, directory\n",
    "):  # Very ugly but because interfacing with Matlab so most elegant solution.\n",
    "    # TODO(FK)\n",
    "    path_snap = os.path.join(directory, folder)\n",
    "    file1 = open(os.path.join(path_snap, \"param.m\"), \"r\")\n",
    "    Lines = file1.readlines()\n",
    "    ldict = {}\n",
    "    for line in Lines:\n",
    "        to_execute = line.split(\";\")[0]\n",
    "        relation = to_execute.split(\"=\")\n",
    "        if len(relation) == 2:\n",
    "            ldict[relation[0].strip()] = relation[1].strip()\n",
    "        # exec(line.split(';')[0],globals(),ldict)\n",
    "    files = [\n",
    "        \"/Img/TileConfiguration.txt.registered\",\n",
    "        \"/Analysis/skeleton_compressed.mat\",\n",
    "        \"/Analysis/skeleton_masked_compressed.mat\",\n",
    "        \"/Analysis/skeleton_pruned_compressed.mat\",\n",
    "        \"/Analysis/transform.mat\",\n",
    "        \"/Analysis/transform_corrupt.mat\",\n",
    "        \"/Analysis/skeleton_realigned_compressed.mat\",\n",
    "        \"/Analysis/nx_graph_pruned.p\",\n",
    "        \"/Analysis/nx_graph_pruned_width.p\",\n",
    "        \"/Analysis/nx_graph_pruned_labeled.p\",\n",
    "    ]\n",
    "    for file in files:\n",
    "        ldict[file] = os.path.isfile(path_snap + file)  # TODO(FK) change here\n",
    "    return ldict\n",
    "\n",
    "\n",
    "def update_plate_info_local(directory: str) -> None:\n",
    "    \"\"\"\n",
    "    An acquisition repositorie has a param.m file inside it.\n",
    "    :param directory: full path to a directory containing acquisition directories\n",
    "    \"\"\"\n",
    "    target = env_config.get(\"DATA_PATH\")\n",
    "    listdir = os.listdir(directory)\n",
    "    info_path = os.path.join(storage_path, \"data_info.json\")\n",
    "    # TODO(FK): Crashes when there is no basic file\n",
    "    # try:\n",
    "    #     with open(target) as f:\n",
    "    #         plate_info = json.load(f)\n",
    "    # except:\n",
    "    #     s = \"/home/ipausers/kahane/Wks/AMFtrack/template_data_info.json\"\n",
    "    #     dest = os.path.join(storage_path, \"data_info.json\")\n",
    "    #     shutil.copy(s, dest)\n",
    "    #     with open(target) as f:\n",
    "    #         plate_info = json.load(f)\n",
    "    with open(info_path) as f:\n",
    "        plate_info = json.load(f)\n",
    "\n",
    "    # TOFIX\n",
    "    for folder in listdir:\n",
    "        if os.path.isfile(os.path.join(directory, folder, \"param.m\")):\n",
    "            params = get_param(folder, directory)\n",
    "            ss = folder.split(\"_\")[0]\n",
    "            ff = folder.split(\"_\")[1]\n",
    "            date = datetime(\n",
    "                year=int(ss[:4]),\n",
    "                month=int(ss[4:6]),\n",
    "                day=int(ss[6:8]),\n",
    "                hour=int(ff[0:2]),\n",
    "                minute=int(ff[2:4]),\n",
    "            )\n",
    "            params[\"date\"] = datetime.strftime(date, \"%d.%m.%Y, %H:%M:\")\n",
    "            params[\"folder\"] = folder\n",
    "            total_path = os.path.join(directory, folder)\n",
    "            plate_info[total_path] = params\n",
    "    with open(target, \"w\") as jsonf:\n",
    "        json.dump(plate_info, jsonf, indent=4)\n",
    "\n",
    "\n",
    "def update_plate_info(\n",
    "    directory: str, local=True, strong_constraint=True, suffix_data_info=\"\"\n",
    ") -> None:\n",
    "    \"\"\"*\n",
    "    1/ Download `data_info.json` file containing all information about acquisitions.\n",
    "    2/ Add all acquisition files in the `directory` path to the `data_info.json`.\n",
    "    3/ Upload the new version of data_info (actuliased) to the dropbox.\n",
    "    An acquisition repositorie has a param.m file inside it.\n",
    "    \"\"\"\n",
    "    # TODO(FK): add a local version without dropbox modification\n",
    "    listdir = os.listdir(directory)\n",
    "    source = f\"/data_info.json\"\n",
    "    target = os.path.join(temp_path, f\"data_info{suffix_data_info}.json\")\n",
    "    if local:\n",
    "        plate_info = {}\n",
    "    else:\n",
    "        download(source, target, end=\"\")\n",
    "        plate_info = json.load(open(target, \"r\"))\n",
    "    with tqdm(total=len(listdir), desc=\"analysed\") as pbar:\n",
    "        for folder in listdir:\n",
    "            path_snap = os.path.join(directory, folder)\n",
    "            if os.path.exists(os.path.join(path_snap, \"Img\")):\n",
    "                sub_list_files = os.listdir(os.path.join(path_snap, \"Img\"))\n",
    "                is_real_folder = os.path.isfile(os.path.join(path_snap, \"param.m\"))\n",
    "                if strong_constraint:\n",
    "                    is_real_folder *= (\n",
    "                        os.path.isfile(\n",
    "                            os.path.join(path_snap, \"Img\", \"Img_r03_c05.tif\")\n",
    "                        )\n",
    "                        * len(sub_list_files)\n",
    "                        >= 100\n",
    "                    )\n",
    "                if is_real_folder:\n",
    "                    params = get_param(folder, directory)\n",
    "                    ss = folder.split(\"_\")[0]\n",
    "                    ff = folder.split(\"_\")[1]\n",
    "                    date = datetime(\n",
    "                        year=int(ss[:4]),\n",
    "                        month=int(ss[4:6]),\n",
    "                        day=int(ss[6:8]),\n",
    "                        hour=int(ff[0:2]),\n",
    "                        minute=int(ff[2:4]),\n",
    "                    )\n",
    "                    params[\"date\"] = datetime.strftime(date, \"%d.%m.%Y, %H:%M:\")\n",
    "                    params[\"folder\"] = folder\n",
    "                    total_path = os.path.join(directory, folder)\n",
    "                    plate_info[total_path] = params\n",
    "            pbar.update(1)\n",
    "    with open(target, \"w\") as jsonf:\n",
    "        json.dump(plate_info, jsonf, indent=4)\n",
    "    if not local:\n",
    "        upload(target, f\"{source}\", chunk_size=256 * 1024 * 1024)\n",
    "\n",
    "\n",
    "def get_data_info(local=False, suffix_data_info=\"\"):\n",
    "    source = f\"/data_info.json\"\n",
    "    target = os.path.join(temp_path, f\"data_info{suffix_data_info}.json\")\n",
    "\n",
    "    if not local:\n",
    "        download(source, target, end=\"\")\n",
    "    data_info = pd.read_json(target, convert_dates=True).transpose()\n",
    "    if len(data_info) > 0:\n",
    "        data_info.index.name = \"total_path\"\n",
    "        data_info.reset_index(inplace=True)\n",
    "        data_info[\"unique_id\"] = (\n",
    "            data_info[\"Plate\"].astype(str).astype(int).astype(str)\n",
    "            + \"_\"\n",
    "            + data_info[\"CrossDate\"].str.replace(\"'\", \"\").astype(str)\n",
    "        )\n",
    "\n",
    "        data_info[\"datetime\"] = pd.to_datetime(\n",
    "            data_info[\"date\"], format=\"%d.%m.%Y, %H:%M:\"\n",
    "        )\n",
    "    return data_info\n",
    "\n",
    "\n",
    "def get_current_folders_local(directory: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    :param directory: full path to a directory containing acquisition files\n",
    "    \"\"\"\n",
    "    plate_info = pd.read_json(\n",
    "        os.path.join(storage_path, \"data_info.json\"), convert_dates=True\n",
    "    ).transpose()\n",
    "    plate_info.index.name = \"total_path\"\n",
    "    plate_info.reset_index(inplace=True)\n",
    "    listdir = os.listdir(directory)\n",
    "    selected_df = plate_info.loc[\n",
    "        np.isin(plate_info[\"folder\"], listdir)  # folder exist\n",
    "        & (\n",
    "            plate_info[\"total_path\"]\n",
    "            == plate_info[\"folder\"].apply(\n",
    "                lambda x: os.path.join(directory, x)\n",
    "            )  # folder is registered\n",
    "        )\n",
    "    ]\n",
    "    return selected_df\n",
    "\n",
    "\n",
    "def get_current_folders(\n",
    "    directory: str, local=True, suffix_data_info=\"\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a pandas data frame with all informations about the acquisition files\n",
    "    inside the directory.\n",
    "    WARNING: directory must finish with '/'\n",
    "    \"\"\"\n",
    "    # TODO(FK): solve the / problem\n",
    "    plate_info = get_data_info(local, suffix_data_info)\n",
    "    listdir = os.listdir(directory)\n",
    "    if len(plate_info) > 0:\n",
    "        return plate_info.loc[\n",
    "            np.isin(plate_info[\"folder\"], listdir)\n",
    "            & (plate_info[\"total_path\"] == directory + plate_info[\"folder\"])\n",
    "        ]\n",
    "    else:\n",
    "        return plate_info\n",
    "\n",
    "\n",
    "def get_folders_by_plate_id(plate_id, begin=0, end=-1, directory=None):\n",
    "    data_info = get_data_info() if directory is None else get_current_folders(directory)\n",
    "    folders = data_info.loc[\n",
    "        10**8 * data_info[\"Plate\"] + data_info[\"CrossDate\"] == plate_id\n",
    "    ]\n",
    "    dates_datetime = [\n",
    "        datetime.strptime(row[\"date\"], \"%d.%m.%Y, %H:%M:\")\n",
    "        for index, row in folders.iterrows()\n",
    "    ]\n",
    "    dates_datetime.sort()\n",
    "    dates_datetime_select = dates_datetime[begin:end]\n",
    "    dates_str = [\n",
    "        datetime.strftime(date, \"%d.%m.%Y, %H:%M:\") for date in dates_datetime_select\n",
    "    ]\n",
    "    select_folders = folders.loc[np.isin(folders[\"date\"], dates_str)]\n",
    "    return select_folders\n",
    "\n",
    "\n",
    "def update_analysis_info(directory, suffix_analysis_info=\"\"):\n",
    "    listdir = os.listdir(directory)\n",
    "    analysis_dir = [fold for fold in listdir if fold.split(\"_\")[0] == \"Analysis\"]\n",
    "    infos_analysed = {}\n",
    "    for folder in analysis_dir:\n",
    "        metadata = {}\n",
    "        version = folder.split(\"_\")[-1]\n",
    "        op_id = int(folder.split(\"_\")[-2])\n",
    "        dt = datetime.fromtimestamp(op_id // 1000000000)\n",
    "        path = f\"{directory}{folder}/folder_info.json\"\n",
    "        if os.path.exists(path):\n",
    "            infos = pd.read_json(path, dtype={\"unique_id\": str})\n",
    "            if len(infos) > 0:\n",
    "                column_interest = [\n",
    "                    column for column in infos.columns if column[0] != \"/\"\n",
    "                ]\n",
    "                metadata[\"version\"] = version\n",
    "                for column in column_interest:\n",
    "                    if column != \"folder\":\n",
    "                        info = str(infos[column].iloc[0])\n",
    "                        metadata[column] = info\n",
    "                metadata[\"date_begin\"] = datetime.strftime(\n",
    "                    infos[\"datetime\"].iloc[0], \"%d.%m.%Y, %H:%M:\"\n",
    "                )\n",
    "                metadata[\"date_end\"] = datetime.strftime(\n",
    "                    infos[\"datetime\"].iloc[-1], \"%d.%m.%Y, %H:%M:\"\n",
    "                )\n",
    "                metadata[\"number_timepoints\"] = len(infos)\n",
    "                metadata[\"path_exp\"] = f\"{folder}/experiment.pick\"\n",
    "                metadata[\"path_global_hypha_info\"] = f\"{folder}/global_hypha_info.json\"\n",
    "                metadata[\"path_time_hypha_info\"] = f\"{folder}/time_hypha_info\"\n",
    "                metadata[\"path_time_plate_info\"] = f\"{folder}/time_plate_info.json\"\n",
    "                metadata[\"path_global_plate_info\"] = f\"{folder}/global_plate_info.json\"\n",
    "                metadata[\"date_run_analysis\"] = datetime.strftime(\n",
    "                    dt, \"%d.%m.%Y, %H:%M:\"\n",
    "                )\n",
    "                infos_analysed[folder] = metadata\n",
    "        else:\n",
    "            print(folder)\n",
    "    target = os.path.join(temp_path, f\"analysis_info{suffix_analysis_info}.json\")\n",
    "\n",
    "    with open(target, \"w\") as jsonf:\n",
    "        json.dump(infos_analysed, jsonf, indent=4)\n",
    "\n",
    "\n",
    "def get_analysis_info(directory, suffix_analysis_info=\"\"):\n",
    "    target = os.path.join(temp_path, f\"analysis_info{suffix_analysis_info}.json\")\n",
    "\n",
    "    analysis_info = pd.read_json(target, convert_dates=True).transpose()\n",
    "    analysis_info.index.name = \"folder_analysis\"\n",
    "    analysis_info.reset_index(inplace=True)\n",
    "    return analysis_info\n",
    "\n",
    "\n",
    "def get_analysis_folders(path=dropbox_path):\n",
    "    analysis_folders = pd.DataFrame()\n",
    "    for dire in os.walk(path):\n",
    "        name_analysis = dire[0].split(os.sep)[-1].split(\"_\")\n",
    "        if name_analysis[0] == \"Analysis\":\n",
    "            analysis_dir = dire[0]\n",
    "            path_save = os.path.join(analysis_dir, \"folder_info.json\")\n",
    "            if os.path.exists(path_save):\n",
    "                folders_plate = pd.read_json(path_save)\n",
    "                infos = folders_plate.iloc[0][1:10]\n",
    "                infos[\"total_path\"] = analysis_dir\n",
    "                infos[\"time_plate\"] = os.path.isfile(\n",
    "                    os.path.join(analysis_dir, \"time_plate_info.json\")\n",
    "                )\n",
    "                infos[\"global_hypha\"] = os.path.isfile(\n",
    "                    os.path.join(analysis_dir, \"global_hypha_info.json\")\n",
    "                )\n",
    "                infos[\"time_hypha\"] = os.path.isdir(\n",
    "                    os.path.join(analysis_dir, \"time_hypha_info.json\")\n",
    "                )\n",
    "\n",
    "                infos[\"num_folders\"] = len(folders_plate)\n",
    "                analysis_folders = pd.concat([analysis_folders, infos], axis=1)\n",
    "\n",
    "    analysis_folders = analysis_folders.transpose().reset_index().drop(\"index\", axis=1)\n",
    "    analysis_folders[\"unique_id\"] = (\n",
    "        analysis_folders[\"Plate\"].astype(str)\n",
    "        + \"_\"\n",
    "        + analysis_folders[\"CrossDate\"].astype(str).str.replace(\"'\", \"\")\n",
    "    )\n",
    "    return analysis_folders\n",
    "\n",
    "\n",
    "def get_time_plate_info_from_analysis(analysis_folders, use_saved=True):\n",
    "    plates_in = analysis_folders[\"unique_id\"].unique()\n",
    "    plates_in.sort()\n",
    "    ide = hashlib.sha256(np.sum(plates_in).encode(\"utf-8\")).hexdigest()\n",
    "    path_save_info = os.path.join(temp_path, f\"time_plate_info_{ide}\")\n",
    "    path_save_folders = os.path.join(temp_path, f\"folders_{ide}\")\n",
    "\n",
    "    if os.path.exists(path_save_info) and use_saved:\n",
    "        time_plate_info = pd.read_json(path_save_info)\n",
    "        folders = pd.read_json(path_save_folders)\n",
    "        return (folders, time_plate_info)\n",
    "    analysis_dirs = analysis_folders[\"total_path\"]\n",
    "    time_plate_info = pd.DataFrame()\n",
    "    folders = pd.DataFrame()\n",
    "    for analysis_dir in analysis_dirs:\n",
    "        path_save = os.path.join(analysis_dir, \"time_plate_info.json\")\n",
    "        table = pd.read_json(path_save)\n",
    "        table = table.transpose()\n",
    "        table = table.fillna(-1)\n",
    "        path_save = os.path.join(analysis_dir, \"folder_info.json\")\n",
    "        folders_plate = pd.read_json(path_save)\n",
    "        folders_plate = folders_plate.reset_index()\n",
    "        table = pd.concat(\n",
    "            (table, (folders_plate[\"datetime\"] - folders_plate[\"datetime\"].iloc[0])),\n",
    "            axis=1,\n",
    "        )\n",
    "        table = table.rename(columns={\"datetime\": \"time_since_begin\"})\n",
    "        table[\"time_since_begin_h\"] = table[\"time_since_begin\"].copy() / np.timedelta64(\n",
    "            1, \"h\"\n",
    "        )\n",
    "        table = pd.concat((table, pd.DataFrame(folders_plate.index.values)), axis=1)\n",
    "        table = table.rename(columns={0: \"timestep\"})\n",
    "        table = pd.concat((table, (folders_plate[\"folder\"])), axis=1)\n",
    "        table = pd.concat((table, (folders_plate[\"unique_id\"])), axis=1)\n",
    "        table = pd.concat((table, (folders_plate[\"datetime\"])), axis=1)\n",
    "        for column in [\"PrincePos\", \"root\", \"strain\", \"medium\"]:\n",
    "            try:\n",
    "                table = pd.concat((table, (folders_plate[column])), axis=1)\n",
    "            except KeyError:\n",
    "                continue\n",
    "        time_plate_info = pd.concat([time_plate_info, table], ignore_index=True)\n",
    "        folders = pd.concat(\n",
    "            [folders.copy(), folders_plate.copy()], axis=0, ignore_index=True\n",
    "        )\n",
    "    time_plate_info.to_json(path_save_info)\n",
    "    folders.to_json(path_save_folders)\n",
    "    return (folders, time_plate_info)\n",
    "\n",
    "\n",
    "def get_global_hypha_info_from_analysis(analysis_folders, use_saved=True):\n",
    "    plates_in = analysis_folders[\"unique_id\"].unique()\n",
    "    plates_in.sort()\n",
    "    ide = hashlib.sha256(np.sum(plates_in).encode(\"utf-8\")).hexdigest()\n",
    "    path_save_info = os.path.join(temp_path, f\"global_hypha_info_{ide}\")\n",
    "    path_save_folders = os.path.join(temp_path, f\"folders_{ide}\")\n",
    "\n",
    "    if os.path.exists(path_save_info) and use_saved:\n",
    "        global_hypha_info = pd.read_json(path_save_info)\n",
    "        folders = pd.read_json(path_save_folders)\n",
    "        return (folders, global_hypha_info)\n",
    "    analysis_dirs = analysis_folders[\"total_path\"]\n",
    "    global_hypha_info = pd.DataFrame()\n",
    "    folders = pd.DataFrame()\n",
    "    for analysis_dir in analysis_dirs:\n",
    "        path_save = os.path.join(analysis_dir, \"global_hypha_info.json\")\n",
    "        if os.path.exists(path_save):\n",
    "            table = pd.read_json(path_save)\n",
    "            table = table.transpose()\n",
    "            table = table.fillna(-1)\n",
    "            path_save = os.path.join(analysis_dir, \"folder_info.json\")\n",
    "            folders_plate = pd.read_json(path_save)\n",
    "            folders_plate = folders_plate.reset_index()\n",
    "            table = table.reset_index()\n",
    "            table[\"unique_id\"] = folders_plate[\"unique_id\"].iloc[0]\n",
    "            table[\"PrincePos\"] = folders_plate[\"PrincePos\"].iloc[0]\n",
    "            table[\"root\"] = folders_plate[\"root\"].iloc[0]\n",
    "            table[\"strain\"] = folders_plate[\"strain\"].iloc[0]\n",
    "            table[\"medium\"] = folders_plate[\"medium\"].iloc[0]\n",
    "            global_hypha_info = pd.concat([global_hypha_info, table], ignore_index=True)\n",
    "            folders = pd.concat(\n",
    "                [folders.copy(), folders_plate.copy()], axis=0, ignore_index=True\n",
    "            )\n",
    "    global_hypha_info.to_json(path_save_info)\n",
    "    folders.to_json(path_save_folders)\n",
    "    return (folders, global_hypha_info)\n",
    "\n",
    "\n",
    "def get_time_hypha_info_from_analysis(analysis_folders, use_saved=True):\n",
    "    plates_in = analysis_folders[\"unique_id\"].unique()\n",
    "    plates_in.sort()\n",
    "    ide = hashlib.sha256(np.sum(plates_in).encode(\"utf-8\")).hexdigest()\n",
    "    path_save_info = os.path.join(temp_path, f\"time_hypha_info_{ide}\")\n",
    "    path_save_folders = os.path.join(temp_path, f\"folders_{ide}\")\n",
    "\n",
    "    if os.path.exists(path_save_info) and use_saved:\n",
    "        time_hypha_infos = pd.read_json(path_save_info)\n",
    "        folders = pd.read_json(path_save_folders)\n",
    "        return (folders, time_hypha_infos)\n",
    "    analysis_dirs = analysis_folders[\"total_path\"]\n",
    "    folders = pd.DataFrame()\n",
    "    time_hypha_infos = []\n",
    "    for analysis_dir in analysis_dirs:\n",
    "        path_time_hypha = os.path.join(analysis_dir, \"time_hypha_info\")\n",
    "        if os.path.exists(path_time_hypha):\n",
    "            path_save = os.path.join(analysis_dir, \"folder_info.json\")\n",
    "            folders_plate = pd.read_json(path_save)\n",
    "            folders_plate = folders_plate.reset_index()\n",
    "            folders_plate = folders_plate.sort_values(\"datetime\")\n",
    "            json_paths = os.listdir(path_time_hypha)\n",
    "            tables = []\n",
    "            for path in json_paths:\n",
    "                print(path)\n",
    "                index = int(path.split(\"_\")[-1].split(\".\")[0])\n",
    "                line = folders_plate.iloc[index]\n",
    "                try:\n",
    "                    table = pd.read_json(os.path.join(path_time_hypha, path))\n",
    "                except:\n",
    "                    print(os.path.join(path_time_hypha, path))\n",
    "                    continue\n",
    "                table = table.transpose()\n",
    "                table = table.fillna(-1)\n",
    "                table[\"time_since_begin_h\"] = (\n",
    "                    line[\"datetime\"] - folders_plate[\"datetime\"].iloc[0]\n",
    "                )\n",
    "                table[\"folder\"] = line[\"folder\"]\n",
    "                table[\"Plate\"] = line[\"Plate\"]\n",
    "                table[\"unique_id\"] = line[\"unique_id\"]\n",
    "                table[\"datetime\"] = line[\"datetime\"]\n",
    "                table[\"PrincePos\"] = line[\"PrincePos\"]\n",
    "                table[\"root\"] = line[\"root\"]\n",
    "                table[\"strain\"] = line[\"strain\"]\n",
    "                table[\"medium\"] = line[\"medium\"]\n",
    "                tables.append(table)\n",
    "            time_hypha_info_plate = pd.concat(tables, axis=0, ignore_index=True)\n",
    "            time_hypha_info_plate.reset_index(inplace=True, drop=True)\n",
    "            time_hypha_infos.append(time_hypha_info_plate)\n",
    "            folders = pd.concat([folders, folders_plate], axis=0, ignore_index=True)\n",
    "    time_hypha_info = pd.concat(time_hypha_infos, axis=0, ignore_index=True)\n",
    "    time_hypha_info.to_json(path_save_info)\n",
    "    folders.to_json(path_save_folders)\n",
    "    return (folders, time_hypha_info)\n",
    "\n",
    "\n",
    "def get_time_edge_info_from_analysis(analysis_folders, use_saved=True):\n",
    "    plates_in = analysis_folders[\"unique_id\"].unique()\n",
    "    plates_in.sort()\n",
    "    ide = hashlib.sha256(np.sum(plates_in).encode(\"utf-8\")).hexdigest()\n",
    "    path_save_info = os.path.join(temp_path, f\"time_edge_info_{ide}\")\n",
    "    path_save_folders = os.path.join(temp_path, f\"folders_{ide}\")\n",
    "\n",
    "    if os.path.exists(path_save_info) and use_saved:\n",
    "        time_edge_infos = pd.read_json(path_save_info)\n",
    "        folders = pd.read_json(path_save_folders)\n",
    "        return (folders, time_edge_infos)\n",
    "    analysis_dirs = analysis_folders[\"total_path\"]\n",
    "    folders = pd.DataFrame()\n",
    "    time_edge_infos = []\n",
    "    for analysis_dir in analysis_dirs:\n",
    "        path_time_edge = os.path.join(analysis_dir, \"time_edge_info\")\n",
    "        if os.path.exists(path_time_edge):\n",
    "            path_save = os.path.join(analysis_dir, \"folder_info.json\")\n",
    "            folders_plate = pd.read_json(path_save)\n",
    "            folders_plate = folders_plate.reset_index()\n",
    "            folders_plate = folders_plate.sort_values(\"datetime\")\n",
    "            json_paths = os.listdir(path_time_edge)\n",
    "            tables = []\n",
    "            for path in json_paths:\n",
    "                index = int(path.split(\"_\")[-1].split(\".\")[0])\n",
    "                line = folders_plate.iloc[index]\n",
    "                try:\n",
    "                    table = pd.read_json(os.path.join(path_time_edge, path))\n",
    "                except:\n",
    "                    print(os.path.join(path_time_edge, path))\n",
    "                    continue\n",
    "                table = table.transpose()\n",
    "                table = table.fillna(-1)\n",
    "                table[\"time_since_begin_h\"] = (\n",
    "                    line[\"datetime\"] - folders_plate[\"datetime\"].iloc[0]\n",
    "                )\n",
    "                table[\"folder\"] = line[\"folder\"]\n",
    "                table[\"Plate\"] = line[\"Plate\"]\n",
    "                table[\"unique_id\"] = line[\"unique_id\"]\n",
    "                table[\"datetime\"] = line[\"datetime\"]\n",
    "                for column in [\"PrincePos\", \"root\", \"strain\", \"medium\"]:\n",
    "                    try:\n",
    "                        table[column] = line[column]\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "\n",
    "                tables.append(table)\n",
    "            time_edge_info_plate = pd.concat(tables, axis=0, ignore_index=True)\n",
    "            time_edge_info_plate.reset_index(inplace=True, drop=True)\n",
    "            time_edge_infos.append(time_edge_info_plate)\n",
    "            folders = pd.concat([folders, folders_plate], axis=0, ignore_index=True)\n",
    "    time_edge_info = pd.concat(time_edge_infos, axis=0, ignore_index=True)\n",
    "    time_edge_info.to_json(path_save_info)\n",
    "    folders.to_json(path_save_folders)\n",
    "    return (folders, time_edge_info)\n",
    "\n",
    "\n",
    "def get_time_plate_info_long_from_analysis(analysis_folders, use_saved=True):\n",
    "    plates_in = analysis_folders[\"unique_id\"].unique()\n",
    "    plates_in.sort()\n",
    "    ide = hashlib.sha256(np.sum(plates_in).encode(\"utf-8\")).hexdigest()\n",
    "    path_save_info = os.path.join(temp_path, f\"time_plate_info_long_{ide}\")\n",
    "    path_save_folders = os.path.join(temp_path, f\"folders_{ide}\")\n",
    "\n",
    "    if os.path.exists(path_save_info) and use_saved:\n",
    "        time_plate_infos = pd.read_json(path_save_info)\n",
    "        folders = pd.read_json(path_save_folders)\n",
    "        return (folders, time_plate_infos)\n",
    "    analysis_dirs = analysis_folders[\"total_path\"]\n",
    "    folders = pd.DataFrame()\n",
    "    time_plate_infos = []\n",
    "    for analysis_dir in analysis_dirs:\n",
    "        path_time_plate = os.path.join(analysis_dir, \"time_plate_info_long\")\n",
    "        if os.path.exists(path_time_plate):\n",
    "            path_save = os.path.join(analysis_dir, \"folder_info.json\")\n",
    "            folders_plate = pd.read_json(path_save)\n",
    "            folders_plate = folders_plate.reset_index()\n",
    "            folders_plate = folders_plate.sort_values(\"datetime\")\n",
    "            json_paths = os.listdir(path_time_plate)\n",
    "            tables = []\n",
    "            for path in json_paths:\n",
    "                index = int(path.split(\"_\")[-1].split(\".\")[0])\n",
    "                line = folders_plate.iloc[index]\n",
    "                try:\n",
    "                    table = pd.read_json(\n",
    "                        os.path.join(path_time_plate, path), orient=\"index\"\n",
    "                    )\n",
    "                except:\n",
    "                    print(os.path.join(path_time_plate, path))\n",
    "                    continue\n",
    "                table = table.transpose()\n",
    "                table = table.fillna(-1)\n",
    "                table[\"time_since_begin_h\"] = (\n",
    "                    line[\"datetime\"] - folders_plate[\"datetime\"].iloc[0]\n",
    "                )\n",
    "                table[\"folder\"] = line[\"folder\"]\n",
    "                table[\"Plate\"] = line[\"Plate\"]\n",
    "                table[\"unique_id\"] = line[\"unique_id\"]\n",
    "                table[\"datetime\"] = line[\"datetime\"]\n",
    "                for column in [\"PrincePos\", \"root\", \"strain\", \"medium\"]:\n",
    "                    try:\n",
    "                        table[column] = line[column]\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "\n",
    "                tables.append(table)\n",
    "            time_plate_info_plate = pd.concat(tables, axis=0, ignore_index=True)\n",
    "            time_plate_info_plate = time_plate_info_plate.sort_values(\"datetime\")\n",
    "            time_plate_info_plate.reset_index(inplace=True, drop=True)\n",
    "            time_plate_info_plate[\"timestep\"] = time_plate_info_plate.index\n",
    "\n",
    "            time_plate_infos.append(time_plate_info_plate)\n",
    "            folders = pd.concat([folders, folders_plate], axis=0, ignore_index=True)\n",
    "    time_plate_info = pd.concat(time_plate_infos, axis=0, ignore_index=True)\n",
    "    time_plate_info.to_json(path_save_info)\n",
    "    folders.to_json(path_save_folders)\n",
    "    return (folders, time_plate_info)\n",
    "\n",
    "\n",
    "def get_data_tables(op_id=time_ns(), redownload=True):\n",
    "    API = str(np.load(os.getenv(\"HOME\") + \"/pycode/API_drop.npy\"))\n",
    "    dir_drop = \"data_tables\"\n",
    "    root = os.getenv(\"TEMP\")\n",
    "    # op_id = time_ns()\n",
    "    if redownload:\n",
    "        path_save = f\"{root}global_hypha_info{op_id}.pick\"\n",
    "        download(f\"/{dir_drop}/global_hypha_infos.pick\", path_save)\n",
    "        path_save = f\"{root}time_plate_infos{op_id}.pick\"\n",
    "        download(f\"/{dir_drop}/time_plate_infos.pick\", path_save)\n",
    "        path_save = f\"{root}time_hypha_info{op_id}.pick\"\n",
    "        download(f\"/{dir_drop}/time_hypha_infos.pick\", path_save)\n",
    "    path_save = f\"{root}time_plate_infos{op_id}.pick\"\n",
    "    time_plate_info = pd.read_pickle(path_save)\n",
    "    path_save = f\"{root}global_hypha_info{op_id}.pick\"\n",
    "    global_hypha_info = pd.read_pickle(path_save)\n",
    "    path_save = f\"{root}time_hypha_info{op_id}.pick\"\n",
    "    time_hypha_info = pd.read_pickle(path_save)\n",
    "    return (time_plate_info, global_hypha_info, time_hypha_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a640f6-ea4e-4577-b74d-b2496f1566ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_analysis_folders = get_analysis_folders(directory_targ)\n",
    "analysis_folders_info = all_analysis_folders.loc[\n",
    "    all_analysis_folders[\"unique_id\"].isin(plates)\n",
    "]\n",
    "folders, time_plate_info = get_time_plate_info_from_analysis(\n",
    "    analysis_folders_info, use_saved=False\n",
    ")\n",
    "folders, time_hypha_info = get_time_hypha_info_from_analysis(\n",
    "    analysis_folders_info, use_saved=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac376c-d742-45c0-ba65-6e5888454f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_exp = f'{directory_targ}{analysis_folders[\"path_exp\"].iloc[0]}'\n",
    "exp = pickle.load(open(path_exp, \"rb\"))\n",
    "exp.save_location = \"/\"\n",
    "try:\n",
    "    exp.labeled\n",
    "except AttributeError:\n",
    "    exp.labeled = True\n",
    "load_graphs(exp, directory_targ, indexes=range(90, 92))\n",
    "# load_graphs(exp, directory_targ, indexes=[90, 70, 60])\n",
    "\n",
    "load_study_zone(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc4b71-a67a-4e47-a55c-b56ee38a373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(exp.ts):\n",
    "    exp.load_tile_information(t)\n",
    "im = exp.get_image(0, 0)\n",
    "exp.dimX_dimY = im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b16755c-a64f-4c04-9c5e-f42a6ff65893",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_hyph = [0, 6, 118, 58, 1, 85]\n",
    "# list_hyph = [521770,535353,509730]\n",
    "# list_hyph = [509730]\n",
    "# list_hyph = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9672ff59-cc77-46b4-a284-b1b4b04a4035",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyph = [hyph for hyph in exp.hyphaes if hyph.end.label == 0][0]\n",
    "hyph.root = Node(44, exp)\n",
    "hyph = [hyph for hyph in exp.hyphaes if hyph.end.label == 6][0]\n",
    "hyph.root = Node(147, exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e1933b-3e12-4576-b62a-33b94bcdf0a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf = 90\n",
    "\n",
    "# list_hyph = [hyph for hyph in exp.hyphaes if hyph.end.is_in(hyph.ts[-1]) and np.linalg.norm(hyph.end.pos(hyph.ts[-1])-hyph.get_root(hyph.ts[-1]).pos(hyph.ts[-1]))>=1e3]\n",
    "list_hyph_obj = [\n",
    "    hyph\n",
    "    for hyph in exp.hyphaes\n",
    "    if hyph.end.is_in(tf) and len(hyph.root.ts()) >= 1\n",
    "    # and np.linalg.norm(hyph.end.pos(tf) - hyph.get_root(tf).pos(tf)) >= 1e4\n",
    "    and len(hyph.end.ts()) > 3\n",
    "    and np.linalg.norm(hyph.end.pos(hyph.end.ts()[0]) - hyph.end.pos(hyph.end.ts()[-1]))\n",
    "    >= 1000\n",
    "]\n",
    "list_hyph_tot = [hyph.end.label for hyph in list_hyph_obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf34e2f3-f955-425f-81e8-a45922fd2c1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_hyph = list_hyph_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ab6ab-cef1-43f6-a6c1-777efc23d17a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_hyph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4537776-fe3f-42f3-b5f9-85f41435a2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "list_hyph = [choice(list_hyph_tot)]\n",
    "list_hyph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d75f35-2dea-4fae-9343-8e1e6437372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length_fast(edge, t):\n",
    "    return np.linalg.norm(edge.begin.pos(t) - edge.end.pos(t))\n",
    "\n",
    "\n",
    "fbas = lambda edge: np.log(\n",
    "    edge.width(edge.ts()[-1])\n",
    "    * edge.length_um(edge.ts()[-1])\n",
    "    * edge.end.degree(edge.ts()[-1])\n",
    "    * edge.begin.degree(edge.ts()[-1])\n",
    ")  # function to evaluate BASness, typical threshold is 10\n",
    "f = lambda edge: np.log(\n",
    "    edge.width(edge.ts()[-1])\n",
    "    * get_length_fast(edge, edge.ts()[-1])\n",
    "    * edge.end.degree(edge.ts()[-1])\n",
    "    * edge.begin.degree(edge.ts()[-1])\n",
    ")  # function to evaluate BASness, typical threshold is 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e68e18-bdc1-4ea9-ad53-90bd94df0a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "actual_list_hyph = []\n",
    "all_distances = []\n",
    "all_time_distances = []\n",
    "tot_junctions = []\n",
    "tot_lengths = []\n",
    "tot_speeds = []\n",
    "for end in list_hyph:\n",
    "    hyph_label = end\n",
    "    hyph = [hyph for hyph in exp.hyphaes if hyph.end.label == end][0]\n",
    "    t0 = hyph.ts[0]\n",
    "    if hyph.end.is_in(t0):\n",
    "        exp = hyph.experiment\n",
    "        thresh = 1600\n",
    "        junctions_found = [hyph.end.neighbours(t0)[0]]\n",
    "        ts = []\n",
    "        mult = []\n",
    "        speeds = []\n",
    "        print(end)\n",
    "        select = time_hypha_info.loc[time_hypha_info[\"end\"] == end]\n",
    "\n",
    "        for t in hyph.ts:\n",
    "            select_t = select.loc[select[\"timestep\"] == t - 1]\n",
    "            speed = select_t[\"speed\"].iloc[0] if len(select_t) > 0 else -1\n",
    "            if t < tf:\n",
    "                try:\n",
    "                    G = exp.nx_graph[t]\n",
    "                    G = G.subgraph(nx.node_connected_component(G, hyph.end.label))\n",
    "                    tim = hyph.end.ts()[-1]\n",
    "                    tip = hyph.end\n",
    "                    if (\n",
    "                        hyph.end.degree(t) == 1\n",
    "                        and np.linalg.norm(tip.pos(tim) - tip.pos(t)) >= 40\n",
    "                    ):\n",
    "                        tot_speeds.append(speed)\n",
    "                        nodes, edges = hyph.get_nodes_within(t)\n",
    "                        potentials = []\n",
    "                        nodes = [Node(node, exp) for node in nodes]\n",
    "                        try:\n",
    "                            last_junction_index = nodes.index(junctions_found[-1])\n",
    "                        except:\n",
    "                            last_junction_index = 0\n",
    "                        for node in nodes[last_junction_index + 1 : -1]:\n",
    "                            dist = np.linalg.norm(node.pos(t) - hyph.end.pos(t))\n",
    "                            # To avoid detecting two times the same  node with different labels\n",
    "                            dists_junction_found = [np.inf] + [\n",
    "                                np.linalg.norm(node.pos(t) - nodo.pos(t))\n",
    "                                for nodo in junctions_found\n",
    "                                if nodo.is_in(t)\n",
    "                            ]\n",
    "                            if (\n",
    "                                dist < thresh\n",
    "                                and min(dists_junction_found) > 40\n",
    "                                and (node not in junctions_found)\n",
    "                            ):\n",
    "                                extra_hypha_neighbours = [\n",
    "                                    nodo\n",
    "                                    for nodo in node.neighbours(t)\n",
    "                                    if nodo not in nodes\n",
    "                                ]\n",
    "\n",
    "                                edges = [\n",
    "                                    Edge(node, nodo, exp)\n",
    "                                    for nodo in extra_hypha_neighbours\n",
    "                                ]\n",
    "                                is_rh = [f(edge) >= 10 for edge in edges]\n",
    "                                tips = [\n",
    "                                    nodo\n",
    "                                    for nodo in extra_hypha_neighbours\n",
    "                                    if nx.edge_connectivity(G, nodo.label, node.label)\n",
    "                                    == 1\n",
    "                                ]\n",
    "                                # if len(tips) == node.degree(t) - 2:\n",
    "\n",
    "                                if len(tips) == node.degree(t) - 2 and np.any(is_rh):\n",
    "                                    junctions_found.append(node)\n",
    "                                    mult.append(node.degree(t) - 2)\n",
    "                                    ts.append(t)\n",
    "                                    speeds.append(speed)\n",
    "                # except nx.exception.NetworkXNoPath:\n",
    "                except:\n",
    "\n",
    "                    print(hyph, t)\n",
    "\n",
    "        try:\n",
    "            junctions_found_tf = [find_node_equ(node, tf) for node in junctions_found]\n",
    "            junctions_found_tf = [\n",
    "                node\n",
    "                for node in junctions_found_tf\n",
    "                if nx.has_path(\n",
    "                    exp.nx_graph[tf], node.label, junctions_found_tf[0].label\n",
    "                )\n",
    "            ]\n",
    "            junctions_found_label = [node.label for node in junctions_found_tf]\n",
    "            t_real = [get_time(exp, t0, t) for t in ts]\n",
    "            distances = [\n",
    "                get_distance(junctions_found_tf[i], junctions_found_tf[i + 1], tf)\n",
    "                for i in range(1, len(junctions_found_tf) - 1)\n",
    "            ]\n",
    "            tot_lengths.append(get_distance(junctions_found_tf[0], hyph.end, tf))\n",
    "            time_distances = [\n",
    "                distance / speeds[i + 1] for i, distance in enumerate(distances)\n",
    "            ]\n",
    "            all_distances += distances\n",
    "            all_time_distances += time_distances\n",
    "            tot_junctions += mult\n",
    "            np.save(f\"branches/ts_{hyph_label}_auto_rh\", t_real)\n",
    "            np.save(f\"branches/distances_{hyph_label}_auto_rh\", distances)\n",
    "            np.save(f\"branches/times_{hyph_label}_auto_rh\", time_distances)\n",
    "\n",
    "            actual_list_hyph.append(end)\n",
    "        # except nx.exception.NetworkXNoPath:\n",
    "        except:\n",
    "\n",
    "            print(\"problem with\", end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0fd035-b6d9-4575-a4aa-1718eed8620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tot_speeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf1ca1-2e62-4786-afff-5a6c0099bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tot_speeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45584772-8270-4521-95d2-8fea25c8601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(tot_junctions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa7c16d-2119-419c-8dd9-c64eafcad3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(tot_lengths) / np.sum(tot_junctions) / np.mean(tot_speeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9230ec7e-6a88-4d6b-bcd8-ebeea75ed16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / (np.sum(tot_lengths) / np.sum(tot_junctions) / np.mean(tot_speeds)) * 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94747ee4-450b-41d9-b0a9-dcc405c352e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87596c55-30a3-4110-8ac7-77e3bdc7bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([time for time in all_time_distances if not np.isnan(time)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca63b335-4784-4328-b75f-8372328862a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(all_time_distances, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05edbf19-fb08-40ac-9360-8d0450e953b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / np.nanmean(all_time_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b197962-7628-4393-89ad-c48e79cf0f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(all_time_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c46862-10b8-4fe5-b377-2de1658b8b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_time_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db487e45-2ca7-49fb-9653-6a042a99a0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "2 * 0.12 / 0.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44e8f8b-036d-4aa8-943f-495d634a97b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(all_distances, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971e7220-f67c-4aa3-b59c-a5b0cb704458",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = 90\n",
    "\n",
    "list_hyph_obj = [\n",
    "    hyph for hyph in exp.hyphaes if hyph.end.is_in(tf) and len(hyph.root.ts()) >= 1\n",
    "]\n",
    "edges = [hyph.get_nodes_within(tf)[1] for hyph in list_hyph_obj]\n",
    "edges = [item for sublist in edges for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9fe363-d6d7-4842-946d-0a9e93db1146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = 70\n",
    "tips = [node for node in get_all_nodes(exp, t) if node.degree(t) == 1]\n",
    "\n",
    "\n",
    "def h(edge, t):\n",
    "    boolean = (edge.end.degree(t) == 1 or edge.begin.degree(t) == 1) and edge.length_um(\n",
    "        t\n",
    "    ) >= 1000\n",
    "    # boolean +=((edge.width(t)*edge.length_um(t))<3000)*edge.width(t)<7\n",
    "    return boolean\n",
    "\n",
    "\n",
    "edges = get_all_edges(exp, t)\n",
    "edges = [\n",
    "    edge\n",
    "    for edge in edges\n",
    "    if np.all(is_in_study_zone(edge.begin, t, 1000, 150, is_circle))\n",
    "]\n",
    "edges = [\n",
    "    edge\n",
    "    for edge in edges\n",
    "    if np.all(is_in_study_zone(edge.end, t, 1000, 150, is_circle))\n",
    "]\n",
    "edge_tip = [edge for edge in edges if h(edge, t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ad9373-74b0-4239-9959-c00560f8c2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "exp.load_tile_information(t)\n",
    "\n",
    "ax = plot_full(\n",
    "    exp,\n",
    "    t,\n",
    "    # nodes = junctions_found_tf\n",
    "    edges=edge_tip,\n",
    "    # nodes = tip_disappear,\n",
    "    dilation=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939fa7db-2fda-497b-a954-d4a95500c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = cv2.SimpleBlobDetector_Params()\n",
    "params.maxArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a76cc8-b578-4abd-b138-e8dd7e1408c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "ax = plot_full_image_with_features(\n",
    "    exp,\n",
    "    tf,\n",
    "    nodes=junctions_found_tf\n",
    "    # nodes = [Node(607965,exp),Node(518481,exp)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3cb6a-37f2-47df-bd1a-716f70f09428",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 50\n",
    "nodes = get_all_nodes(exp, t)\n",
    "tips = [\n",
    "    node\n",
    "    for node in nodes\n",
    "    if node.degree(t) == 1 and node.is_in(t + 1) and len(node.ts()) > 2\n",
    "]\n",
    "tips = [tip for tip in tips if np.all(is_in_study_zone(tip, t, 1000, 150, False))]\n",
    "max_t = 99\n",
    "growing_tips = []\n",
    "\n",
    "for tip in tips:\n",
    "    timesteps = [tim for tim in tip.ts() if tim <= max_t]\n",
    "    tim = timesteps[-1] if len(timesteps) > 0 else tip.ts[-1]\n",
    "    if np.linalg.norm(tip.pos(tim) - tip.pos(t)) >= 40:\n",
    "        growing_tips.append(tip)\n",
    "\n",
    "\n",
    "growing_rhs = [\n",
    "    node\n",
    "    for node in growing_tips\n",
    "    if np.linalg.norm(node.pos(node.ts()[0]) - node.pos(min(node.ts()[-1], 99))) >= 0\n",
    "]\n",
    "\n",
    "# growing_rhs = [\n",
    "#     node\n",
    "#     for node in growing_tips\n",
    "#     if f(node.edges(node.ts()[-1])[0])>=10\n",
    "# ]\n",
    "new_tips = [tip for tip in growing_tips if tip.ts()[0] == t]\n",
    "new_tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c070c0f9-933b-4471-8ef7-e0fee2f630b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tips = [\n",
    "    node\n",
    "    for node in nodes\n",
    "    if node.degree(t) == 1 and node.is_in(t + 1) and len(node.ts()) > 2\n",
    "]\n",
    "tips = [tip for tip in tips if np.all(is_in_study_zone(tip, t, 1000, 150, False))]\n",
    "growing_tips = []\n",
    "for tip in tips:\n",
    "    timesteps = [tim for tim in tip.ts() if tim <= max_t]\n",
    "    tim = timesteps[-1] if len(timesteps) > 0 else tip.ts()[-1]\n",
    "    if np.linalg.norm(tip.pos(tim) - tip.pos(t)) >= 40:\n",
    "        growing_tips.append(tip)\n",
    "new_tips = [tip for tip in growing_tips if tip.ts()[0] == t]\n",
    "new_tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f6f695-f421-4276-b3a1-b8538af69194",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 50\n",
    "new_tips = [tip for tip in growing_tips if tip.ts()[0] == t]\n",
    "new_tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6367079a-29b8-4efa-9b52-b0d9b7ddeffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [hyph.end for hyph in list_hyph_obj] + [\n",
    "    hyph.get_root(tf) for hyph in list_hyph_obj\n",
    "]\n",
    "# nodes = [hyph.end for hyph in list_hyph if hyph.end.is_in(tf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e85aff-df33-4b09-93d1-89d9d7a728b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "ax = plot_full_image_with_features(exp, t, nodes=new_tips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedefe26-e7ab-457e-a0a0-b921ed9780d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 185\n",
    "Node(label, exp).show_source_image(Node(label, exp).ts()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c755916c-4b60-4f95-a21a-e0f540978563",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Node(254, exp).ts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abd4ffd-261f-4434-85e2-3156679804d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "ax = plot_full_image_with_features(exp, t, nodes=growing_rhs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
