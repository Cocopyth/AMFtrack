{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import re\n",
    "import dropbox\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio.v2 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import logging\n",
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import scipy\n",
    "import matplotlib as mpl\n",
    "\n",
    "from subprocess import call\n",
    "from tifffile import imwrite\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from amftrack.util.dbx import (\n",
    "    upload_folder,\n",
    "    download,\n",
    "    read_saved_dropbox_state,\n",
    "    save_dropbox_state,\n",
    "    load_dbx,\n",
    "    get_dropbox_folders_prince,\n",
    "    get_dropbox_video_folders,\n",
    "    download_video_folders_drop,\n",
    "    download_analysis_folders_drop,\n",
    ")\n",
    "from amftrack.pipeline.launching.run import (\n",
    "    run_transfer,\n",
    ")\n",
    "from amftrack.pipeline.launching.run_super import run_parallel_transfer\n",
    "from amftrack.pipeline.launching.run_super import run_parallel_flows\n",
    "from amftrack.pipeline.functions.transport_processing.high_mag_videos.plot_data import (\n",
    "    plot_summary,\n",
    "    save_raw_data,\n",
    ")\n",
    "from amftrack.pipeline.functions.transport_processing.high_mag_videos.high_mag_analysis import (\n",
    "    HighmagDataset,\n",
    "    VideoDataset,\n",
    "    EdgeDataset,\n",
    "    index_videos_dropbox_new,\n",
    "    analysis_run,\n",
    ")\n",
    "from amftrack.pipeline.functions.transport_processing.high_mag_videos.kymo_class import (\n",
    "    KymoVideoAnalysis,\n",
    "    KymoEdgeAnalysis,\n",
    ")\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.debug)\n",
    "mpl.rcParams[\"figure.dpi\"] = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File declaration\n",
    "As this notebook is designed to work with Snellius (now also on a local computer!), two items to separate are the raw video files and the analysis. The raw video files are large, bulky and not so easy to flip through. Ideally, the video files would be downloaded and the analysis would be stored on a separate folder structure entirely. That way, large scale analysis of analysis folders can happen when there are thousands of videos in the dataset, without having to have those raw video folders on hand.\n",
    "\n",
    "Below function will basically make your folders fertile ground to accept all the video info folders and raw video files.\n",
    "\n",
    "### Input:\n",
    "Please give separately the folder where raw video data is stored, and where the analysis will be stored. Also give the dropbox address of the dataset you want to analyze.\n",
    "\n",
    "### Output:\n",
    "The specified dropbox folder will be looked through, and all relevant video information will be downloaded to an analysis folder structure identical to what is present on teh dropbox. The relevant raw video folder structure will also be generated, if specified so. Will also create cache files in the form of .json files such that next time, the scrounging does not have to happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# videos_folder = \"F:\\\\AMOLF_Data\\\\videos\\\\\"\n",
    "# analysis_folder = \"F:\\\\AMOLF_Data\\\\analysis\\\\\"\n",
    "\n",
    "# videos_folder = \"/gpfs/scratch1/shared/amftrackflow/videos/\"\n",
    "# analysis_folder = \"/gpfs/home6/svstaalduine/Analysis/\"\n",
    "videos_folder = \"/projects/0/einf914/videos/\"\n",
    "\n",
    "analysis_folder = \"/projects/0/einf914/analysis_videos/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dropbox_address = \"/DATA/FLUORESCENCE/DATA_NileRed/\"\n",
    "# dropbox_address=  \"/DATA/MYRISTATE/DATA/2_weeks/\"\n",
    "# dropbox_address = \"/DATA/TransportROOT/DATA/\"\n",
    "# dropbox_address = \"/DATA/MYRISTATE/MorrisonDATA/20230508_Plate067/\"\n",
    "dropbox_address = \"/DATA/CocoTransport/\"\n",
    "dropbox_address = \"/DATA/CocoCut/\"\n",
    "\n",
    "# dropbox_address = \"/DATA/MYRISTATE/MorrisonDATA/\"\n",
    "\n",
    "# dropbox_address = \"/DATA/TRANSPORT/DATA/20230308_Plate070/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"20230901_Plate310\",\n",
    "    \"20230902_Plate310\",\n",
    "    \"20230903_Plate310\",\n",
    "    \"20230904_Plate310\",\n",
    "    \"20230905_Plate310\",\n",
    "    \"20230906_Plate310\",\n",
    "]\n",
    "names = [\n",
    "    \"20230810_Plate441\",\n",
    "    \"20230811_Plate441\",\n",
    "    \"20230812_Plate441\",\n",
    "    \"20230813_Plate441\",\n",
    "]\n",
    "names = [\n",
    "    \"20230813_Plate449\",\n",
    "    \"20230814_Plate449\",\n",
    "    \"20230815_Plate449\",\n",
    "    \"20230816_Plate449\",\n",
    "    \"20230818_Plate449\",\n",
    "]\n",
    "names = [\n",
    "    \"20240414_Plate625\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***To delete for 441***\n",
    "\n",
    "20230809_1806_Plate14\n",
    "\n",
    "20230809_2005_Plate14\n",
    "\n",
    "20230809_2205_Plate14\n",
    "\n",
    "20230812_0004_Plate14\n",
    "\n",
    "20230812_1618_Plate14\n",
    "\n",
    "20230813_2230_Plate14\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***To delete for 449***\n",
    "\n",
    "20230814_1218_Plate10\n",
    "\n",
    "20230813_2219_Plate10\n",
    "\n",
    "20230816_1227_Plate10\n",
    "\n",
    "20230816_1628_Plate10\n",
    "\n",
    "20230818_1307_Plate10\n",
    "\n",
    "20230818_1523_Plate10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    video_param_frame = index_videos_dropbox_new(\n",
    "        analysis_folder,\n",
    "        videos_folder,\n",
    "        f\"{dropbox_address}{name}/\",\n",
    "        REDO_SCROUNGING=True,\n",
    "        # date_start=20230801,\n",
    "        # date_end=20230813,\n",
    "        plate_names=None,\n",
    "    )\n",
    "    download_frame = video_param_frame.copy()\n",
    "    run_parallel_transfer(\n",
    "        \"from_drop_video.py\",\n",
    "        [videos_folder],\n",
    "        download_frame,\n",
    "        20,\n",
    "        \"24:00:00\",\n",
    "        \"transfer_test\",\n",
    "    )\n",
    "    clear_output(wait=False)\n",
    "\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Processing\n",
    "\n",
    "Now that the files have been downloaded, it's time to analyse them. In the below code, you'll be able to either do a complete survey of the analysis folder for as many videos as possible, or use the DataFrame of recently downloaded videos to filter for the videos you want to analyse.\n",
    "\n",
    "Also possible to analyse videos directly in this notebook. Be aware again that this is a sequential, and slower analysis than running a SLURM job. \n",
    "\n",
    "### Input:\n",
    "DataFrame filters of all videos to be analysed\n",
    "### Output:\n",
    "Print statements for all parameters of the analysis session that is about to take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For indexing analysis folders\n",
    "for name in names:\n",
    "    video_param_frame = index_videos_dropbox_new(\n",
    "        analysis_folder,\n",
    "        videos_folder,\n",
    "        f\"{dropbox_address}{name}/\",\n",
    "        REDO_SCROUNGING=True,\n",
    "        # date_start=20230801,\n",
    "        # date_end=20230813,\n",
    "        plate_names=None,\n",
    "    )\n",
    "    clear_output(wait=False)\n",
    "\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder_filter = dropbox_address[5:]\n",
    "\n",
    "img_infos = glob.glob(\n",
    "    f\"{analysis_folder}{folder_filter}/**/video_data.json\", recursive=True\n",
    ")\n",
    "vid_anls_frame = pd.DataFrame()\n",
    "for address in img_infos:\n",
    "    add_info = pd.read_json(address, orient=\"index\").T\n",
    "    vid_anls_frame = pd.concat([vid_anls_frame, add_info], ignore_index=True)\n",
    "\n",
    "vid_anls_frame = vid_anls_frame.sort_values(\"unique_id\").reset_index(drop=True)\n",
    "# vid_anls_frame.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "### This is where you can apply the filters. Only those videos will be analyzed. ###\n",
    "####################################################################################\n",
    "\n",
    "# analysis_frame = vid_anls_frame[\n",
    "#     vid_anls_frame[\"imaging_day\"].ge(\"20230814\")\n",
    "# ].reset_index(drop=True)\n",
    "# analysis_frame = vid_anls_frame[vid_anls_frame['xpos'].le(100)].reset_index(drop=True)\n",
    "# analysis_frame = analysis_frame[analysis_frame['mode']==\"F\"]\n",
    "# analysis_frame = vid_anls_frame[vid_anls_frame['plate_id'] != \"20230729_Plate440\"]\n",
    "# analysis_frame = analysis_frame[analysis_frame['video_int'].isin([1])]\n",
    "analysis_frame = vid_anls_frame\n",
    "analysis_frame = analysis_frame.loc[analysis_frame[\"plate_id\"].isin(names)]\n",
    "####################################################################################\n",
    "### Below code will prepare for those videos to be downloaded to videos_folder.  ###\n",
    "####################################################################################\n",
    "\n",
    "print(f\"Number of videos to be analyzed: {len(analysis_frame)}\")\n",
    "# analysis_frame.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analysis_frame[\"plate_id\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SLURM Analysis job\n",
    "Two options: For small analysis, use the first block. This will just do the calculations on the machine. For large-scale analysis, use the second block, as it will create a Snellius job.\n",
    "## Input:\n",
    "Snellius job parameters\n",
    "## Output:\n",
    "Analysis folder will be populated with analysis tiffs and csv sheets. At the same time, this analysis folder will also be uploaded to the dropbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### LARGE VIDEO ANALYSIS\n",
    "\n",
    "nr_parallel = np.min([len(analysis_frame.index), 2])\n",
    "\n",
    "run_parallel_flows(\n",
    "    \"flux_extract.py\",\n",
    "    [analysis_folder, 9, 0.95, 0.005, 200, dropbox_address],\n",
    "    analysis_frame,\n",
    "    nr_parallel,\n",
    "    \"2:00:00\",\n",
    "    \"flux_extract\",\n",
    "    node=\"fat_rome\",\n",
    "    name_job=\"transport\",\n",
    ")\n",
    "clear_output(wait=False)\n",
    "\n",
    "print(\n",
    "    \"Sent all the jobs! Use the command '$ squeue' in the terminal to see the progress\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nr_parallel = np.min([len(analysis_frame.index), 5])\n",
    "\n",
    "run_parallel_transfer(\n",
    "    \"flux_upload.py\",\n",
    "    [analysis_folder, 9, 0.95, 0.005, 200, dropbox_address],\n",
    "    analysis_frame,\n",
    "    nr_parallel,\n",
    "    \"6:00:00\",\n",
    "    \"flux_upload\",\n",
    "    node=\"staging\",\n",
    "    cpus=1,\n",
    "    # dependency = \"flux_extract.sh\",\n",
    "    name_job=\"flux_upload.sh\",\n",
    ")\n",
    "clear_output(wait=False)\n",
    "\n",
    "print(\n",
    "    \"Sent all the jobs! Use the command '$ squeue' in the terminal to see the progress\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"20230810_Plate441_001\" in list(analysis_frame[\"unique_id\"].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "values_id = list(analysis_frame[\"unique_id\"])\n",
    "dbx = load_dbx()\n",
    "img_infos = glob.glob(f\"{analysis_folder}/**/video_data.json\", recursive=True)\n",
    "vid_anls_frame = pd.DataFrame()\n",
    "for address in img_infos:\n",
    "    if os.path.exists(address):\n",
    "        add_info = pd.read_json(address, orient=\"index\").T\n",
    "        # print(add_info['unique_id'].iloc[0])\n",
    "        if add_info[\"unique_id\"].iloc[0] in values_id:\n",
    "            plate_id_video = add_info[\"plate_id\"].iloc[0]\n",
    "            original_path = add_info[\"tot_path_drop\"].iloc[0]\n",
    "\n",
    "            # Replace the specific substring in the target path\n",
    "            target_path = original_path.replace(\n",
    "                f\"/{plate_id_video}\", f\"/KymoSpeeDExtract/{plate_id_video}\"\n",
    "            )\n",
    "            source = \"/\" + target_path + \"/video_data_network.json\"\n",
    "            target = address.replace(\"video_data.json\", \"video_data_network.json\")\n",
    "            try:\n",
    "                results = dbx.files_search(\n",
    "                    \"/\" + target_path, \"video_data_network.json\"\n",
    "                ).matches\n",
    "                if results:\n",
    "                    download(\n",
    "                        source,\n",
    "                        target,\n",
    "                    )\n",
    "            except dropbox.exceptions.ApiError:\n",
    "                print(address)\n",
    "    # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
