{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys  \n",
    "sys.path.insert(0, '/home/cbisot/pycode/MscThesis/')\n",
    "# sys.path.insert(0,r'C:\\Users\\coren\\Documents\\PhD\\Code\\AMFtrack')\n",
    "\n",
    "import pandas as pd\n",
    "from amftrack.util import get_dates_datetime, get_dirname, get_data_info, update_plate_info, \\\n",
    "get_current_folders, get_folders_by_plate_id\n",
    "\n",
    "\n",
    "import ast\n",
    "from amftrack.plotutil import plot_t_tp1\n",
    "from scipy import sparse\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "from pymatreader import read_mat\n",
    "from matplotlib import colors\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.filters import frangi\n",
    "from skimage import filters\n",
    "from random import choice\n",
    "import scipy.sparse\n",
    "import os\n",
    "from amftrack.pipeline.functions.image_processing.extract_graph import from_sparse_to_graph, generate_nx_graph, sparse_to_doc\n",
    "from skimage.feature import hessian_matrix_det\n",
    "from amftrack.pipeline.functions.image_processing.experiment_class_surf import Experiment\n",
    "from amftrack.pipeline.paths.directory import run_parallel_transfer, find_state, directory_scratch, directory_project, directory_archive\n",
    "import dropbox\n",
    "from amftrack.transfer.functions.transfer import upload\n",
    "from subprocess import call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = directory_project\n",
    "# update_plate_info(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_folders = get_current_folders(directory)\n",
    "folders = all_folders.loc[all_folders['Plate']==94]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_drop = 'prince_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_targ = os.path.join(directory_scratch,'temp')+'/'\n",
    "# directory_targ = os.path.join(directory_archive,'Prince_Data')+'/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileMetadata(client_modified=datetime.datetime(2021, 12, 23, 8, 55, 46), content_hash='c47ecb86baa70489f9978b728e675c048cca2625e5bb4da6e128586c2abd6e0d', export_info=NOT_SET, file_lock_info=NOT_SET, has_explicit_shared_members=NOT_SET, id='id:mcSwhUrTbbAAAAAAAAAFcg', is_downloadable=True, media_info=NOT_SET, name='data_info.json', parent_shared_folder_id=NOT_SET, path_display='/data_info.json', path_lower='/data_info.json', property_groups=NOT_SET, rev='5d3cc677af9368b8306a1', server_modified=datetime.datetime(2021, 12, 23, 8, 55, 46), sharing_info=NOT_SET, size=1487862, symlink_info=NOT_SET)\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "from amftrack.transfer.functions.transfer import download, zip_file,unzip_file\n",
    "\n",
    "API = str(np.load('/home/cbisot/pycode/API_drop.npy'))\n",
    "source = f'data_info.json'\n",
    "target = '/scratch-shared/amftrack/temp/data_info.json'\n",
    "# download(API,source,target,end='')\n",
    "upload(API,target,f\"/{source}\", chunk_size=256 * 1024 * 1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbx = dropbox.Dropbox(API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropbox.files.CommitInfo(path=target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To transfer data from surfsara to dropbox**\n",
    "- use the 'toward_drop.py' function including the folders of interest and using dir_drop as a parameter indicating where in dropbox the data should go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 242716\n",
      "Submitted batch job 242717\n",
      "Submitted batch job 242718\n",
      "Submitted batch job 242719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 242720\n",
      "Submitted batch job 242721\n",
      "Submitted batch job 242722\n",
      "Submitted batch job 242723\n",
      "Submitted batch job 242724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 242725\n",
      "Submitted batch job 242726\n",
      "Submitted batch job 242727\n",
      "Submitted batch job 242728\n",
      "Submitted batch job 242729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 242730\n",
      "Submitted batch job 242731\n",
      "Submitted batch job 242732\n",
      "Submitted batch job 242733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 242734\n",
      "Submitted batch job 242735\n",
      "Submitted batch job 242736\n",
      "Submitted batch job 242737\n",
      "Submitted batch job 242738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 242739\n",
      "Submitted batch job 242740\n",
      "Submitted batch job 242741\n",
      "Submitted batch job 242742\n",
      "Submitted batch job 242743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 242744\n",
      "Submitted batch job 242745\n",
      "Submitted batch job 242746\n",
      "Submitted batch job 242747\n",
      "Submitted batch job 242748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 242749\n"
     ]
    }
   ],
   "source": [
    "dir_drop = 'prince_data'\n",
    "run_parallel_transfer('toward_drop.py',[directory,dir_drop],folders,10,'10:00','staging',cpus = 1,node = 'staging')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To transfer data from dropbox to surfsara**\n",
    "- use the 'from_drop.py' function including the folders of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 242758\n",
      "Submitted batch job 242759\n",
      "Submitted batch job 242760\n",
      "Submitted batch job 242761\n",
      "Submitted batch job 242762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 242763\n",
      "Submitted batch job 242764\n",
      "Submitted batch job 242765\n",
      "Submitted batch job 242766\n",
      "Submitted batch job 242767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 242768\n",
      "Submitted batch job 242769\n",
      "Submitted batch job 242770\n",
      "Submitted batch job 242771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 242772\n",
      "Submitted batch job 242773\n",
      "Submitted batch job 242774\n",
      "Submitted batch job 242775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 242776\n",
      "Submitted batch job 242777\n",
      "Submitted batch job 242778\n",
      "Submitted batch job 242779\n",
      "Submitted batch job 242780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 242781\n",
      "Submitted batch job 242782\n",
      "Submitted batch job 242783\n",
      "Submitted batch job 242784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 242785\n",
      "Submitted batch job 242786\n",
      "Submitted batch job 242787\n",
      "Submitted batch job 242788\n",
      "Submitted batch job 242789\n",
      "Submitted batch job 242790\n",
      "Submitted batch job 242791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n",
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n"
     ]
    }
   ],
   "source": [
    "unzip = 'False'\n",
    "flatten = False\n",
    "run_parallel_transfer('from_drop.py',[directory_targ,dir_drop,unzip,flatten],folders,5,'10:00','staging',cpus = 1,node = 'staging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 242022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: Single node jobs run on a shared node by default. Add --exclusive if you want to use a node exclusively.\n"
     ]
    }
   ],
   "source": [
    "must_zip = True\n",
    "must_unzip = False\n",
    "run_parallel_transfer('within_surf.py',[directory,directory_targ,must_zip,must_unzip],folders,5,'10:00','staging',cpus = 1,node = 'staging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plates = set(folders['Plate'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tar: /archive/cbisot/prince_data/94_20201123.tar: Cannot open: No such file or directory\n",
      "tar: Error is not recoverable: exiting now\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call(f\"tar cvf /archive/cbisot/prince_data/94_20201123.tar {directory_targ}94_20201123*\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_parallel_whole_plate(code, args, folders, num_parallel, time, name,cpus = 128,node = 'thin'):\n",
    "    op_id = time_ns()\n",
    "    folders.to_json(f'{directory_scratch}temp/{op_id}.json')# temporary file\n",
    "    plates = set(folders['Plate'].values)\n",
    "    length = len(plates)\n",
    "    begin_skel = 0\n",
    "    end_skel = length // num_parallel + 1\n",
    "    args_str = [str(arg) for arg in args]\n",
    "    arg_str = \" \".join(args_str)\n",
    "    arg_str_out = \"_\".join([str(arg) for arg in args if type(arg)!=str])\n",
    "    for j in range(begin_skel, end_skel):\n",
    "        start = num_parallel * j\n",
    "        stop = num_parallel * j + num_parallel - 1\n",
    "        ide = time_ns()\n",
    "        my_file = open(path_job, \"w\")\n",
    "        my_file.write(\n",
    "            f\"#!/bin/bash \\n#Set job requirements \\n#SBATCH --nodes=1 \\n#SBATCH -t {time}\\n #SBATCH --ntask=1 \\n#SBATCH --cpus-per-task={cpus}\\n#SBATCH -p {node} \\n\"\n",
    "        )\n",
    "        my_file.write(\n",
    "            f'#SBATCH -o \"{path_code}slurm/{name}_{arg_str_out}_{start}_{stop}_{ide}.out\" \\n'\n",
    "        )\n",
    "        my_file.write(f\"source /home/cbisot/miniconda3/etc/profile.d/conda.sh\\n\")\n",
    "        my_file.write(f\"conda activate amftrack\\n\")\n",
    "        my_file.write(f\"for i in `seq {start} {stop}`; do\\n\")\n",
    "        my_file.write(f\"\\t python {path_code}amftrack/pipeline/scripts/image_processing/{code} {arg_str} {op_id} $i &\\n\")\n",
    "        my_file.write(\"done\\n\")\n",
    "        my_file.write(\"wait\\n\")\n",
    "        my_file.close()\n",
    "        call(f\"sbatch {path_job}\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel_transfer(code, args, folders, num_parallel, time, name,cpus = 1,node = 'staging',name_job='transfer.sh'):\n",
    "    path_job = f'{path_bash}{name_job}'\n",
    "    op_id = time_ns()\n",
    "    folders.to_json(f'{directory_scratch}temp/{op_id}.json')# temporary file\n",
    "    length = len(folders)\n",
    "    begin_skel = 0\n",
    "    end_skel = length // num_parallel + 1\n",
    "    args_str = [str(arg) for arg in args]\n",
    "    arg_str = \" \".join(args_str)\n",
    "    arg_str_out = \"_\".join([str(arg) for arg in args if type(arg)!=str])\n",
    "    for j in range(begin_skel, end_skel):\n",
    "        start = num_parallel * j\n",
    "        stop = num_parallel * j + num_parallel - 1\n",
    "        ide = time_ns()\n",
    "        my_file = open(path_job, \"w\")\n",
    "        my_file.write(\n",
    "            f\"#!/bin/bash \\n#Set job requirements \\n#SBATCH --nodes=1 \\n#SBATCH -t {time}\\n #SBATCH --ntask=1 \\n#SBATCH --cpus-per-task={cpus}\\n#SBATCH -p {node} \\n\"\n",
    "        )\n",
    "        my_file.write(\n",
    "            f'#SBATCH -o \"{path_code}slurm/{name}_{arg_str_out}_{start}_{stop}_{ide}.out\" \\n'\n",
    "        )\n",
    "        my_file.write(f\"source /home/cbisot/miniconda3/etc/profile.d/conda.sh\\n\")\n",
    "        my_file.write(f\"conda activate amftrack\\n\")\n",
    "        my_file.write(f\"for i in `seq {start} {stop}`; do\\n\")\n",
    "        my_file.write(f\"\\t python {path_code}amftrack/transfer/scripts/{code} {arg_str} {op_id} $i &\\n\")\n",
    "        my_file.write(\"done\\n\")\n",
    "        my_file.write(\"wait\\n\")\n",
    "        my_file.close()\n",
    "        call(f\"sbatch {path_job}\", shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
